{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc9b596e-5bd9-4be8-8dff-749f1f3b4d89",
   "metadata": {},
   "source": [
    "# 模型微调(英译中翻译)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562901d7-f9eb-4449-a638-ad05fd8c516e",
   "metadata": {},
   "source": [
    "### 1 数据预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b966c5af-6754-408b-9a0a-949939fcbf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1929 or 1989?\t1929年还是1989年?\n",
      "PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.\t巴黎-随着经济危机不断加深和蔓延，整个世界一直在寻找历史上的类似事件希望有助于我们了解目前正在发生的情况。\n",
      "At the start of the crisis, many people likened it to 1982 or 1973, which was reassuring, because both dates refer to classical cyclical downturns.\t一开始，很多人把这次危机比作1982年或1973年所发生的情况，这样得类比是令人宽心的，因为这两段时期意味着典型的周期性衰退。\n",
      "Today, the mood is much grimmer, with references to 1929 and 1931 beginning to abound, even if some governments continue to behave as if the crisis was more classical than exceptional.\t如今人们的心情却是沉重多了，许多人开始把这次危机与1929年和1931年相比，即使一些国家政府的表现仍然似乎把视目前的情况为是典型的而看见的衰退。\n",
      "The tendency is either excessive restraint (Europe) or a diffusion of the effort (the United States).\t目前的趋势是，要么是过度的克制（欧洲），要么是努力的扩展（美国）。\n"
     ]
    }
   ],
   "source": [
    "!head -5 News-Commentary-v16-1000.en-zh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24b805d5-ea63-49e3-8c2b-b772a6cecd21",
   "metadata": {},
   "source": [
    "### 2 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8b3e9d3-7047-42be-b4cc-3bc48e42d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84006d49-d727-4f49-a760-ef03c876120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt='\"{txt}\"\\n请将上面的句子翻译成中文(仅输出翻译结果):\\n'\n",
    "with open('News-Commentary-v16-1000.en-zh','r') as f:\n",
    "    with open('sample.jsonl','w') as g:\n",
    "        for i,line in enumerate(f):\n",
    "            en,zh=line.strip().split('\\t')\n",
    "            s= [{'user':prompt.format(txt=en)},\n",
    "                {'assistant':zh}]\n",
    "            g.write(json.dumps(s,ensure_ascii=False)+'\\n')\n",
    "        for _ in range(5):\n",
    "            g.write(json.dumps([{'user':'你是谁？'}, {'assistant':'我是工银智涌大模型。'}],ensure_ascii=False)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bf3ce31-8c19-4875-b025-a61008b0bd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'user': '\"Irish workers are leaving in droves to Canada, Australia, and the United States.\"\\n请将上面的句子翻译成中文(仅输出翻译结果):\\n'},\n",
       " {'assistant': '爱尔兰工人正在迁往加拿大、澳大利亚和美国。'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5402fe21-2ae3-48b8-a776-2bcd98f8b246",
   "metadata": {},
   "source": [
    "### 3 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be00088-d4d5-4f1e-9c89-74dab315321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(t='ft', i='sample.jsonl', o='', n=8388608, c='gzip', batch_size=65536, max_len=8193, cores=-1, max_num=1, max_pixels=2073600, tokenizer='Qwen2.5-7B-Instruct', image_path='', tmp='tmp', stack=False, T=False, C=True, pad=False, filter=True)\n",
      "/mnt\n",
      "########## begine converting ft data with 204 executors.###########\n",
      "1005it [00:00, 1343.70it/s]\n",
      "/mnt/tmp/sample-00000 stored in parquet with 1005 samples\n",
      "/mnt/sample.jsonl has been converted into /mnt/sample_Qwen2.5-7B-Instruct successfully!\n"
     ]
    }
   ],
   "source": [
    "!python -m jllm.raw2ids \\\n",
    "    --tokenizer Qwen2.5-7B-Instruct \\\n",
    "    -i sample.jsonl \\\n",
    "    --max_len 8193 -C --filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36eeb743-e622-4563-9597-3f9965ff9635",
   "metadata": {},
   "source": [
    "### 3.1 数据检查"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be35198f-a346-4915-8110-e53b6103697f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('Qwen2.5-7B-Instruct')\n",
    "import pyarrow.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab54f8f1-4dc1-448b-8eaf-cb90c12064d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "253a903b-8503-4e14-8204-be1e95d7443f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d8802b0e-abf3-43bb-a27d-3438e289e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyarrow.parquet.read_table('sample_Qwen2.5-7B-Instruct/sample-00000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0caaaa71-b6d3-4a52-8742-a3857a423d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids=data['input_ids'].to_numpy().tolist()\n",
    "labels=[i.copy() for i in data['labels'].to_numpy().tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eaa2bbb1-a774-437b-aa06-74f95df5b7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9482efe9-d97b-49a5-868d-018c6a7ad2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels[idx][0]=-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2586ef9e-9680-4ee9-947a-20798433ae87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "你是谁？<|im_end|>\n",
      "<|im_start|>assistant\n",
      "我是工银智涌大模型。<|im_end|>\n",
      "********************************我是工银智涌大模型。<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids[idx]));\n",
    "labels[idx][labels[idx]==-100]=9\n",
    "print(tokenizer.decode(labels[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2a40cc-c6a5-409b-b0a9-5afba39c1b1a",
   "metadata": {},
   "source": [
    "## 4 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae4e2ce-afd6-4bff-9122-084d140a4950",
   "metadata": {},
   "source": [
    "#### 4.1全参微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02853282-09e4-4587-aaa7-68a09c430ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-28 19:19:00,024] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-28 19:19:01,406] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-11-28 19:19:01,406] [INFO] [runner.py:585:main] cmd = /home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --module --enable_each_rank_log=None jllm.train_pipe --model pretrained_hf --num_train_epochs 2 --train_data sample_Qwen2.5-7B-Instruct --pipe_parallel_size 4 --model_parallel_size 1 --sequence_parallel_size 1 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --partition_method fast --split_dlayer --only_ckpt_model --max_num_checkpoints 2 --split_dlayer --learning_rate 1e-5 --low_mem\n",
      "[2024-11-28 19:19:06,410] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-28 19:19:07,830] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2024-11-28 19:19:07,830] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2024-11-28 19:19:07,830] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2024-11-28 19:19:07,830] [INFO] [launch.py:164:main] dist_world_size=8\n",
      "[2024-11-28 19:19:07,830] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "[2024-11-28 19:19:07,851] [INFO] [launch.py:256:main] process 2474750 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=0', '--model', 'pretrained_hf', '--num_train_epochs', '2', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--sequence_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_model', '--max_num_checkpoints', '2', '--split_dlayer', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-28 19:19:07,866] [INFO] [launch.py:256:main] process 2474751 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=1', '--model', 'pretrained_hf', '--num_train_epochs', '2', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--sequence_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_model', '--max_num_checkpoints', '2', '--split_dlayer', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-28 19:19:07,881] [INFO] [launch.py:256:main] process 2474752 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=2', '--model', 'pretrained_hf', '--num_train_epochs', '2', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--sequence_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_model', '--max_num_checkpoints', '2', '--split_dlayer', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-28 19:19:07,898] [INFO] [launch.py:256:main] process 2474753 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=3', '--model', 'pretrained_hf', '--num_train_epochs', '2', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--sequence_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_model', '--max_num_checkpoints', '2', '--split_dlayer', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-28 19:19:07,916] [INFO] [launch.py:256:main] process 2474754 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=4', '--model', 'pretrained_hf', '--num_train_epochs', '2', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--sequence_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_model', '--max_num_checkpoints', '2', '--split_dlayer', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-28 19:19:07,935] [INFO] [launch.py:256:main] process 2474755 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=5', '--model', 'pretrained_hf', '--num_train_epochs', '2', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--sequence_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_model', '--max_num_checkpoints', '2', '--split_dlayer', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-28 19:19:07,962] [INFO] [launch.py:256:main] process 2474756 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=6', '--model', 'pretrained_hf', '--num_train_epochs', '2', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--sequence_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_model', '--max_num_checkpoints', '2', '--split_dlayer', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-28 19:19:07,995] [INFO] [launch.py:256:main] process 2474758 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=7', '--model', 'pretrained_hf', '--num_train_epochs', '2', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--sequence_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_model', '--max_num_checkpoints', '2', '--split_dlayer', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-28 19:19:13,513] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-28 19:19:13,567] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-28 19:19:13,605] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-28 19:19:13,612] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-28 19:19:13,618] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-28 19:19:13,632] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-28 19:19:13,636] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-28 19:19:13,647] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:209: ImportWarning: \n",
      "    *************************************************************************************************************\n",
      "    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..\n",
      "    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..\n",
      "    The backend in torch.distributed.init_process_group set to hccl now..\n",
      "    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..\n",
      "    The device parameters have been replaced with npu in the function below:\n",
      "    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty\n",
      "    *************************************************************************************************************\n",
      "    \n",
      "  warnings.warn(msg, ImportWarning)\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "[2024-11-28 19:19:29,087] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-28 19:19:29,147] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-28 19:19:29,604] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-28 19:19:29,605] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-28 19:19:30,331] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-28 19:19:30,499] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-28 19:19:30,568] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-28 19:19:31,015] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-28 19:19:31,174] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(data=0, pipe=0, model=0): 0, ProcessCoord(data=0, pipe=1, model=0): 1, ProcessCoord(data=0, pipe=2, model=0): 2, ProcessCoord(data=0, pipe=3, model=0): 3, ProcessCoord(data=1, pipe=0, model=0): 4, ProcessCoord(data=1, pipe=1, model=0): 5, ProcessCoord(data=1, pipe=2, model=0): 6, ProcessCoord(data=1, pipe=3, model=0): 7}\n",
      "[2024-11-28 19:19:35,705] [INFO] [train_pipe.py:484:custom_partition_layers] Partitioning pipeline stages with method 0, 1, 2, 3, 4\n",
      "stage=0 layers=1\n",
      "     0: LlamaStartPipe 15\n",
      "stage=1 layers=1\n",
      "     1: LlamaMiddlePipe 15\n",
      "stage=2 layers=1\n",
      "     2: LlamaMiddlePipe 16\n",
      "stage=3 layers=1\n",
      "     3: LlamaEndPipe 10\n",
      "  loss: LlamaCrossEntropyLoss\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-28 19:20:18,305] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-28 19:20:18,305] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-28 19:20:18,305] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-28 19:20:18,305] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-28 19:20:18,305] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-28 19:20:18,305] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-28 19:20:18,306] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-28 19:20:18,308] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-28 19:20:18,307] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-28 19:20:18,552] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-28 19:20:18,553] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-28 19:20:18,553] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-28 19:20:18,554] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2024-11-28 19:20:18,554] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer\n",
      "[2024-11-28 19:20:18,569] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-28 19:20:18,570] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-28 19:20:18,576] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-28 19:20:18,577] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-28 19:20:18,577] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-28 19:20:18,578] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-28 19:20:18,578] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-28 19:20:18,843] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer\n",
      "[2024-11-28 19:20:18,844] [INFO] [utils.py:782:see_memory_usage] MA 4.15 GB         Max_MA 8.47 GB         CA 9.63 GB         Max_CA 10 GB \n",
      "[2024-11-28 19:20:18,844] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.83 GB, percent = 6.7%\n",
      "[2024-11-28 19:20:19,092] [INFO] [utils.py:781:see_memory_usage] before initializing group 0\n",
      "[2024-11-28 19:20:19,093] [INFO] [utils.py:782:see_memory_usage] MA 4.15 GB         Max_MA 4.15 GB         CA 9.63 GB         Max_CA 10 GB \n",
      "[2024-11-28 19:20:19,094] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.83 GB, percent = 6.7%\n",
      "[2024-11-28 19:20:19,360] [INFO] [utils.py:781:see_memory_usage] after initializing group 0\n",
      "[2024-11-28 19:20:19,361] [INFO] [utils.py:782:see_memory_usage] MA 16.47 GB         Max_MA 16.47 GB         CA 30.18 GB         Max_CA 30 GB \n",
      "[2024-11-28 19:20:19,362] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.83 GB, percent = 6.7%\n",
      "[2024-11-28 19:20:19,609] [INFO] [utils.py:781:see_memory_usage] before initializing group 1\n",
      "[2024-11-28 19:20:19,610] [INFO] [utils.py:782:see_memory_usage] MA 16.47 GB         Max_MA 16.47 GB         CA 30.18 GB         Max_CA 30 GB \n",
      "[2024-11-28 19:20:19,611] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.83 GB, percent = 6.7%\n",
      "[2024-11-28 19:20:19,860] [INFO] [utils.py:781:see_memory_usage] after initializing group 1\n",
      "[2024-11-28 19:20:19,861] [INFO] [utils.py:782:see_memory_usage] MA 16.47 GB         Max_MA 16.47 GB         CA 30.18 GB         Max_CA 30 GB \n",
      "[2024-11-28 19:20:19,861] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.83 GB, percent = 6.7%\n",
      "[2024-11-28 19:20:20,128] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer\n",
      "[2024-11-28 19:20:20,129] [INFO] [utils.py:782:see_memory_usage] MA 16.47 GB         Max_MA 16.47 GB         CA 30.18 GB         Max_CA 30 GB \n",
      "[2024-11-28 19:20:20,129] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.83 GB, percent = 6.7%\n",
      "[2024-11-28 19:20:20,129] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = BF16_Optimizer\n",
      "[2024-11-28 19:20:20,130] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-28 19:20:20,130] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0xffff1450bb80>\n",
      "[2024-11-28 19:20:20,130] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 1e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2024-11-28 19:20:20,131] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2024-11-28 19:20:20,131] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-28 19:20:20,131] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2024-11-28 19:20:20,131] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2024-11-28 19:20:20,131] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xffff1450ba90>\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2024-11-28 19:20:20,132] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 16\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-28 19:20:20,133] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   loss_scale ................... 1.0\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-28 19:20:20,134] [INFO] [config.py:1003:print]   steps_per_print .............. 1\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   train_batch_size ............. 32\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   zero_enabled ................. False\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0\n",
      "[2024-11-28 19:20:20,135] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"seed\": 1.234000e+03, \n",
      "    \"steps_per_print\": 1, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": false, \n",
      "        \"cpu_checkpointing\": false, \n",
      "        \"contiguous_memory_optimization\": false, \n",
      "        \"number_checkpoints\": null, \n",
      "        \"synchronize_checkpoint_boundary\": false, \n",
      "        \"profile\": false\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }\n",
      "}\n",
      "[2024-11-28 19:20:20,135] [INFO] [engine.py:105:__init__] CONFIG: micro_batches=16 micro_batch_size=1\n",
      "[2024-11-28 19:20:20,136] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-28 19:20:20,680] [INFO] [engine.py:165:__init__] RANK=0 STAGE=0 LAYERS=1 [0, 1) STAGE_PARAMS=2205770240 (2205.770M) TOTAL_PARAMS=7615616512 (7615.617M) UNIQUE_PARAMS=7615616512 (7615.617M)\n",
      "[2024-11-28 19:20:20,680] [INFO] [engine.py:165:__init__] RANK=3 STAGE=3 LAYERS=1 [3, 4) STAGE_PARAMS=1710289920 (1710.290M) TOTAL_PARAMS=7615616512 (7615.617M) UNIQUE_PARAMS=7615616512 (7615.617M)\n",
      "[2024-11-28 19:20:20,680] [INFO] [engine.py:165:__init__] RANK=1 STAGE=1 LAYERS=1 [1, 2) STAGE_PARAMS=1835094016 (1835.094M) TOTAL_PARAMS=7615616512 (7615.617M) UNIQUE_PARAMS=7615616512 (7615.617M)\n",
      "[2024-11-28 19:20:20,680] [INFO] [engine.py:165:__init__] RANK=2 STAGE=2 LAYERS=1 [2, 3) STAGE_PARAMS=1864462336 (1864.462M) TOTAL_PARAMS=7615616512 (7615.617M) UNIQUE_PARAMS=7615616512 (7615.617M)\n",
      "Namespace(local_rank=0, model='pretrained_hf', train_data='sample_Qwen2.5-7B-Instruct', eval_data='', from_ckpt='', resume_ckpt='', only_load_model=False, tag=None, ds_config=None, zero_stage=0, split_dlayer=True, num_layers_per_decoder=None, emb_partitions=1, timeout=1800, pipe_parallel_size=4, encoder_pipe_parallel_size=0, model_parallel_size=1, sequence_parallel_size=1, offload=False, partition_method='fast', multi_layerspec=False, num_train_epochs=2, per_device_train_batch_size=1, gradient_accumulation_steps=16, weight_decay=0.0, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=0, learning_rate=1e-05, seq_len=195, block_mask=False, skip_train_steps=set(), steps_per_print=1, steps_per_eval=-1, steps_per_checkpoint=-1, checkpoint='', background_executor='process', ckpt_step_gt=0, best_of=1, ckpt_epoch=set(), skip_epoch=set(), max_num_checkpoints=2, only_ckpt_model=True, only_ckpt_lora=False, only_cache_model=False, early_stop=-1, checkpoint_grad_interval=0, no_checkpoint_grad_step=1000000, low_mem=True, no_shuf=False, no_safetensor=False, init=False, cache_model=None, seed=1234, output_dir='', lora_dim=0, lora_alpha=1, lora_module_name='qkv_proj,o_proj,gate_up_proj,down_proj', only_optimize_lora=False, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None, device='npu', global_rank=0, world_size=8, data_parallel_size=2, num_training_steps=62, block_class_id=-100)\n",
      "Data Reading Info:\n",
      "Rank\tSamples\tLength\tReadTime\n",
      "0\t1005\t195\t1\n",
      "4\t1005\t195\t1\n",
      "Beginning of Epoch: 1/2\n",
      "Partition Rank: 1/1\n",
      "Partition Name: sample_Qwen2.5-7B-Instruct/sample-00000\n",
      "Total Partition Steps: 31/62\n",
      "[2024-11-28 19:20:33,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[9.993582535855265e-06, 9.993582535855265e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 1 loss: 3.1875 iter time (s): 9.172 samples/sec: 3.489\n",
      "[2024-11-28 19:20:36,667] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[9.974346616959476e-06, 9.974346616959476e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 2 loss: 1.9375 iter time (s): 2.633 samples/sec: 12.152\n",
      "[2024-11-28 19:20:39,358] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[9.942341621640558e-06, 9.942341621640558e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 3 loss: 1.7188 iter time (s): 2.690 samples/sec: 11.894\n",
      "[2024-11-28 19:20:41,766] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[9.897649706262474e-06, 9.897649706262474e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 4 loss: 1.7969 iter time (s): 2.407 samples/sec: 13.297\n",
      "[2024-11-28 19:20:44,217] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[9.840385594331022e-06, 9.840385594331022e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 5 loss: 1.5781 iter time (s): 2.450 samples/sec: 13.063\n",
      "[2024-11-28 19:20:46,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[9.770696282000245e-06, 9.770696282000245e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 6 loss: 1.5469 iter time (s): 2.432 samples/sec: 13.156\n",
      "[2024-11-28 19:20:49,221] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.688760660735403e-06, 9.688760660735403e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 7 loss: 1.4609 iter time (s): 2.569 samples/sec: 12.457\n",
      "[2024-11-28 19:20:51,780] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[9.594789058101154e-06, 9.594789058101154e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 8 loss: 1.5078 iter time (s): 2.559 samples/sec: 12.507\n",
      "[2024-11-28 19:20:54,545] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[9.48902269785371e-06, 9.48902269785371e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 9 loss: 1.5625 iter time (s): 2.764 samples/sec: 11.579\n",
      "[2024-11-28 19:20:57,165] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[9.371733080722911e-06, 9.371733080722911e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 10 loss: 1.7031 iter time (s): 2.619 samples/sec: 12.219\n",
      "[2024-11-28 19:20:59,693] [INFO] [logging.py:96:log_dist] [Rank 0] step=11, skipped=0, lr=[9.243221287473755e-06, 9.243221287473755e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 11 loss: 1.4219 iter time (s): 2.527 samples/sec: 12.662\n",
      "[2024-11-28 19:21:02,236] [INFO] [logging.py:96:log_dist] [Rank 0] step=12, skipped=0, lr=[9.103817206036383e-06, 9.103817206036383e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 12 loss: 1.4531 iter time (s): 2.542 samples/sec: 12.588\n",
      "[2024-11-28 19:21:04,745] [INFO] [logging.py:96:log_dist] [Rank 0] step=13, skipped=0, lr=[8.953878684688492e-06, 8.953878684688492e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 13 loss: 1.5781 iter time (s): 2.508 samples/sec: 12.760\n",
      "[2024-11-28 19:21:07,521] [INFO] [logging.py:96:log_dist] [Rank 0] step=14, skipped=0, lr=[8.793790613463956e-06, 8.793790613463956e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 14 loss: 1.5234 iter time (s): 2.775 samples/sec: 11.530\n",
      "[2024-11-28 19:21:10,041] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[8.6239639361456e-06, 8.6239639361456e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 15 loss: 1.7188 iter time (s): 2.519 samples/sec: 12.706\n",
      "[2024-11-28 19:21:12,628] [INFO] [logging.py:96:log_dist] [Rank 0] step=16, skipped=0, lr=[8.444834595378434e-06, 8.444834595378434e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 16 loss: 1.5391 iter time (s): 2.586 samples/sec: 12.374\n",
      "[2024-11-28 19:21:15,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=17, skipped=0, lr=[8.256862413611113e-06, 8.256862413611113e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 17 loss: 1.3047 iter time (s): 2.534 samples/sec: 12.627\n",
      "[2024-11-28 19:21:17,650] [INFO] [logging.py:96:log_dist] [Rank 0] step=18, skipped=0, lr=[8.060529912738316e-06, 8.060529912738316e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 18 loss: 1.4219 iter time (s): 2.486 samples/sec: 12.872\n",
      "[2024-11-28 19:21:20,106] [INFO] [logging.py:96:log_dist] [Rank 0] step=19, skipped=0, lr=[7.856341075473963e-06, 7.856341075473963e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 19 loss: 1.4766 iter time (s): 2.454 samples/sec: 13.038\n",
      "[2024-11-28 19:21:22,510] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[7.644820051634813e-06, 7.644820051634813e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 20 loss: 1.6484 iter time (s): 2.404 samples/sec: 13.312\n",
      "[2024-11-28 19:21:25,049] [INFO] [logging.py:96:log_dist] [Rank 0] step=21, skipped=0, lr=[7.4265098126554065e-06, 7.4265098126554065e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 21 loss: 1.5000 iter time (s): 2.537 samples/sec: 12.612\n",
      "[2024-11-28 19:21:27,554] [INFO] [logging.py:96:log_dist] [Rank 0] step=22, skipped=0, lr=[7.201970757788172e-06, 7.201970757788172e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 22 loss: 1.4844 iter time (s): 2.504 samples/sec: 12.782\n",
      "[2024-11-28 19:21:29,997] [INFO] [logging.py:96:log_dist] [Rank 0] step=23, skipped=0, lr=[6.971779275566593e-06, 6.971779275566593e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 23 loss: 1.5078 iter time (s): 2.442 samples/sec: 13.103\n",
      "[2024-11-28 19:21:32,635] [INFO] [logging.py:96:log_dist] [Rank 0] step=24, skipped=0, lr=[6.736526264224101e-06, 6.736526264224101e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 24 loss: 1.6953 iter time (s): 2.637 samples/sec: 12.135\n",
      "[2024-11-28 19:21:35,219] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[6.496815614866792e-06, 6.496815614866792e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 25 loss: 1.6094 iter time (s): 2.577 samples/sec: 12.417\n",
      "[2024-11-28 19:21:37,767] [INFO] [logging.py:96:log_dist] [Rank 0] step=26, skipped=0, lr=[6.2532626612936035e-06, 6.2532626612936035e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 26 loss: 1.6094 iter time (s): 2.553 samples/sec: 12.535\n",
      "[2024-11-28 19:21:40,135] [INFO] [logging.py:96:log_dist] [Rank 0] step=27, skipped=0, lr=[6.006492600443301e-06, 6.006492600443301e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 27 loss: 1.4844 iter time (s): 2.367 samples/sec: 13.521\n",
      "[2024-11-28 19:21:42,585] [INFO] [logging.py:96:log_dist] [Rank 0] step=28, skipped=0, lr=[5.757138887522884e-06, 5.757138887522884e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 28 loss: 1.6562 iter time (s): 2.448 samples/sec: 13.070\n",
      "[2024-11-28 19:21:45,248] [INFO] [logging.py:96:log_dist] [Rank 0] step=29, skipped=0, lr=[5.505841609937162e-06, 5.505841609937162e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 29 loss: 1.4297 iter time (s): 2.663 samples/sec: 12.017\n",
      "[2024-11-28 19:21:47,897] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[5.253245844193564e-06, 5.253245844193564e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 30 loss: 1.5781 iter time (s): 2.647 samples/sec: 12.087\n",
      "[2024-11-28 19:21:50,437] [INFO] [logging.py:96:log_dist] [Rank 0] step=31, skipped=0, lr=[5e-06, 5e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 31 loss: 1.6484 iter time (s): 2.535 samples/sec: 12.623\n",
      "Wash the memory of train data clean for 1 seconds ......\n",
      "Beginning of Epoch: 2/2\n",
      "Partition Rank: 1/1\n",
      "Partition Name: sample_Qwen2.5-7B-Instruct/sample-00000\n",
      "Total Partition Steps: 62/62\n",
      "[2024-11-28 19:22:00,314] [INFO] [logging.py:96:log_dist] [Rank 0] step=32, skipped=0, lr=[4.746754155806437e-06, 4.746754155806437e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 32 loss: 0.9648 iter time (s): 2.896 samples/sec: 11.049\n",
      "[2024-11-28 19:22:03,004] [INFO] [logging.py:96:log_dist] [Rank 0] step=33, skipped=0, lr=[4.49415839006284e-06, 4.49415839006284e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 33 loss: 0.8516 iter time (s): 2.690 samples/sec: 11.897\n",
      "[2024-11-28 19:22:05,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=34, skipped=0, lr=[4.2428611124771184e-06, 4.2428611124771184e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 34 loss: 0.9297 iter time (s): 2.357 samples/sec: 13.578\n",
      "[2024-11-28 19:22:07,818] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[3.993507399556699e-06, 3.993507399556699e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 35 loss: 0.8477 iter time (s): 2.455 samples/sec: 13.033\n",
      "[2024-11-28 19:22:10,330] [INFO] [logging.py:96:log_dist] [Rank 0] step=36, skipped=0, lr=[3.7467373387063973e-06, 3.7467373387063973e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 36 loss: 0.8359 iter time (s): 2.510 samples/sec: 12.748\n",
      "[2024-11-28 19:22:12,932] [INFO] [logging.py:96:log_dist] [Rank 0] step=37, skipped=0, lr=[3.5031843851332105e-06, 3.5031843851332105e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 37 loss: 0.7461 iter time (s): 2.601 samples/sec: 12.303\n",
      "[2024-11-28 19:22:15,614] [INFO] [logging.py:96:log_dist] [Rank 0] step=38, skipped=0, lr=[3.2634737357758994e-06, 3.2634737357758994e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 38 loss: 0.7852 iter time (s): 2.681 samples/sec: 11.936\n",
      "[2024-11-28 19:22:18,318] [INFO] [logging.py:96:log_dist] [Rank 0] step=39, skipped=0, lr=[3.0282207244334084e-06, 3.0282207244334084e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 39 loss: 0.5781 iter time (s): 2.703 samples/sec: 11.840\n",
      "[2024-11-28 19:22:20,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[2.7980292422118282e-06, 2.7980292422118282e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 40 loss: 0.7539 iter time (s): 2.645 samples/sec: 12.097\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --module jllm.train_pipe \\\n",
    "    --model pretrained_hf \\\n",
    "    --num_train_epochs 2 \\\n",
    "    --train_data sample_Qwen2.5-7B-Instruct \\\n",
    "    --pipe_parallel_size 4 \\\n",
    "    --model_parallel_size 1 \\\n",
    "    --sequence_parallel_size 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --global_batch_size 16 \\\n",
    "    --partition_method fast \\\n",
    "    --split_dlayer \\\n",
    "    --only_ckpt_model \\\n",
    "    --max_num_checkpoints 2 \\\n",
    "    --split_dlayer \\\n",
    "    --learning_rate 1e-5 --low_mem |tee finetuning.log\n",
    "\n",
    "# 注释：\n",
    "# --model 模型路径至少需要包含config.json\n",
    "# --num_train_epochs 训练轮数\n",
    "# --train_data 训练数据\n",
    "# --pipe_parallel_size 流水线并行个数\n",
    "# --model_parallel_size 张量并行个数\n",
    "# --per_device_train_batch_size 一次输入训练多少样本\n",
    "# --global_batch_size 全局训练完多少样本后（累加完多少个梯度后）进行一次参数更新\n",
    "# --partition_method fast 流水线拆分策略\n",
    "# --only_ckpt_model 只check模型参数，此时会直接存成huggingface格式\n",
    "# --checkpoint checkpoint 模型检查点目录\n",
    "# --output_dir pretrained 最终模型输出目录\n",
    "# --max_num_checkpoints 2 最大保留多少个检查点\n",
    "# --split_dlayer 是否拆分docoder layer\n",
    "# --low_mem 是否使用低内存读取数据\n",
    "# --learning_rate 1e-5 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e87f593f-99d9-4d1e-a06a-769da2c35085",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!grep  'steps:.*loss:' finetuning.log|awk '{print $2,$4}'>loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xy=np.loadtxt('loss')  \n",
    "plt.plot(xy[:,0], xy[:,1])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819336d8-dc0d-42ca-b347-eb3d947004d1",
   "metadata": {},
   "source": [
    "#### 4.2 LORA微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58818709-18f3-4e42-9a06-ccecadf1aee1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-14 21:58:58,632] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-14 21:59:00,052] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-11-14 21:59:00,053] [INFO] [runner.py:585:main] cmd = /home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --module --enable_each_rank_log=None jllm.train_pipe --model pretrained_hf --num_train_epochs 3 --train_data sample_Qwen2.5-7B-Instruct --pipe_parallel_size 4 --model_parallel_size 1 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --partition_method fast --split_dlayer --only_ckpt_lora --checkpoint checkpoint_lora --max_num_checkpoints 2 --skip_epoch 1,2 --split_dlayer --lora_dim 32 --lora_alpha 32 --lora_module_name qkv_proj,o_proj,gate_up_proj,down_proj --only_optimize_lora --learning_rate 1e-5 --low_mem\n",
      "[2024-11-14 21:59:05,153] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-14 21:59:06,594] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2024-11-14 21:59:06,594] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2024-11-14 21:59:06,594] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2024-11-14 21:59:06,594] [INFO] [launch.py:164:main] dist_world_size=8\n",
      "[2024-11-14 21:59:06,594] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "[2024-11-14 21:59:06,613] [INFO] [launch.py:256:main] process 456800 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=0', '--model', 'pretrained_hf', '--num_train_epochs', '3', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_lora', '--checkpoint', 'checkpoint_lora', '--max_num_checkpoints', '2', '--skip_epoch', '1,2', '--split_dlayer', '--lora_dim', '32', '--lora_alpha', '32', '--lora_module_name', 'qkv_proj,o_proj,gate_up_proj,down_proj', '--only_optimize_lora', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-14 21:59:06,627] [INFO] [launch.py:256:main] process 456801 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=1', '--model', 'pretrained_hf', '--num_train_epochs', '3', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_lora', '--checkpoint', 'checkpoint_lora', '--max_num_checkpoints', '2', '--skip_epoch', '1,2', '--split_dlayer', '--lora_dim', '32', '--lora_alpha', '32', '--lora_module_name', 'qkv_proj,o_proj,gate_up_proj,down_proj', '--only_optimize_lora', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-14 21:59:06,641] [INFO] [launch.py:256:main] process 456802 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=2', '--model', 'pretrained_hf', '--num_train_epochs', '3', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_lora', '--checkpoint', 'checkpoint_lora', '--max_num_checkpoints', '2', '--skip_epoch', '1,2', '--split_dlayer', '--lora_dim', '32', '--lora_alpha', '32', '--lora_module_name', 'qkv_proj,o_proj,gate_up_proj,down_proj', '--only_optimize_lora', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-14 21:59:06,656] [INFO] [launch.py:256:main] process 456803 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=3', '--model', 'pretrained_hf', '--num_train_epochs', '3', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_lora', '--checkpoint', 'checkpoint_lora', '--max_num_checkpoints', '2', '--skip_epoch', '1,2', '--split_dlayer', '--lora_dim', '32', '--lora_alpha', '32', '--lora_module_name', 'qkv_proj,o_proj,gate_up_proj,down_proj', '--only_optimize_lora', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-14 21:59:06,671] [INFO] [launch.py:256:main] process 456804 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=4', '--model', 'pretrained_hf', '--num_train_epochs', '3', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_lora', '--checkpoint', 'checkpoint_lora', '--max_num_checkpoints', '2', '--skip_epoch', '1,2', '--split_dlayer', '--lora_dim', '32', '--lora_alpha', '32', '--lora_module_name', 'qkv_proj,o_proj,gate_up_proj,down_proj', '--only_optimize_lora', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-14 21:59:06,688] [INFO] [launch.py:256:main] process 456805 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=5', '--model', 'pretrained_hf', '--num_train_epochs', '3', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_lora', '--checkpoint', 'checkpoint_lora', '--max_num_checkpoints', '2', '--skip_epoch', '1,2', '--split_dlayer', '--lora_dim', '32', '--lora_alpha', '32', '--lora_module_name', 'qkv_proj,o_proj,gate_up_proj,down_proj', '--only_optimize_lora', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-14 21:59:06,717] [INFO] [launch.py:256:main] process 456806 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=6', '--model', 'pretrained_hf', '--num_train_epochs', '3', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_lora', '--checkpoint', 'checkpoint_lora', '--max_num_checkpoints', '2', '--skip_epoch', '1,2', '--split_dlayer', '--lora_dim', '32', '--lora_alpha', '32', '--lora_module_name', 'qkv_proj,o_proj,gate_up_proj,down_proj', '--only_optimize_lora', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-14 21:59:06,748] [INFO] [launch.py:256:main] process 456807 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=7', '--model', 'pretrained_hf', '--num_train_epochs', '3', '--train_data', 'sample_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '4', '--model_parallel_size', '1', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--split_dlayer', '--only_ckpt_lora', '--checkpoint', 'checkpoint_lora', '--max_num_checkpoints', '2', '--skip_epoch', '1,2', '--split_dlayer', '--lora_dim', '32', '--lora_alpha', '32', '--lora_module_name', 'qkv_proj,o_proj,gate_up_proj,down_proj', '--only_optimize_lora', '--learning_rate', '1e-5', '--low_mem']\n",
      "[2024-11-14 21:59:11,921] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-14 21:59:12,151] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-14 21:59:12,253] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-14 21:59:12,260] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-14 21:59:12,365] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-14 21:59:12,371] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "[2024-11-14 21:59:12,376] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-14 21:59:12,437] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "FlashAttention is not installed.\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:209: ImportWarning: \n",
      "    *************************************************************************************************************\n",
      "    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..\n",
      "    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..\n",
      "    The backend in torch.distributed.init_process_group set to hccl now..\n",
      "    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..\n",
      "    The device parameters have been replaced with npu in the function below:\n",
      "    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty\n",
      "    *************************************************************************************************************\n",
      "    \n",
      "  warnings.warn(msg, ImportWarning)\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "[2024-11-14 21:59:27,565] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-14 21:59:27,569] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl\n",
      "[2024-11-14 21:59:28,463] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-14 21:59:28,645] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-14 21:59:28,958] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-14 21:59:28,971] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-14 21:59:29,066] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-14 21:59:29,690] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-14 21:59:29,850] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='sample_Qwen2.5-7B-Instruct/.sample-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(data=0, pipe=0, model=0): 0, ProcessCoord(data=0, pipe=1, model=0): 1, ProcessCoord(data=0, pipe=2, model=0): 2, ProcessCoord(data=0, pipe=3, model=0): 3, ProcessCoord(data=1, pipe=0, model=0): 4, ProcessCoord(data=1, pipe=1, model=0): 5, ProcessCoord(data=1, pipe=2, model=0): 6, ProcessCoord(data=1, pipe=3, model=0): 7}\n",
      "[2024-11-14 21:59:34,108] [INFO] [train_pipe.py:475:custom_partition_layers] Partitioning pipeline stages with method 0, 1, 2, 3, 4\n",
      "stage=0 layers=1\n",
      "     0: LlamaStartPipe 15\n",
      "stage=1 layers=1\n",
      "     1: LlamaMiddlePipe 15\n",
      "stage=2 layers=1\n",
      "     2: LlamaMiddlePipe 16\n",
      "stage=3 layers=1\n",
      "     3: LlamaEndPipe 10\n",
      "  loss: LlamaCrossEntropyLoss\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-14 22:00:07,195] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-14 22:00:07,752] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 22:00:07,865] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-14 22:00:08,020] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-14 22:00:08,076] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-14 22:00:08,078] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 22:00:08,100] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-14 22:00:08,715] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-14 22:00:08,806] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-14 22:00:08,889] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "[2024-11-14 22:00:08,915] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-14 22:00:09,033] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-14 22:00:09,099] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-14 22:00:09,184] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-14 22:00:09,185] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-14 22:00:09,395] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-14 22:00:09,396] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-14 22:00:09,397] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-14 22:00:09,399] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2024-11-14 22:00:09,399] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer\n",
      "[2024-11-14 22:00:09,410] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-14 22:00:09,713] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer\n",
      "[2024-11-14 22:00:09,714] [INFO] [utils.py:782:see_memory_usage] MA 4.19 GB         Max_MA 8.54 GB         CA 6.52 GB         Max_CA 9 GB \n",
      "[2024-11-14 22:00:09,715] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.86 GB, percent = 7.4%\n",
      "[2024-11-14 22:00:09,963] [INFO] [utils.py:781:see_memory_usage] before initializing group 0\n",
      "[2024-11-14 22:00:09,965] [INFO] [utils.py:782:see_memory_usage] MA 4.19 GB         Max_MA 4.19 GB         CA 6.52 GB         Max_CA 7 GB \n",
      "[2024-11-14 22:00:09,965] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.86 GB, percent = 7.4%\n",
      "[2024-11-14 22:00:10,226] [INFO] [utils.py:781:see_memory_usage] after initializing group 0\n",
      "[2024-11-14 22:00:10,227] [INFO] [utils.py:782:see_memory_usage] MA 4.29 GB         Max_MA 4.29 GB         CA 6.52 GB         Max_CA 7 GB \n",
      "[2024-11-14 22:00:10,227] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.86 GB, percent = 7.4%\n",
      "[2024-11-14 22:00:10,478] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer\n",
      "[2024-11-14 22:00:10,479] [INFO] [utils.py:782:see_memory_usage] MA 4.29 GB         Max_MA 4.29 GB         CA 6.52 GB         Max_CA 7 GB \n",
      "[2024-11-14 22:00:10,479] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 74.86 GB, percent = 7.4%\n",
      "[2024-11-14 22:00:10,479] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = BF16_Optimizer\n",
      "[2024-11-14 22:00:10,479] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-14 22:00:10,479] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0xfffef8025b50>\n",
      "[2024-11-14 22:00:10,480] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05], mom=[(0.9, 0.95)]\n",
      "[2024-11-14 22:00:10,480] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-14 22:00:10,481] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xfffef8025a30>\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-14 22:00:10,482] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 16\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   loss_scale ................... 1.0\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-14 22:00:10,483] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   steps_per_print .............. 1\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   train_batch_size ............. 32\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-14 22:00:10,484] [INFO] [config.py:1003:print]   zero_enabled ................. False\n",
      "[2024-11-14 22:00:10,485] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-14 22:00:10,485] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0\n",
      "[2024-11-14 22:00:10,485] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_batch_size\": 32, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": 1, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": false, \n",
      "        \"cpu_checkpointing\": false, \n",
      "        \"contiguous_memory_optimization\": false, \n",
      "        \"number_checkpoints\": null, \n",
      "        \"synchronize_checkpoint_boundary\": false, \n",
      "        \"profile\": false\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }\n",
      "}\n",
      "[2024-11-14 22:00:10,485] [INFO] [engine.py:105:__init__] CONFIG: micro_batches=16 micro_batch_size=1\n",
      "[2024-11-14 22:00:10,485] [INFO] [engine.py:146:__init__] is_pipe_partitioned= False is_grad_partitioned= False\n",
      "[2024-11-14 22:00:11,036] [INFO] [engine.py:165:__init__] RANK=0 STAGE=0 LAYERS=1 [0, 1) STAGE_PARAMS=18268161 (18.268M) TOTAL_PARAMS=71106564 (71.107M) UNIQUE_PARAMS=71106564 (71.107M)\n",
      "[2024-11-14 22:00:11,036] [INFO] [engine.py:165:__init__] RANK=1 STAGE=1 LAYERS=1 [1, 2) STAGE_PARAMS=19824641 (19.825M) TOTAL_PARAMS=71106564 (71.107M) UNIQUE_PARAMS=71106564 (71.107M)\n",
      "[2024-11-14 22:00:11,036] [INFO] [engine.py:165:__init__] RANK=2 STAGE=2 LAYERS=1 [2, 3) STAGE_PARAMS=20316161 (20.316M) TOTAL_PARAMS=71106564 (71.107M) UNIQUE_PARAMS=71106564 (71.107M)\n",
      "[2024-11-14 22:00:11,036] [INFO] [engine.py:165:__init__] RANK=3 STAGE=3 LAYERS=1 [3, 4) STAGE_PARAMS=12697601 (12.698M) TOTAL_PARAMS=71106564 (71.107M) UNIQUE_PARAMS=71106564 (71.107M)\n",
      "Namespace(local_rank=0, model='pretrained_hf', train_data='sample_Qwen2.5-7B-Instruct', eval_data='', from_ckpt='', resume_ckpt='', only_load_model=False, tag=None, ds_config=None, zero_stage=0, split_dlayer=True, num_layers_per_decoder=None, emb_partitions=1, timeout=1800, pipe_parallel_size=4, encoder_pipe_parallel_size=0, model_parallel_size=1, offload=False, partition_method='fast', multi_layerspec=False, num_train_epochs=3, per_device_train_batch_size=1, gradient_accumulation_steps=16, weight_decay=0.0, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=0, learning_rate=1e-05, seq_len=195, block_mask=False, skip_train_steps=set(), steps_per_print=1, steps_per_eval=-1, steps_per_checkpoint=-1, checkpoint='checkpoint_lora', background_executor='process', ckpt_step_gt=0, best_of=1, ckpt_epoch=set(), skip_epoch={1, 2}, max_num_checkpoints=2, only_ckpt_model=True, only_ckpt_lora=True, only_cache_model=False, early_stop=-1, checkpoint_grad_interval=0, no_checkpoint_grad_step=1000000, low_mem=True, no_shuf=False, no_safetensor=False, init=False, cache_model=None, seed=1234, output_dir='', lora_dim=32, lora_alpha=32, lora_module_name='qkv_proj,o_proj,gate_up_proj,down_proj', only_optimize_lora=True, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None, device='npu', global_rank=0, world_size=8, data_parallel_size=2, num_training_steps=93, block_class_id=-100)\n",
      "Data Reading Info:\n",
      "Rank\tSamples\tLength\tReadTime\n",
      "0\t1005\t195\t1\n",
      "4\t1005\t195\t1\n",
      "Beginning of Epoch: 1/3\n",
      "Partition Rank: 1/1\n",
      "Partition Name: sample_Qwen2.5-7B-Instruct/sample-00000\n",
      "Total Partition Steps: 31/93\n",
      "[2024-11-14 22:00:23,362] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[9.99714745464859e-06], mom=[(0.9, 0.95)]\n",
      "steps: 1 loss: 3.2031 iter time (s): 7.774 samples/sec: 4.116\n",
      "[2024-11-14 22:00:24,975] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[9.988593073400354e-06], mom=[(0.9, 0.95)]\n",
      "steps: 2 loss: 3.3906 iter time (s): 0.980 samples/sec: 32.640\n",
      "[2024-11-14 22:00:25,960] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[9.974346616959476e-06], mom=[(0.9, 0.95)]\n",
      "steps: 3 loss: 2.6875 iter time (s): 0.983 samples/sec: 32.549\n",
      "[2024-11-14 22:00:26,913] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[9.954424340791195e-06], mom=[(0.9, 0.95)]\n",
      "steps: 4 loss: 2.3125 iter time (s): 0.954 samples/sec: 33.559\n",
      "[2024-11-14 22:00:27,815] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[9.92884897657402e-06], mom=[(0.9, 0.95)]\n",
      "steps: 5 loss: 1.8125 iter time (s): 0.898 samples/sec: 35.645\n",
      "[2024-11-14 22:00:28,753] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[9.897649706262474e-06], mom=[(0.9, 0.95)]\n",
      "steps: 6 loss: 1.6016 iter time (s): 0.936 samples/sec: 34.193\n",
      "[2024-11-14 22:00:29,643] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[9.860862128789954e-06], mom=[(0.9, 0.95)]\n",
      "steps: 7 loss: 1.5156 iter time (s): 0.894 samples/sec: 35.800\n",
      "[2024-11-14 22:00:30,527] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[9.818528219449705e-06], mom=[(0.9, 0.95)]\n",
      "steps: 8 loss: 1.5938 iter time (s): 0.884 samples/sec: 36.219\n",
      "[2024-11-14 22:00:31,430] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[9.770696282000245e-06], mom=[(0.9, 0.95)]\n",
      "steps: 9 loss: 1.7656 iter time (s): 0.901 samples/sec: 35.498\n",
      "[2024-11-14 22:00:32,310] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[9.717420893549902e-06], mom=[(0.9, 0.95)]\n",
      "steps: 10 loss: 1.8672 iter time (s): 0.878 samples/sec: 36.434\n",
      "[2024-11-14 22:00:33,178] [INFO] [logging.py:96:log_dist] [Rank 0] step=11, skipped=0, lr=[9.658762842283343e-06], mom=[(0.9, 0.95)]\n",
      "steps: 11 loss: 1.5000 iter time (s): 0.861 samples/sec: 37.146\n",
      "[2024-11-14 22:00:34,000] [INFO] [logging.py:96:log_dist] [Rank 0] step=12, skipped=0, lr=[9.594789058101154e-06], mom=[(0.9, 0.95)]\n",
      "steps: 12 loss: 1.5391 iter time (s): 0.826 samples/sec: 38.749\n",
      "[2024-11-14 22:00:34,795] [INFO] [logging.py:96:log_dist] [Rank 0] step=13, skipped=0, lr=[9.525572536251608e-06], mom=[(0.9, 0.95)]\n",
      "steps: 13 loss: 1.6250 iter time (s): 0.789 samples/sec: 40.552\n",
      "[2024-11-14 22:00:35,682] [INFO] [logging.py:96:log_dist] [Rank 0] step=14, skipped=0, lr=[9.451192254041759e-06], mom=[(0.9, 0.95)]\n",
      "steps: 14 loss: 1.5703 iter time (s): 0.891 samples/sec: 35.914\n",
      "[2024-11-14 22:00:36,524] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[9.371733080722911e-06], mom=[(0.9, 0.95)]\n",
      "steps: 15 loss: 1.7969 iter time (s): 0.836 samples/sec: 38.277\n",
      "[2024-11-14 22:00:37,347] [INFO] [logging.py:96:log_dist] [Rank 0] step=16, skipped=0, lr=[9.287285680653254e-06], mom=[(0.9, 0.95)]\n",
      "steps: 16 loss: 1.5938 iter time (s): 0.822 samples/sec: 38.948\n",
      "[2024-11-14 22:00:38,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=17, skipped=0, lr=[9.197946409848196e-06], mom=[(0.9, 0.95)]\n",
      "steps: 17 loss: 1.4453 iter time (s): 0.803 samples/sec: 39.857\n",
      "[2024-11-14 22:00:39,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=18, skipped=0, lr=[9.103817206036383e-06], mom=[(0.9, 0.95)]\n",
      "steps: 18 loss: 1.5312 iter time (s): 0.863 samples/sec: 37.090\n",
      "[2024-11-14 22:00:39,823] [INFO] [logging.py:96:log_dist] [Rank 0] step=19, skipped=0, lr=[9.005005472346923e-06], mom=[(0.9, 0.95)]\n",
      "steps: 19 loss: 1.5625 iter time (s): 0.807 samples/sec: 39.639\n",
      "[2024-11-14 22:00:40,632] [INFO] [logging.py:96:log_dist] [Rank 0] step=20, skipped=0, lr=[8.90162395476046e-06], mom=[(0.9, 0.95)]\n",
      "steps: 20 loss: 1.7109 iter time (s): 0.811 samples/sec: 39.472\n",
      "[2024-11-14 22:00:41,484] [INFO] [logging.py:96:log_dist] [Rank 0] step=21, skipped=0, lr=[8.793790613463956e-06], mom=[(0.9, 0.95)]\n",
      "steps: 21 loss: 1.6172 iter time (s): 0.850 samples/sec: 37.631\n",
      "[2024-11-14 22:00:42,333] [INFO] [logging.py:96:log_dist] [Rank 0] step=22, skipped=0, lr=[8.681628488255986e-06], mom=[(0.9, 0.95)]\n",
      "steps: 22 loss: 1.5312 iter time (s): 0.846 samples/sec: 37.804\n",
      "[2024-11-14 22:00:43,191] [INFO] [logging.py:96:log_dist] [Rank 0] step=23, skipped=0, lr=[8.565265558156101e-06], mom=[(0.9, 0.95)]\n",
      "steps: 23 loss: 1.6562 iter time (s): 0.855 samples/sec: 37.433\n",
      "[2024-11-14 22:00:44,063] [INFO] [logging.py:96:log_dist] [Rank 0] step=24, skipped=0, lr=[8.444834595378434e-06], mom=[(0.9, 0.95)]\n",
      "steps: 24 loss: 1.7500 iter time (s): 0.872 samples/sec: 36.710\n",
      "[2024-11-14 22:00:44,871] [INFO] [logging.py:96:log_dist] [Rank 0] step=25, skipped=0, lr=[8.320473013836197e-06], mom=[(0.9, 0.95)]\n",
      "steps: 25 loss: 1.6797 iter time (s): 0.811 samples/sec: 39.458\n",
      "[2024-11-14 22:00:45,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=26, skipped=0, lr=[8.192322712349917e-06], mom=[(0.9, 0.95)]\n",
      "steps: 26 loss: 1.8906 iter time (s): 0.828 samples/sec: 38.664\n",
      "[2024-11-14 22:00:46,519] [INFO] [logging.py:96:log_dist] [Rank 0] step=27, skipped=0, lr=[8.060529912738316e-06], mom=[(0.9, 0.95)]\n",
      "steps: 27 loss: 1.5625 iter time (s): 0.822 samples/sec: 38.942\n",
      "[2024-11-14 22:00:47,297] [INFO] [logging.py:96:log_dist] [Rank 0] step=28, skipped=0, lr=[7.925244992976538e-06], mom=[(0.9, 0.95)]\n",
      "steps: 28 loss: 1.7031 iter time (s): 0.772 samples/sec: 41.439\n",
      "[2024-11-14 22:00:48,088] [INFO] [logging.py:96:log_dist] [Rank 0] step=29, skipped=0, lr=[7.786622315612182e-06], mom=[(0.9, 0.95)]\n",
      "steps: 29 loss: 1.4844 iter time (s): 0.785 samples/sec: 40.756\n",
      "[2024-11-14 22:00:48,908] [INFO] [logging.py:96:log_dist] [Rank 0] step=30, skipped=0, lr=[7.644820051634813e-06], mom=[(0.9, 0.95)]\n",
      "steps: 30 loss: 1.6250 iter time (s): 0.818 samples/sec: 39.114\n",
      "[2024-11-14 22:00:49,712] [INFO] [logging.py:96:log_dist] [Rank 0] step=31, skipped=0, lr=[7.500000000000001e-06], mom=[(0.9, 0.95)]\n",
      "steps: 31 loss: 1.7344 iter time (s): 0.803 samples/sec: 39.862\n",
      "Wash the memory of train data clean for 1 seconds ......\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "Data Reading Info:\n",
      "Rank\tSamples\tLength\tReadTime\n",
      "0\t1005\t195\t1\n",
      "4\t1005\t195\t1\n",
      "Beginning of Epoch: 2/3\n",
      "Partition Rank: 1/1\n",
      "Partition Name: sample_Qwen2.5-7B-Instruct/sample-00000\n",
      "Total Partition Steps: 62/93\n",
      "[2024-11-14 22:00:59,931] [INFO] [logging.py:96:log_dist] [Rank 0] step=32, skipped=0, lr=[7.352327403013779e-06], mom=[(0.9, 0.95)]\n",
      "steps: 32 loss: 1.5000 iter time (s): 1.184 samples/sec: 27.028\n",
      "[2024-11-14 22:01:00,734] [INFO] [logging.py:96:log_dist] [Rank 0] step=33, skipped=0, lr=[7.201970757788172e-06], mom=[(0.9, 0.95)]\n",
      "steps: 33 loss: 1.5234 iter time (s): 0.801 samples/sec: 39.944\n",
      "[2024-11-14 22:01:01,607] [INFO] [logging.py:96:log_dist] [Rank 0] step=34, skipped=0, lr=[7.049101623982938e-06], mom=[(0.9, 0.95)]\n",
      "steps: 34 loss: 1.6562 iter time (s): 0.877 samples/sec: 36.480\n",
      "[2024-11-14 22:01:02,431] [INFO] [logging.py:96:log_dist] [Rank 0] step=35, skipped=0, lr=[6.893894428052881e-06], mom=[(0.9, 0.95)]\n",
      "steps: 35 loss: 1.5000 iter time (s): 0.822 samples/sec: 38.929\n",
      "[2024-11-14 22:01:03,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=36, skipped=0, lr=[6.736526264224101e-06], mom=[(0.9, 0.95)]\n",
      "steps: 36 loss: 1.4922 iter time (s): 0.837 samples/sec: 38.215\n",
      "[2024-11-14 22:01:04,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=37, skipped=0, lr=[6.5771766924262795e-06], mom=[(0.9, 0.95)]\n",
      "steps: 37 loss: 1.4531 iter time (s): 0.847 samples/sec: 37.802\n",
      "[2024-11-14 22:01:04,994] [INFO] [logging.py:96:log_dist] [Rank 0] step=38, skipped=0, lr=[6.41602753341152e-06], mom=[(0.9, 0.95)]\n",
      "steps: 38 loss: 1.4375 iter time (s): 0.871 samples/sec: 36.752\n",
      "[2024-11-14 22:01:05,838] [INFO] [logging.py:96:log_dist] [Rank 0] step=39, skipped=0, lr=[6.2532626612936035e-06], mom=[(0.9, 0.95)]\n",
      "steps: 39 loss: 1.3047 iter time (s): 0.845 samples/sec: 37.849\n",
      "[2024-11-14 22:01:06,640] [INFO] [logging.py:96:log_dist] [Rank 0] step=40, skipped=0, lr=[6.089067793744258e-06], mom=[(0.9, 0.95)]\n",
      "steps: 40 loss: 1.3750 iter time (s): 0.798 samples/sec: 40.083\n",
      "[2024-11-14 22:01:07,490] [INFO] [logging.py:96:log_dist] [Rank 0] step=41, skipped=0, lr=[5.923630280085948e-06], mom=[(0.9, 0.95)]\n",
      "steps: 41 loss: 1.5625 iter time (s): 0.847 samples/sec: 37.791\n",
      "[2024-11-14 22:01:08,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=42, skipped=0, lr=[5.757138887522884e-06], mom=[(0.9, 0.95)]\n",
      "steps: 42 loss: 1.7031 iter time (s): 0.771 samples/sec: 41.501\n",
      "[2024-11-14 22:01:09,062] [INFO] [logging.py:96:log_dist] [Rank 0] step=43, skipped=0, lr=[5.5897835857542315e-06], mom=[(0.9, 0.95)]\n",
      "steps: 43 loss: 1.3594 iter time (s): 0.803 samples/sec: 39.872\n",
      "[2024-11-14 22:01:09,835] [INFO] [logging.py:96:log_dist] [Rank 0] step=44, skipped=0, lr=[5.421755330215223e-06], mom=[(0.9, 0.95)]\n",
      "steps: 44 loss: 1.5156 iter time (s): 0.769 samples/sec: 41.601\n",
      "[2024-11-14 22:01:10,702] [INFO] [logging.py:96:log_dist] [Rank 0] step=45, skipped=0, lr=[5.253245844193564e-06], mom=[(0.9, 0.95)]\n",
      "steps: 45 loss: 1.3672 iter time (s): 0.870 samples/sec: 36.790\n",
      "[2024-11-14 22:01:11,681] [INFO] [logging.py:96:log_dist] [Rank 0] step=46, skipped=0, lr=[5.084447400069656e-06], mom=[(0.9, 0.95)]\n",
      "steps: 46 loss: 1.4688 iter time (s): 0.979 samples/sec: 32.700\n",
      "[2024-11-14 22:01:12,567] [INFO] [logging.py:96:log_dist] [Rank 0] step=47, skipped=0, lr=[4.915552599930345e-06], mom=[(0.9, 0.95)]\n",
      "steps: 47 loss: 1.4062 iter time (s): 0.879 samples/sec: 36.411\n",
      "[2024-11-14 22:01:13,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=48, skipped=0, lr=[4.746754155806437e-06], mom=[(0.9, 0.95)]\n",
      "steps: 48 loss: 1.4219 iter time (s): 0.891 samples/sec: 35.905\n",
      "[2024-11-14 22:01:14,309] [INFO] [logging.py:96:log_dist] [Rank 0] step=49, skipped=0, lr=[4.5782446697847775e-06], mom=[(0.9, 0.95)]\n",
      "steps: 49 loss: 1.5000 iter time (s): 0.848 samples/sec: 37.752\n",
      "[2024-11-14 22:01:15,164] [INFO] [logging.py:96:log_dist] [Rank 0] step=50, skipped=0, lr=[4.410216414245771e-06], mom=[(0.9, 0.95)]\n",
      "steps: 50 loss: 1.4531 iter time (s): 0.857 samples/sec: 37.350\n",
      "[2024-11-14 22:01:16,003] [INFO] [logging.py:96:log_dist] [Rank 0] step=51, skipped=0, lr=[4.2428611124771184e-06], mom=[(0.9, 0.95)]\n",
      "steps: 51 loss: 1.3750 iter time (s): 0.835 samples/sec: 38.318\n",
      "[2024-11-14 22:01:16,879] [INFO] [logging.py:96:log_dist] [Rank 0] step=52, skipped=0, lr=[4.076369719914055e-06], mom=[(0.9, 0.95)]\n",
      "steps: 52 loss: 1.4844 iter time (s): 0.875 samples/sec: 36.578\n",
      "[2024-11-14 22:01:17,722] [INFO] [logging.py:96:log_dist] [Rank 0] step=53, skipped=0, lr=[3.910932206255742e-06], mom=[(0.9, 0.95)]\n",
      "steps: 53 loss: 1.6406 iter time (s): 0.842 samples/sec: 38.023\n",
      "[2024-11-14 22:01:18,560] [INFO] [logging.py:96:log_dist] [Rank 0] step=54, skipped=0, lr=[3.7467373387063973e-06], mom=[(0.9, 0.95)]\n",
      "steps: 54 loss: 1.5781 iter time (s): 0.837 samples/sec: 38.235\n",
      "[2024-11-14 22:01:19,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=55, skipped=0, lr=[3.58397246658848e-06], mom=[(0.9, 0.95)]\n",
      "steps: 55 loss: 1.3750 iter time (s): 0.794 samples/sec: 40.323\n",
      "[2024-11-14 22:01:20,151] [INFO] [logging.py:96:log_dist] [Rank 0] step=56, skipped=0, lr=[3.4228233075737225e-06], mom=[(0.9, 0.95)]\n",
      "steps: 56 loss: 1.5625 iter time (s): 0.794 samples/sec: 40.293\n",
      "[2024-11-14 22:01:20,938] [INFO] [logging.py:96:log_dist] [Rank 0] step=57, skipped=0, lr=[3.2634737357758994e-06], mom=[(0.9, 0.95)]\n",
      "steps: 57 loss: 1.4531 iter time (s): 0.786 samples/sec: 40.717\n",
      "[2024-11-14 22:01:21,805] [INFO] [logging.py:96:log_dist] [Rank 0] step=58, skipped=0, lr=[3.10610557194712e-06], mom=[(0.9, 0.95)]\n",
      "steps: 58 loss: 1.4531 iter time (s): 0.866 samples/sec: 36.968\n",
      "[2024-11-14 22:01:22,718] [INFO] [logging.py:96:log_dist] [Rank 0] step=59, skipped=0, lr=[2.950898376017064e-06], mom=[(0.9, 0.95)]\n",
      "steps: 59 loss: 1.3125 iter time (s): 0.912 samples/sec: 35.072\n",
      "[2024-11-14 22:01:23,530] [INFO] [logging.py:96:log_dist] [Rank 0] step=60, skipped=0, lr=[2.7980292422118282e-06], mom=[(0.9, 0.95)]\n",
      "steps: 60 loss: 1.5000 iter time (s): 0.811 samples/sec: 39.458\n",
      "[2024-11-14 22:01:24,355] [INFO] [logging.py:96:log_dist] [Rank 0] step=61, skipped=0, lr=[2.6476725969862227e-06], mom=[(0.9, 0.95)]\n",
      "steps: 61 loss: 1.4375 iter time (s): 0.823 samples/sec: 38.880\n",
      "[2024-11-14 22:01:25,171] [INFO] [logging.py:96:log_dist] [Rank 0] step=62, skipped=0, lr=[2.5000000000000015e-06], mom=[(0.9, 0.95)]\n",
      "steps: 62 loss: 1.4609 iter time (s): 0.819 samples/sec: 39.069\n",
      "Wash the memory of train data clean for 1 seconds ......\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.91it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.95it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "Data Reading Info:\n",
      "Rank\tSamples\tLength\tReadTime\n",
      "0\t1005\t195\t1\n",
      "4\t1005\t195\t1\n",
      "Beginning of Epoch: 3/3\n",
      "Partition Rank: 1/1\n",
      "Partition Name: sample_Qwen2.5-7B-Instruct/sample-00000\n",
      "Total Partition Steps: 93/93\n",
      "[2024-11-14 22:01:34,864] [INFO] [logging.py:96:log_dist] [Rank 0] step=63, skipped=0, lr=[2.3551799483651894e-06], mom=[(0.9, 0.95)]\n",
      "steps: 63 loss: 1.2891 iter time (s): 1.090 samples/sec: 29.364\n",
      "[2024-11-14 22:01:35,666] [INFO] [logging.py:96:log_dist] [Rank 0] step=64, skipped=0, lr=[2.2133776843878185e-06], mom=[(0.9, 0.95)]\n",
      "steps: 64 loss: 1.4844 iter time (s): 0.797 samples/sec: 40.153\n",
      "[2024-11-14 22:01:36,711] [INFO] [logging.py:96:log_dist] [Rank 0] step=65, skipped=0, lr=[2.074755007023461e-06], mom=[(0.9, 0.95)]\n",
      "steps: 65 loss: 1.4297 iter time (s): 1.044 samples/sec: 30.655\n",
      "[2024-11-14 22:01:37,499] [INFO] [logging.py:96:log_dist] [Rank 0] step=66, skipped=0, lr=[1.9394700872616856e-06], mom=[(0.9, 0.95)]\n",
      "steps: 66 loss: 1.4922 iter time (s): 0.790 samples/sec: 40.511\n",
      "[2024-11-14 22:01:38,269] [INFO] [logging.py:96:log_dist] [Rank 0] step=67, skipped=0, lr=[1.8076772876500831e-06], mom=[(0.9, 0.95)]\n",
      "steps: 67 loss: 1.4375 iter time (s): 0.765 samples/sec: 41.846\n",
      "[2024-11-14 22:01:39,119] [INFO] [logging.py:96:log_dist] [Rank 0] step=68, skipped=0, lr=[1.6795269861638041e-06], mom=[(0.9, 0.95)]\n",
      "steps: 68 loss: 1.3438 iter time (s): 0.850 samples/sec: 37.655\n",
      "[2024-11-14 22:01:39,936] [INFO] [logging.py:96:log_dist] [Rank 0] step=69, skipped=0, lr=[1.555165404621567e-06], mom=[(0.9, 0.95)]\n",
      "steps: 69 loss: 1.4531 iter time (s): 0.815 samples/sec: 39.268\n",
      "[2024-11-14 22:01:40,761] [INFO] [logging.py:96:log_dist] [Rank 0] step=70, skipped=0, lr=[1.434734441843899e-06], mom=[(0.9, 0.95)]\n",
      "steps: 70 loss: 1.3203 iter time (s): 0.826 samples/sec: 38.760\n",
      "[2024-11-14 22:01:41,550] [INFO] [logging.py:96:log_dist] [Rank 0] step=71, skipped=0, lr=[1.3183715117440143e-06], mom=[(0.9, 0.95)]\n",
      "steps: 71 loss: 1.3516 iter time (s): 0.785 samples/sec: 40.790\n",
      "[2024-11-14 22:01:42,340] [INFO] [logging.py:96:log_dist] [Rank 0] step=72, skipped=0, lr=[1.2062093865360458e-06], mom=[(0.9, 0.95)]\n",
      "steps: 72 loss: 1.3750 iter time (s): 0.789 samples/sec: 40.538\n",
      "[2024-11-14 22:01:43,179] [INFO] [logging.py:96:log_dist] [Rank 0] step=73, skipped=0, lr=[1.0983760452395415e-06], mom=[(0.9, 0.95)]\n",
      "steps: 73 loss: 1.3906 iter time (s): 0.837 samples/sec: 38.225\n",
      "[2024-11-14 22:01:44,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=74, skipped=0, lr=[9.949945276530782e-07], mom=[(0.9, 0.95)]\n",
      "steps: 74 loss: 1.2266 iter time (s): 0.832 samples/sec: 38.481\n",
      "[2024-11-14 22:01:44,826] [INFO] [logging.py:96:log_dist] [Rank 0] step=75, skipped=0, lr=[8.961827939636198e-07], mom=[(0.9, 0.95)]\n",
      "steps: 75 loss: 1.6406 iter time (s): 0.813 samples/sec: 39.361\n",
      "[2024-11-14 22:01:45,697] [INFO] [logging.py:96:log_dist] [Rank 0] step=76, skipped=0, lr=[8.02053590151805e-07], mom=[(0.9, 0.95)]\n",
      "steps: 76 loss: 1.3828 iter time (s): 0.870 samples/sec: 36.790\n",
      "[2024-11-14 22:01:46,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=77, skipped=0, lr=[7.127143193467445e-07], mom=[(0.9, 0.95)]\n",
      "steps: 77 loss: 1.3438 iter time (s): 0.850 samples/sec: 37.637\n",
      "[2024-11-14 22:01:47,378] [INFO] [logging.py:96:log_dist] [Rank 0] step=78, skipped=0, lr=[6.282669192770896e-07], mom=[(0.9, 0.95)]\n",
      "steps: 78 loss: 1.4844 iter time (s): 0.829 samples/sec: 38.618\n",
      "[2024-11-14 22:01:48,210] [INFO] [logging.py:96:log_dist] [Rank 0] step=79, skipped=0, lr=[5.488077459582425e-07], mom=[(0.9, 0.95)]\n",
      "steps: 79 loss: 1.4297 iter time (s): 0.830 samples/sec: 38.544\n",
      "[2024-11-14 22:01:49,011] [INFO] [logging.py:96:log_dist] [Rank 0] step=80, skipped=0, lr=[4.7442746374839363e-07], mom=[(0.9, 0.95)]\n",
      "steps: 80 loss: 1.3438 iter time (s): 0.800 samples/sec: 40.010\n",
      "[2024-11-14 22:01:49,819] [INFO] [logging.py:96:log_dist] [Rank 0] step=81, skipped=0, lr=[4.05210941898847e-07], mom=[(0.9, 0.95)]\n",
      "steps: 81 loss: 1.4453 iter time (s): 0.807 samples/sec: 39.667\n",
      "[2024-11-14 22:01:50,604] [INFO] [logging.py:96:log_dist] [Rank 0] step=82, skipped=0, lr=[3.4123715771665786e-07], mom=[(0.9, 0.95)]\n",
      "steps: 82 loss: 1.6719 iter time (s): 0.784 samples/sec: 40.841\n",
      "[2024-11-14 22:01:51,404] [INFO] [logging.py:96:log_dist] [Rank 0] step=83, skipped=0, lr=[2.8257910645009935e-07], mom=[(0.9, 0.95)]\n",
      "steps: 83 loss: 1.5703 iter time (s): 0.799 samples/sec: 40.055\n",
      "[2024-11-14 22:01:52,249] [INFO] [logging.py:96:log_dist] [Rank 0] step=84, skipped=0, lr=[2.2930371799975593e-07], mom=[(0.9, 0.95)]\n",
      "steps: 84 loss: 1.5312 iter time (s): 0.845 samples/sec: 37.885\n",
      "[2024-11-14 22:01:53,078] [INFO] [logging.py:96:log_dist] [Rank 0] step=85, skipped=0, lr=[1.814717805502958e-07], mom=[(0.9, 0.95)]\n",
      "steps: 85 loss: 1.3125 iter time (s): 0.827 samples/sec: 38.680\n",
      "[2024-11-14 22:01:53,904] [INFO] [logging.py:96:log_dist] [Rank 0] step=86, skipped=0, lr=[1.3913787121004717e-07], mom=[(0.9, 0.95)]\n",
      "steps: 86 loss: 1.4766 iter time (s): 0.825 samples/sec: 38.787\n",
      "[2024-11-14 22:01:54,868] [INFO] [logging.py:96:log_dist] [Rank 0] step=87, skipped=0, lr=[1.0235029373752758e-07], mom=[(0.9, 0.95)]\n",
      "steps: 87 loss: 1.2500 iter time (s): 0.962 samples/sec: 33.249\n",
      "[2024-11-14 22:01:55,677] [INFO] [logging.py:96:log_dist] [Rank 0] step=88, skipped=0, lr=[7.115102342598101e-08], mom=[(0.9, 0.95)]\n",
      "steps: 88 loss: 1.2344 iter time (s): 0.815 samples/sec: 39.265\n",
      "[2024-11-14 22:01:56,548] [INFO] [logging.py:96:log_dist] [Rank 0] step=89, skipped=0, lr=[4.55756592088058e-08], mom=[(0.9, 0.95)]\n",
      "steps: 89 loss: 1.3594 iter time (s): 0.862 samples/sec: 37.111\n",
      "[2024-11-14 22:01:57,353] [INFO] [logging.py:96:log_dist] [Rank 0] step=90, skipped=0, lr=[2.5653383040524228e-08], mom=[(0.9, 0.95)]\n",
      "steps: 90 loss: 1.4062 iter time (s): 0.804 samples/sec: 39.779\n",
      "[2024-11-14 22:01:58,218] [INFO] [logging.py:96:log_dist] [Rank 0] step=91, skipped=0, lr=[1.1406926599646373e-08], mom=[(0.9, 0.95)]\n",
      "steps: 91 loss: 1.3125 iter time (s): 0.868 samples/sec: 36.867\n",
      "[2024-11-14 22:01:59,077] [INFO] [logging.py:96:log_dist] [Rank 0] step=92, skipped=0, lr=[2.8525453514099966e-09], mom=[(0.9, 0.95)]\n",
      "steps: 92 loss: 1.3594 iter time (s): 0.851 samples/sec: 37.584\n",
      "[2024-11-14 22:01:59,981] [INFO] [logging.py:96:log_dist] [Rank 0] step=93, skipped=0, lr=[0.0], mom=[(0.9, 0.95)]\n",
      "steps: 93 loss: 1.3750 iter time (s): 0.903 samples/sec: 35.447\n",
      "Rank 7: saving the final model to checkpoint_lora/93: model-00004-of-00004.safetensors in background ...\n",
      "Rank 7: saved the final model to checkpoint_lora/93: model-00004-of-00004.safetensors.\n",
      "Rank 5: saving the final model to checkpoint_lora/93: model-00002-of-00004.safetensors in background ...\n",
      "Rank 2: saving the final model to checkpoint_lora/93: model-00003-of-00004.safetensors in background ...\n",
      "Wash the memory of train data clean for 1 seconds ......\n",
      "Rank 2: saved the final model to checkpoint_lora/93: model-00003-of-00004.safetensors.\n",
      "Rank 5: saved the final model to checkpoint_lora/93: model-00002-of-00004.safetensors.\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      " 80%|███████████████████████████████████▏        | 8/10 [00:00<00:00,  9.96it/s]Rank 0: saving the final model to checkpoint_lora/93: model-00001-of-00004.safetensors in background ...\n",
      " 90%|███████████████████████████████████████▌    | 9/10 [00:00<00:00,  9.96it/s]Rank 0: saved the final model to checkpoint_lora/93: model-00001-of-00004.safetensors.\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "[2024-11-14 22:02:08,929] [INFO] [launch.py:351:main] Process 456803 exits successfully.\n",
      "[2024-11-14 22:02:08,929] [INFO] [launch.py:351:main] Process 456806 exits successfully.\n",
      "[2024-11-14 22:02:08,930] [INFO] [launch.py:351:main] Process 456807 exits successfully.\n",
      "[2024-11-14 22:02:08,930] [INFO] [launch.py:351:main] Process 456802 exits successfully.\n",
      "[2024-11-14 22:02:09,931] [INFO] [launch.py:351:main] Process 456801 exits successfully.\n",
      "[2024-11-14 22:02:11,933] [INFO] [launch.py:351:main] Process 456800 exits successfully.\n",
      "[2024-11-14 22:02:13,935] [INFO] [launch.py:351:main] Process 456805 exits successfully.\n",
      "[2024-11-14 22:02:16,938] [INFO] [launch.py:351:main] Process 456804 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --module jllm.train_pipe \\\n",
    "    --model pretrained_hf \\\n",
    "    --num_train_epochs 3 \\\n",
    "    --train_data sample_Qwen2.5-7B-Instruct \\\n",
    "    --pipe_parallel_size 4 \\\n",
    "    --model_parallel_size 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --global_batch_size 32 \\\n",
    "    --partition_method fast \\\n",
    "    --split_dlayer \\\n",
    "    --only_ckpt_lora \\\n",
    "    --checkpoint checkpoint_lora \\\n",
    "    --max_num_checkpoints 2 \\\n",
    "    --skip_epoch 1,2 \\\n",
    "    --split_dlayer \\\n",
    "    --lora_dim 32 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --lora_module_name 'qkv_proj,o_proj,gate_up_proj,down_proj' \\\n",
    "    --only_optimize_lora \\\n",
    "    --learning_rate 1e-5 --low_mem |tee lora.log\n",
    "\n",
    "#注释：\n",
    "# --model 模型路径至少需要包含config.json\n",
    "# --num_train_epochs 训练轮数\n",
    "# --train_data 训练数据\n",
    "# --pipe_parallel_size 流水线并行个数\n",
    "# --model_parallel_size 张量并行个数\n",
    "# --per_device_train_batch_size 一次输入训练多少样本\n",
    "# --global_batch_size 训练完多少样本后（累加完多少个梯度后）进行一次参数更新\n",
    "# --partition_method 流水线拆分策略\n",
    "# --only_ckpt_model 只check模型参数，此时会直接存成huggingface格式\n",
    "# --checkpoint checkpoint 模型检查点目录\n",
    "# --output_dir pretrained 最终模型输出目录\n",
    "# --max_num_checkpoints 2 最大保留多少个检查点\n",
    "# --skip_epoch 跳过的检查点\n",
    "# --split_dlayer 是否拆分docoder layer\n",
    "# --lora_dim lora参数的秩\n",
    "# --lora_alpha lora参数的权重\n",
    "# --lora_module_name 对哪些线性层执行lora替换\n",
    "# --only_optimize_lora 只对被lora替换的参数进行梯度更新\n",
    "# --low_mem 是否使用低内存读取数据\n",
    "# --learning_rate 1e-5 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "67a9f80e-a498-44cb-9cde-1e52ad24b354",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!grep  'steps:.*loss:' lora.log|awk '{print $2,$4}'>loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xy=np.loadtxt('loss')  \n",
    "plt.plot(xy[:,0], xy[:,1])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b4798d-8c1e-435c-b157-db5cd2c71ec2",
   "metadata": {},
   "source": [
    "## 5 推理测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7884666-eb10-4929-9deb-132e8ba15271",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch_npu\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('Qwen2.5-7B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1fd49ab6-0f49-4a15-8a78-3fa835305cff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3efd31328464f318207e2d0f86ee7ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'checkpoint/31',\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1886d7e-3099-40a7-ae44-bc528f1dcad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "en='PARIS – As the economic crisis deepens and widens, the world has been searching for historical analogies to help us understand what has been happening.'\n",
    "prompt = f'\"{en}\"\\n请将上面的句子翻译成中文(仅输出翻译结果):\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cce277b-0121-41a8-9e99-6e177fb1528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    generated_ids = model.generate(\n",
    "        **model_inputs,\n",
    "        max_new_tokens=128\n",
    "    )\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "    \n",
    "    return tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07407ca7-baa4-490b-8048-dd8314428b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "[W VariableFallbackKernel.cpp:51] Warning: CAUTION: The operator 'aten::isin.Tensor_Tensor_out' is not currently supported on the NPU backend and will fall back to run on the CPU. This may have performance implications. (function npu_cpu_fallback)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'巴黎—随着经济危机不断加深和扩大，世界一直在寻找历史上的类似事件，以帮助我们理解正在发生什么。'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6b035be-e6a1-4289-84ee-be0efc9dc3a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'我是工银智涌大模型。'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer('你是谁？')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b7d39f-3be5-4f24-9b27-062f6c7dc6c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
