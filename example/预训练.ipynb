{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20dbecf0-cd27-4bed-a2cd-7c9e31005806",
   "metadata": {},
   "source": [
    "# 预训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29086589-ef59-4edf-a6e8-69cd6ee2010c",
   "metadata": {},
   "source": [
    "镜像获取：\n",
    "\n",
    "https://support.huaweicloud.com/usermanual-standard-modelarts/docker-modelarts_6022.html\n",
    "\n",
    "```shell\n",
    "docker pull swr.cn-southwest-2.myhuaweicloud.com/atelier/pytorch_2_1_ascend:pytorch_2.1.0-cann_8.0.rc1-py_3.9-euler_2.10.7-aarch64-snt9b-20240521222801-f4f5b88\n",
    "\n",
    "docker run -it --privileged --name=llm -u root --net=host --ipc=host \\\n",
    "--device=/dev/davinci0 \\\n",
    "--device=/dev/davinci1 \\\n",
    "--device=/dev/davinci2 \\\n",
    "--device=/dev/davinci3 \\\n",
    "--device=/dev/davinci4 \\\n",
    "--device=/dev/davinci5 \\\n",
    "--device=/dev/davinci6 \\\n",
    "--device=/dev/davinci7 \\\n",
    "--device=/dev/davinci_manager \\\n",
    "--device=/dev/devmm_svm \\\n",
    "--device=/dev/hisi_hdc \\\n",
    "-v /usr/local/Ascend/driver:/usr/local/Ascend/driver \\\n",
    "-v /usr/local/Ascend/add-ons/:/usr/local/Ascend/add-ons/ \\\n",
    "-v /usr/local/sbin/:/usr/local/sbin/ \\\n",
    "-v /var/log/npu/slog/:/var/log/npu/slog \\\n",
    "-v /var/log/npu/profiling/:/var/log/npu/profiling \\\n",
    "-v /var/log/npu/dump/:/var/log/npu/dump \\\n",
    "-v /var/log/npu/:/usr/slog \\\n",
    "-v /etc/hccn.conf:/etc/hccn.conf \\\n",
    "-v /home/icbc:/mnt \\\n",
    "-w /mnt \\\n",
    "a66637dc4410 \\\n",
    "/bin/bash\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8f06a27-801e-43be-8980-1579f519ec49",
   "metadata": {},
   "source": [
    "### 1 数据预览"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0903320-f0ea-4552-ad73-da2655322c45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"id\": \"13\", \"title\": \"数学\", \"tags\": \"\", \"text\": \"数学是研究数量、结构以及空间等概念及其变化的一门学科，属于形式科学的一种。数学利用抽象化和逻辑推理，从计数、计算、量度、对物体形状及运动的观察发展而成。数学家们拓展这些概念，以公式化新的猜想，以及从选定的公理及定义出发，严谨地推导出一些定理。\\n基础数学的知识与运用是生活中不可或缺的一环。对数学基本概念的完善，早在古埃及、美索不达米亚及古印度历史上的古代数学文本便可观见，而在古希腊那里有更为严谨的处理。从那时开始，数学的发展便持续不断地小幅进展，至16世纪的文艺复兴时期，因为新的科学发现和数学革新两者的交互，致使数学的加速发展，直至今日。数学并成为许多国家及地区的教育中的一部分。\\n数学在许多领域都有应用，包括科学、工程、医学、经济学和金融学等。数学对这些领域的应用通常被称为应用数学，有时亦会激起新的数学发现，并导致全新学科的发展，例如物理学的实质性发展中建立的某些理论激发数学家对于某些问题的不同角度的思考。数学家也研究纯粹数学，就是数学本身的实质性内容，而不以任何实际应用为目标。许多研究虽然以纯粹数学开始，但其过程中也发现许多可用之处。\\n词源.\\n西方语言中“数学”（）一词源自于古希腊语的（），其有“学习”、“学问”、“科学”，还有个较狭义且技术性的意思－「数学研究」，即使在其语源内。其形容词（），意思为「和学习有关的」或「用功的」，亦会被用来指「数学的」。其在英语中表面上的复数形式，及在法语中的表面复数形式'，可溯至拉丁文的中性复数'，由西塞罗译自希腊文复数（），此一希腊语被亚里士多德拿来指「万物皆数」的概念。\\n汉字表示的「数学」一词大约产生于中国宋元时期。多指象数之学，但有时也含有今天上的数学意义，例如，秦九韶的《数学九章》（《永乐大典》记，即《数书九章》也被宋代周密所著的《癸辛杂识》记为《数学大略》）、《数学通轨》（明代柯尚迁著）、《数学钥》（清代杜知耕著）、《数学拾遗》（清代丁取忠撰）。直到1939年，经过中国数学名词审查委员会研究“算学”与“数学”两词的使用状况后，确认以“数学”表示今天意义上的数学含义。\\n历史.\\n数学有着久远的历史。它被认为起源于人类早期的生产活动：中国古代的六艺之一就有「数」，数学一词在西方有希腊语词源（mathematikós），意思是“学问的基础”，源于（máthema，“科学，知识，学问”）。\\n史前的人类就已尝试用自然的法则来衡量物质的多少、时间的长短等抽象的数量关系，比如时间单位有日、季节和年等。算术（加减乘除）也自然而然地产生了。古代的石碑及泥版亦证实了当时已有几何的知识。\\n更进一步则需要写作或其他可记录数字的系统，如符木或于印加帝国内用来储存数据的奇普。历史上曾有过许多不同的记数系统。\\n在最初有历史记录的时候，数学内的主要原理是为了做税务和贸易等相关计算，为了解数字间的关系，为了测量土地，以及为了预测天文事件而形成的。这些可以简单地被概括为数学对数量、结构、空间及时间方面的研究。\\n到了16世纪，算术、初等代数以及三角学等初等数学已大体完备。17世纪变量概念的产生使人们开始研究变化中的量与量的互相关系和图形间的互相变换，微积分的概念也在此时形成。随着数学转向形式化，为研究数学基础而产生的集合论和数理逻辑等也开始发展。数学的重心从求解实际问题转变到对一般形式上的思考。\\n从古至今，数学便一直不断地延展，且与科学有丰富的相互作用，两者的发展都受惠于彼此。在历史上有著许多数学发现，并且直至今日都不断地有新的发现。据米哈伊尔·B·塞甫留克（Mikhail B. Sevryuk）于2006年1月的期刊中所说，「存放于数学评论资料库中论文和书籍的数量自1940年（数学评论的创刊年份）现已超过了一百九十万份，而且每年还增加超过七万五千份。此一学海的绝大部份为新的数学定理及其证明。」\\n形成、纯数学与应用数学及美学.\\n每当有涉及数量、结构、空间及变化等方面的问题时，通常就需要用到数学去解决问题，而这往往也拓展了数学的研究范畴。一开始，数学的运用可见于贸易、土地测量及之后的天文学。今日，所有的科学都存在著值得数学家研究的问题，且数学本身亦给出了许多的问题。牛顿和莱布尼兹是微积分的发明者，费曼发明了费曼路径积分，这是推理及物理洞察二者的产物，而今日的弦理论亦引申出新的数学。一些数学只和生成它的领域有关，且用来解答此领域的更多问题。但一般被一领域生成的数学在其他许多领域内也十分有用，且可以成为一般的数学概念。即使是「最纯的」数学通常亦有实际的用途，此一非比寻常的事实，被1963年诺贝尔物理奖得主维格纳称为「数学在自然科学中不可想像的有效性」。\\n如同大多数的研究领域，科学知识的爆发导致了数学的专业化。主要的分歧为纯数学和应用数学。在应用数学内，又被分成两大领域，并且变成了它们自身的学科——统计学和电脑科学。\\n许多数学家谈论数学的\\\"优美\\\"，其内在的美学及美。「简单」和「一般化」即为美的一种。另外亦包括巧妙的证明，如欧几里得对存在无限多质数的证明；又或者是加快计算的数值方法，如快速傅立叶变换。高德菲·哈罗德·哈代在《一个数学家的自白》一书中表明他相信单单是美学上的意义，就已经足够成为数学研究的正当理由。\\n符号、语言与精确性.\\n我们现今所使用的大部分数学符号在16世纪后才被发明出来的。在此之前，数学以文字的形式书写出来，这种形式会限制了数学的发展。现今的符号使得数学对于专家而言更容易掌握，但初学者却常对此望而却步。它被极度的压缩：少量的符号包含著大量的讯息。如同音乐符号一般，现今的数学符号有明确的语法，并且有效地对讯息作编码，这是其他书写方式难以做到的。符号化和形式化使得数学迅速发展，并帮助各个科学领域建立基础支撑理论。\\n数学语言亦对初学者而言感到困难。如“或”和“只”这些字有著比日常用语更精确的意思。亦困恼著初学者的，如“开放”和“域”等字在数学里有著特别的意思。数学术语亦包括如“同胚”及“可积性”等专有名词。但使用这些特别符号和专有术语是有其原因的：数学需要比日常用语更多的精确性。数学家将此对语言及逻辑精确性的要求称为「严谨」。但在现实应用中，舍弃一些严谨性往往会得到更好的结果。\\n严谨是数学证明中很重要且基本的一部份。数学家希望他们的定理以系统化的推理依著公理被推论下去。这是为了避免依著不可靠的直观而推出错误的「定理」，而这情形在历史上曾出现过许多的例子。在数学中被期许的严谨程度因著时间而不同：希腊人期许著仔细的论证，但在牛顿的时代，所使用的方法则较不严谨。牛顿为了解决问题所做的定义，到了十九世纪才重新以小心的分析及正式的证明来处理。今日，数学家们则持续地在争论电脑协助证明的严谨度。当大量的计算难以被验证时，其证明亦很难说是足够地严谨。\\n公理在传统的思想中是「不证自明的真理」，但这种想法是有问题的。在形式上，公理只是一串符号，其只对可以由公理系统导出的公式之内容有意义。希尔伯特计划即是想将所有的数学放在坚固的公理基础上，但依据哥德尔不完备定理，每一相容且能蕴涵皮亚诺公理的公理系统必含有一不可决定的公式；因而所有数学的最终公理化是不可能的。尽管如此，数学常常被想像成只是某种公理化的集合论，在此意义下，所有数学叙述或证明都可以写成集合论的公式。\\n数学作为科学.\\n卡尔·弗里德里希·高斯称数学为「科学的皇后」。在拉丁原文'，以及其德语'中，对应于「科学」的单字的意思皆为知识（领域）。而实际上，science一词在英语内本来就是这个意思，且无疑问地数学在此意义下确实是一门「科学」。将科学限定在自然科学则是在此之后的事。若认为科学是只指物理的世界时，则数学，或至少是纯数学，不会是一门科学。爱因斯坦曾如此描述：「数学定律越和现实有关，它们越不确定；若它们越是确定的话，它们和现实越不会有关。」许多哲学家相信数学在经验上不具可否证性，且因此不是卡尔·波普尔所定义的科学。但在1930年代时，在数理逻辑上的重大进展显示数学不能归并至逻辑内，且波普尔推断「大部份的数学定律，如物理及生物学一样，是假设演绎的：纯数学因此变得更接近其假设为猜测的自然科学，比它现在看起来更接近。」然而，其他的思想家，如较著名的拉卡托斯，便提供了一个关于数学本身的可否证性版本。\\n另一观点则为某些科学领域（如理论物理）是其公理为尝试著符合现实的数学。而事实上，理论物理学家齐曼（John Ziman）即认为科学是一种公众知识，因此亦包含著数学。在任何的情况下，数学和物理科学的许多领域都有著很多相同的地方，尤其是从假设所得的逻辑推论之探索。直觉和实验在数学和科学的猜想建构上皆扮演著重要的角色。实验数学在数学中的重要性正持续地在增加，且计算和模拟在科学及数学中所扮演的角色也越来越加重，减轻了数学不使用科学方法的缺点。在史蒂芬·沃尔夫勒姆2002年的著作《一种新科学》中他提出，计算数学应被视为其自身的一科学领域来探索。\\n数学家对此的态度并不一致。一些研究应用数学的数学家觉得他们是科学家，而那些研究纯数学的数学家则时常觉得他们是在一门较接近逻辑的领域内工作，且因此基本上是个哲学家。许多数学家认为称他们的工作是一种科学，是低估了其美学方面的重要性，以及其做为七大博雅教育之一的历史；另外亦有人认为若忽略其与科学之间的关联，是假装没看到数学和其在科学与工程之间的交互影响，进而促进了数学在许多科学上的发展此一事实。这两种观点之间的差异在哲学上产生了数学是「被创造」（如艺术）或是「被发现」（如科学）的争议。大学院系划分中常见「科学和数学系」，这指出了这两个领域被看作有紧密联系而非一样。实际上，数学家通常会在大体上与科学家合作，但在细节上却会分开。此争议亦是数学哲学众多议题的其中一个。\\n数学的各领域.\\n如上所述，数学主要的学科最先产生于商业上计算的需要、了解数字间的关系、测量土地及预测天文事件。这四种需要大致地与数量、结构、空间及变化（即算术、代数、几何及分析）等数学上广泛的子领域相关连著。除了上述主要的关注之外，亦有用来探索由数学核心至其他领域上之间的连结的子领域：至逻辑、至集合论（基础）、至不同科学的经验上的数学（应用数学）、及较近代的至不确定性的严格研究。\\n基础与哲学.\\n为了阐明数学基础，数学逻辑和集合论等领域被发展了出来。\\n数学逻辑专注于将数学置在一坚固的公理架构上，并研究此一架构的结果。就数学逻辑本身而言，其为哥德尔第二不完备定理所属的领域，而这或许是逻辑学中最广为流传的成果：总是存在不能被证明的真命题。\\n现代逻辑被分成递归论、模型论和证明论，且和理论电脑科学有著密切的关连性，千禧年大奖难题中的P/NP问题就是理论电脑科学中的著名问题。\\n纯粹数学.\\n数量.\\n数量的研究起于数，一开始为熟悉的自然数及整数与被描述在算术内的自然数及整数的算术运算。整数更深的性质于数论中有详细的研究，此一理论包括了如费马最后定理等著名的结果。数论还包括两个被广为探讨的未解问题：孪生质数猜想及哥德巴赫猜想。\\n当数系更进一步发展时，整数被视为有理数的子集，而有理数则包含于实数中，连续的量即是以实数来表示的。实数则可以被进一步广义化成复数。数的进一步广义化可以持续至包含四元数及八元数。从自然数亦可以推广到超限数，它形式化了计数至无限的这一概念。另一个研究的领域为大小，这个导致了基数和之后对无限的另外一种概念：阿列夫数，它允许无限集合之间的大小可以做有意义的比较。\\n结构.\\n许多如数及函数的集合等数学物件都有著内含的结构。这些物件的结构性质被探讨于群、环、-{zh-cn:域;zh-tw:体}-等抽象系统中，该些物件事实上也就是这样的系统。此为代数的领域。在此有一个很重要的概念，即广义化至向量空间的向量，它于线性代数中被研究。向量的研究结合了数学的三个基本领域：数量、结构及空间。向量分析则将其扩展至第四个基本的领域内，即变化。\\n创立于二十世纪三十年代的法国的布尔巴基学派认为：纯粹数学，是研究抽象结构的理论。\\n结构，就是以初始概念和公理出发的演绎系统。\\n布尔巴基学派认为，有三种基本的抽象结构：代数结构（群，环，域……），序结构（偏序，全序……），拓扑结构（邻域，极限，连通性，维数……）。\\n空间.\\n空间的研究源自于几何－尤其是欧几里得几何。三角学则结合了空间及数，且包含有著名的勾股定理。现今对空间的研究更推广到了更高维的几何、非欧几里得几何（其在广义相对论中扮演著核心的角色）及拓扑学。数和空间在解析几何、微分几何和代数几何中都有著很重要的角色。在微分几何中有著纤维丛及流形上的微积分等概念。在代数几何中有著如多项式方程的解集等几何物件的描述，结合了数和空间的概念；亦有著拓扑群的研究，结合了结构与空间。李群被用来研究空间、结构及变化。在其许多分支中，拓扑学可能是二十世纪数学中有著最大进展的领域，并包含有存在已久的庞加莱猜想，以及有争议的四色定理。庞加莱猜想已在2006年确认由俄罗斯数学家格里戈里·佩雷尔曼证明，而四色定理已在1976年由凯尼斯·阿佩尔和沃夫冈·哈肯用电脑证明，而从来没有由人力来验证过。\\n变化.\\n了解及描述变化在自然科学里是一普遍的议题，而微积分更为研究变化的有利工具。函数诞生于此，作为描述一变化的量的核心概念。对于实数及实变函数的严格研究为实分析，而复分析则为复数的等价领域。黎曼猜想——数学最基本的未决问题之一——便是以复分析来描述的。泛函分析注重在函数的（一般为无限维）空间上。泛函分析的众多应用之一为量子力学。许多的问题很自然地会导出一个量与其变化率之间的关系，而这在微分方程中被研究。在自然界中的许多现象可以被动力系统所描述；混沌理论则是对系统的既不可预测而又是决定的行为作明确的描述。\\n离散数学.\\n离散数学是指对理论电脑科学最有用处的数学领域之总称，这包含有可计算理论、计算复杂性理论及资讯理论。可计算理论检验电脑的不同理论模型之极限，这包含现知最有力的模型——图灵机。复杂性理论研究可以由电脑做为较易处理的程度；有些问题即使理论是可以以电脑解出来，但却因为会花费太多的时间或空间而使得其解答仍然不为实际上可行的，尽管电脑硬体的快速进步。最后，资讯理论专注在可以储存在特定媒介内的资料总量，且因此有压缩及熵等概念。\\n作为一相对较新的领域，离散数学有许多基本的未解问题。其中最有名的为P/NP问题——千禧年大奖难题之一。一般相信此问题的解答是否定的。\\n应用数学.\\n应用数学思考将抽象的数学工具运用在解答科学、工商业及其他领域上之现实问题。应用数学中的一重要领域为统计学，它利用机率论为其工具并允许对含有机会成分的现象进行描述、分析与预测。大部份的实验、调查及观察研究需要统计对其资料的分析。（许多的统计学家并不认为他们是数学家，而比较觉得是合作团体的一份子。）数值分析研究有什么计算方法，可以有效地解决那些人力所限而算不出的数学问题；它亦包含了对计算中舍入误差或其他来源的误差之研究。\\n数学奖项.\\n数学奖通常和其他科学的奖项分开。数学上最有名的奖为菲尔兹奖，创立于1936年，每四年颁奖一次。它通常被认为是数学领域的诺贝尔奖。另一个国际上主要的奖项为阿贝尔奖，创立于2003年。两者都颁奖于特定的工作主题，包括数学新领域的创新或已成熟领域中未解决问题的解答。著名的23个问题，称为希尔伯特的23个问题，于1900年由德国数学家大卫·希尔伯特所提出。这一连串的问题在数学家之间有著极高的名望，且至少有九个问题已经被解答了出来。另一新的七个重要问题，称为千禧年大奖难题，发表于2000年。对其每一个问题的解答都有著一百万美元的奖金，而当中只有一个问题（黎曼猜想）和希尔伯特的问题重复。\"}\n",
      "{\"id\": \"18\", \"title\": \"哲学\", \"tags\": \"\", \"text\": \"哲学是研究普遍的、基本问题的学科，包括存在、知识、价值、理智、心灵、语言、人生、道德等领域。哲学与其他学科不同之处在于哲学有独特之思考方式，例如批判的方式、通常是系统化的方法，并以理性论证为基础。从历史上看，许多单独的学科，例如物理学、生物学等自然科学，或法学、政治学、心理学等社会科学，都曾被视作哲学的一部分或其分支学科。直至其得到后续发展后，才逐渐被视作现代意义上的独立学科。\\n哲学的词义在现代末期发生了变化，成为了今天常见的、更狭隘的含义。在这个新意义上，该术语主要与形而上学、认识论、伦理学和美学等哲学学科相关。除其他主题外，它涵盖了对现实、知识和价值观的理性研究。然而，它不同于其他理性探究学科，例如经验科学和数学。\\n词源.\\n英语词语（）源于古希腊语中的，意思为「爱智慧」，有时也译为「智慧的朋友」，该词由（philos，爱）的派生词（philein，去爱）和（sophia，智慧）组合而成。一般认为，古希腊思想家毕达哥拉斯最先在著作中引入“哲学家”和“哲学”这两个术语。\\n“哲”一词在中国古代指那些善于思辨、学问精深者，如“孔门十哲”、“古圣先哲”等，“哲”或“哲人”意义类似西方近世所谓“哲学家”、“思想家”。1874年，日本启蒙家西周，在《百一新论》中首先用汉语词「哲学」来翻译「philosophy」一词。哲学中的形而上学（英语：metaphysics）的中文名称取自《易经·系辞上传》「形而上者谓之道，形而下者谓之器」一语。\\n哲学的概念.\\n哲学的定义.\\n哲学家们对哲学本身的定义存在分歧。没有共同的共识，这归因于哲学本身的性质是一个开放的哲学问题。许多伟大的哲学家，如柏拉图、黑格尔等，对问题“什么是哲学？”提出了答案，但这些答案在今天不太可能被广泛接受。其中一个原因是哲学的本质本身就是一个哲学问题，因此不应期望得到无争议的答案——如果哲学家们停止争论，哲学家的职业就将结束。\\n英国哲学家罗素对哲学的定义是：\\n胡适在《中国哲学史大纲》中称「凡研究人生切要的问题，从根本上着想，要寻一个根本的解决：这种学问叫做哲学」。\\n虽然哲学源自西方的传统，但许多文明在历史上都存在著一些相似的论题。东亚和南亚的哲学被称之为东方哲学，而北非和中东则因为其和欧洲密切的互动，因此常被视为是西方哲学的一部份。\\n对哲学的主题亦存在许多看法。一些人认为哲学是对问题本身过程的观察。\\n研究基础.\\n古希腊哲学家经常提出问题，他们所提出的问题大概可以归类为三类，这三类问题分别形成了哲学的基础学科——分别是形而上学、伦理学、认识论 。\\n现代哲学上出现\\\"不要求精确理由\\\"之哲学理论，例如\\\"虚假\\\"(认定本质不可知)，这种现象将不可知论(世界上终究有人不能理解的存在)的重要程度提高了。\\n自亚里士多德时代以来，在古典或者现代哲学当中，逻辑通常都扮演着重要的角色。特别是其提出的三段论，对西方哲学发展有着深远影响。\\n分支.\\n主分支.\\n哲学可以分为很多不同的分支，主要包括形而上学、知识论、伦理学、逻辑学和美学。\\n特殊分支.\\n这些分支是应用在其他学科，或者交叉学科的哲学研究。\\n历史.\\n很多人类社群思考过哲学问题并且互相学习建立了各种哲学流派。\\n东方哲学是通过每个地区的历史时期来组织的。西方哲学一般可以分为三个或更多时期，最重要的是古典哲学、中世纪哲学和近代哲学。\\n古典哲学.\\n古印度.\\n印度哲学的历史源远流长，早在吠陀时代已经开始，至公元前6世纪为全盛时期。当时古印度的思想百花齐放，其中最著名的包括佛教创始人释迦牟尼佛、耆那教创始人笩駄摩那、阿耆多·翅舍钦婆罗、波拘陀·迦旃延、富兰那·迦叶、数论派等。\\n中国.\\n中国哲学的主要部分起源东周时期，当时以诸子百家广为人知，以孔子的儒家、老子的道家、墨子的墨家及晚期的法家为代表，还有一些流派例如农家、阴阳家和名家在之后则名声不显。在秦朝焚书坑儒后除了法家、儒家、道家外其他流派都不再活跃。在当代，中国哲学仍然在亚洲文化扮演一定作用，但是学理上仍在争辩中国哲学是否应归为哲学。\\n如牟宗三曾对哲学下定义:「凡是对人性的活动所及，以理智及观念加以反省说明的，便是哲学。」牟宗三认为，中国有数千年文化史，确有哲学。只是中国哲学重视主体性，西方重视客体性。因此如果以西方的逻辑和知识论等对哲学下定义，中国没有这些。但如果以主体性方向对哲学下定义，中国文化就拥有哲学。反之西方对人生的哲学多表现在文学、艺术、音乐等等。就以西方哲学史来说，没有一章特别谈及耶稣。\\n古希腊-罗马.\\n古希腊-罗马哲学是西方哲学的一个时期，时间为公元前6世纪到公元6世纪。它一般被分为三个时期：前苏格拉底时期、柏拉图和亚里士多德的古典希腊时期、和后亚里士多德（或希腊化）时期：有时候会把新柏拉图主义和基督教哲学家们的古典时代晚期加入作为第四个时期。\\n前苏格拉底时期.\\n在公元前6世纪的希腊，西方哲学就从古代神话和诗歌中脱颖而出，逐步开始对宇宙的组成以及本源的思考而开始了独立发展。前苏格拉底时期的自然派哲学家们多关注自然界，被认为是西方最早的哲学家，不管他们认识以及解释世界的方式是否正确，但是他们的想法之所以有别于迷信的原因在于，这些哲学家是以理性辅佐证据的方式归纳出自然界的现象。诸如：\\n公元前5世纪中期，普罗泰戈拉和高尔吉亚等所形成的辩士学派将研究的重点由自然转移到人类本身。认为“人才是万物之本”。他们都不相信有真正的存在和真理。普罗泰戈拉认为是非善恶都是相对于人的感觉而言，而高尔吉亚却认为所有的都是同样的假，这是怀疑论的雏形。 \\n公元前6世纪末，以毕达哥拉斯为主的毕达哥拉斯学派所主张的哲学与前述的观点既相近又有不同。罗马古代的历史上记载毕达哥拉斯第一个称自己为哲学家，或者说是爱智慧。他认为“一切都是数字”。其意思就是说一切事物的实质和结构都是它们所包含的数字关系所决定的。他称平均、秩序和调和是宇宙的三大基调，并以音乐的调和说明宇宙的调和。他所在的学派将宇宙总结为十种性质相异的组合：有限与无限、奇与偶、少与多、左与右、男与女、静与动、直与曲、光明与黑暗、善与恶、方与圆。至此之后，数学的本质及其地位，一直都是哲学的主要问题之一，数学不受观察和实验造成的不确定性影响，而且是通过纯粹的思想加以理解的。\\n其中关于变与不变的关系的争论，真实世界与直觉世界的差别，真理与意见的矛盾，导致产生了认识论的问题。\\n古典希腊时期.\\n在古典希腊时期西方哲学方法的关键特质被建立：依靠诉诸理性和论证，通过一种批判性的方法来接受或建立观点。这包括苏格拉底被称为苏格拉底反诘法或“反驳论证”方法的辩证法，他主要用其来检验例如善良和公平正义的关键道德概念。这种方法将一个问题分解成一系列的疑问，在对疑问的回答中逐步提取想要找到的答案，其极大影响可以从现在使用的科学方法中看出，在科学方法中假说是第一个阶段。\\n苏格拉底没有直接教过人，但之后的柏拉图深受其影响。而其整个哲学思想来源于两大理论：其一，永远不要做坏事；其二，一个内心真正纯洁且正义的人绝不会做相反之事。他认为真理有其客观性，试图推翻智者们以个人主观感觉为真理的思想。然后提出德的概念，以作为人生行事的方向。对于道德是什么的问题，苏格拉底的回复为“知识即道德。”对于知识是何物的问题，他回答说知识是透过理性而得的概念。苏格拉底开创了认识论和伦理学，如此奠定了他的哲学地位。\\n古典希腊时期的的哲学家中柏拉图和亚里士多德对后世的影响力最大，特别是柏拉图被认为是西方哲学的创始人。哲学家阿尔弗雷德·诺思·怀特黑德评价柏拉图：“欧洲哲学传统最被普遍公认的特点，就是它包含了一系列对柏拉图的注脚。我的意思不是怀疑学者们系统体系的思想是提取自柏拉图的著作。我暗示的是那些他们散落的一般思想的财富。”换言之即使数千年后，人们依旧在试著回答他所提出的问题，这也代表著人们依然为这些问题或是这些问题所延伸的更多问题而感到困惑。\\n毕达哥拉斯的思想对柏拉图产生了显著的影响，并通过柏拉图影响了整个西方哲学。柏拉图和亚里士多德作为最早的古典希腊哲学家批判地引用了其它的一些“智者”，当时这些人在希腊被称为“辩士”并在毕达哥拉斯之前相当普遍。从他们的批判看来，在他们的古典时代一个在更高尚地、纯粹地“爱智慧”（真的哲学家）与那些更早更普遍的旅行教师——经常也通过自己的技艺来赚钱——之间的分水岭之后被建立。\\n希腊化时代.\\n亚里士多德死后，整个哲学界陷入了独立时期，称为希腊化哲学时期。因为整个社会和政治陷入混乱。这段时期产生了斯多葛学派和伊壁鸠鲁学派，以及怀疑主义派、新柏拉图派和新毕达哥拉斯主义。这些学派的共同特点是伦理化。斯多葛学派主要是顺应自然和自制。伊壁鸠鲁学派则是把快乐作为生活的本质和善的标准。而新柏拉图派和新毕达哥拉斯派都是带有宗教主义的哲学，并逐渐产生融化基督教和希腊哲学于一体的理论，即为后来的基督教哲学。\\n直到公元529年，罗马皇帝查士丁尼一世尼命令关闭雅典的柏拉图学院。J.B.伯里称一些余下的学院成员逃入了萨珊王朝首都泰西封。\\n古印度.\\n印度哲学是指起源于印度次大陆的哲学思想，包括、佛教哲学、等，这些印度哲学具有一些共同且复杂的起源，都有有关佛法及业的主题，而且都希望达到个人的解放。这些哲学约在西元前一世纪到西元几世纪的时间成形。\\n中世纪哲学（5-16世纪）.\\n中世纪欧洲.\\n中世纪哲学指的是西欧和中东在中世纪的哲学体系，其时间范围没有定论，大致上是从基督化的罗马帝国时期至文艺复兴时期。中世纪哲学被部分定义为对古典希腊和希腊化哲学的再发现和进一步发展，另一部分是需要解决神学问题并把亚伯拉罕诸教（伊斯兰教、犹太教和基督教）的教条同世俗知识一同整合并推广。\\n文艺复兴人文学者们排斥中世纪时期，把它当作在希腊罗马的古典时代与古典文化“复兴”之间的一个“过渡”的野蛮时期。然而在中世纪这将近一千年中哲学在欧洲仍取得了长足地发展。认为\\\"在强度、复杂度还有成就上，可以确信地说哲学在十三世纪的兴盛能与公元前四世纪古希腊哲学的黄金时期媲美。\\\"\\n这个时代讨论的问题有信仰和理智的关系，神的存在与统一，神学话题和形而上学，关于知识、宇宙和个人的问题。\\n中世纪的哲学家包括基督教学者希波的奥古斯丁、波爱修斯、安瑟伦、、皮埃尔·阿伯拉尔、罗吉尔·培根、圣文德、托马斯·阿奎那、邓斯·司各脱、奥卡姆的威廉和让·布里丹等；犹太哲学家迈蒙尼德和吉尔松尼德;还有穆斯林哲学家肯迪、法拉比、海什木、伊本·西那、安萨里、伊本·巴哲、伊本·图费勒、伊本·赫勒敦和伊本·鲁世德等。中世纪的经院哲学传统一直到17世纪仍在活跃，例如弗朗西斯科·苏亚雷斯和等人物。其中托马斯主义之父阿奎那极大地影响了整个天主教欧洲，他特别强调理性和论证，是最先开始使用亚里士多德形而上学和认识论的著作的新译本的学者之一。他的工作明显远离了统治大部分早期经院哲学的新柏拉图主义和奥古斯丁的思想。\\n文艺复兴.\\n从文艺复兴开始，人们的思想开始从清净的僧院走出，来到喧嚣的世界。从而发展自然，也发展人类自身。从而形成人文主义和自然哲学两股既有联系又有区别的想法。\\n\\\"文艺复兴\\\"是对中世纪到近代之间过渡时期的通称，那时对古典文献的重新学习帮助把哲学界的兴趣从对逻辑学、形而上学和神学领域的钻研转移到包括道德、语言学和神秘主义的更加广泛的研究。对经典和人文艺术例如历史学和文学的研究在基督教世界学术界中享有前所未有的兴趣，这个趋势被称为人文主义，它受到柏拉图主义、希腊怀疑主义和罗马斯多葛主义的影响。人文主义者的哲学兴趣跟随彼特拉克转移到造物主与其美德上，替代了中世纪时对形而上学和逻辑学的兴趣。\\n那时对古典哲学的研究出现了两种新方式。一方面对亚里士多德的研究因为的影响而产生了变化。阿威罗伊亚里士多德主义者和更正统的天主教亚里士多德主义者譬如艾尔伯图斯·麦格努斯和托马斯·阿奎那之间的分歧最终在文艺复兴发展出一种“人文亚里斯多德哲学”，譬如伯多禄·蓬波纳齐和的思想。另一方面，在一些之前不为西欧所熟知的作品的重发现的帮助下，对柏拉图和新柏拉图主义的研究作为另一个选择变得普遍起来。著名的文艺复兴时期的柏拉图主义者包括库萨的尼古拉，还有之后的马尔西利奥·费奇诺和若望·皮科·德拉·米兰多拉。\\n文艺复兴也重新产生了对反亚里士多德的把自然看作一个有机的、活生生的整体而不取决于神学的理论的兴趣，例如在库萨的尼古拉、尼古拉·哥白尼、焦尔达诺·布鲁诺、波纳蒂特·特勒肖还有托马索·康帕内拉的著作中。在自然哲学中这样的运动与对神秘主义、魔法、赫尔墨斯主义还有占星学等兴趣重燃相契合，它们被认为隐藏着收获知识和掌控自然方法的大门。\\n这些新的哲学运动伴随着欧洲宗教和政治的剧变同时出现：宗教改革和封建制的衰落。虽然参与宗教改革的神学家们对哲学没有直接的兴趣，他们打破了神学和知识权威的传统基础。同时还伴随着信仰主义和怀疑主义的复兴，体现在伊拉斯谟，蒙泰涅和等思想家身上。同时，民族国家政治上逐步的中央集权的过程得到了世俗政治哲学的响应，如尼可罗·马基亚维利（常被描述为第一个现代政治思想家，或者是现代政治思想形成的关键点）、托马斯·莫尔、伊拉斯谟、尤斯图斯·利普修斯、让·博丹和胡果·格老秀斯等的著作。\\n东亚.\\n先秦诸子之后的两汉经学、魏晋玄学等都是中国哲学的一部份，自唐朝起佛教也开始对哲学产生重要影响；不过中世纪中国哲学最主要的部分是宋明理学的发展。\\n宋明理学反对汉代后开始影响儒学的道教和佛教中的迷信和神秘的元素，是一股倡导更加理性和世俗化儒学的哲学运动。尽管理学遭到道教和佛教徒的批评，理学仍借鉴了它们两个的部分术语和概念。然而和佛教和道教把形而上学看作心灵发展、宗教启示的催化剂并且是不朽的不同，宋明理学把形而上学当作建立一个理性的伦理体系的指导。宋明理学的起源可以追溯到唐朝：韩愈和李翱被视为宋代理学的先驱。宋代理学家周敦颐以道教形而上学理论为框架建立了他的伦理哲学体系，他被看作是宋明理学的创始人。\\n在东亚的其他地方，日本哲学形成于本土的神道信仰和佛教、儒家以及另一些中国哲学和印度哲学学派混合发展。与日本类似，在中巫教的情绪化内容被混合到了从中国传入的理学当中。\\n近代哲学（17-19世纪）.\\n主条目：近代哲学\\n早期近代哲学.\\n西方哲学史上的近代早期一般指17世纪和18世纪，其中18世纪常被称为启蒙时代。现代哲学不同于其前身，它和传统权威例如教会、学院、亚里士多德的关系更加独立，出现了对知识基础和形而上学体系建设的新兴趣；和摆脱了自然哲学的近代物理学的出现。从17世纪开始，近代哲学就以认识论为研究重点。由于经验论（经验主义）与唯理论（理性主义）的争论，使物质与精神的关系作为认识论的首要问题突显出来。\\n当时其他的哲学焦点包括精神的天性和其与身体的关系，新的自然科学对诸如自由意志和神的传统上属于神学的话题的影响，和伦理学和政治哲学的世俗基础\\n。这种潮流最早被鲜明地体现在弗兰西斯·培根的被称为用来扩展知识的新的、经验主义的程序，并很快在笛卡儿的机械主义物理学和理性主义的形而上学中建立了具有巨大影响力的形式。培根运用归纳法，第一个提出思维的主体“人”应该主动干涉自然来为人服务。\\n近现代政治哲学的鼻祖托马斯·霍布斯最早将这套方法论系统地应用在政治哲学上，包括\\\"社会契约\\\"的近代理论。早期近代哲学的学术经典一般包括笛卡尔、斯宾诺莎、莱布尼茨、洛克、贝克莱、休谟和康德。同时期的其他思想家也对哲学做出了贡献，例如伽利略、皮埃尔·伽桑狄、布莱兹·帕斯卡、马勒伯朗士、艾萨克·牛顿、克里斯蒂安·沃尔夫、孟德斯鸠、皮埃尔·贝尔、托马斯·里德、让·勒朗·达朗贝尔和亚当·斯密，而让-雅克·卢梭是反启蒙运动的开创性人物。早期近代哲学的大致结束通常被确定为伊曼努尔·康德的试图限定形而上学范围、证明科学知识并用道德和自由来调和两者的体系。\\n理性主义者中勒内·笛卡儿认为物质世界是由数学关系组成的单一体系，他企图将物理学转化为数学。他在其著作中，对整个经院哲学以及在他那个时代流行的教育与哲学体系加以讽刺。其认为“我思故我在”是认识论的无可怀疑之出发点。笛卡尔是割裂精神和物质的二元论者，为了厘清二者关系，他认为在上帝那里，精神和物质是统一的。其理论被称为笛卡尔主义\\n。斯宾诺莎是笛卡尔之后，又一位著名的现实主义者。他的认识论、几何学和机械观都来自于笛卡尔。但他不认同笛卡尔的二元论，认为精神和物质不过是唯一实体的两种属性\\n。莱布尼茨作为唯理论者坚定地维护笛卡尔的学说并反驳约翰·洛克的理论。与笛卡尔不同的是，他认为万物的实体是“单子”，且互相没有关系，而是由于“前定和谐”才共存一体，即存在于神之中。“前定和谐”调和了笛卡尔之二元论和斯宾诺莎之实体双重性。\\n洛克发展了经验论，他不认同笛卡尔的“天赋观念”，提出白板说，他强调人们从感觉中抽象出普遍的概念，认为感觉中的个别东西才是第一位的。不过他基本认同笛卡尔的二元论。贝克莱发展了洛克的哲学理论，提出了“存在就是被感知”。他认为除了感知的主题和被感知的知觉之外，什么也没有。他非常不赞同物质的抽象概念，认为其既无客观实在，也不能存在于人心。大卫·休谟的理论比贝克莱的更进一步，他不仅仅认为物质实体不存在，更认为精神实体不存在。只承认知觉的存在。他还以自己的不可知论和怀疑论认为不存在统一性和普遍性的东西，认定多样性和个别性才是最高原理。\\n经验论与唯理论的争论也包含了唯物主义与唯心主义的争论。在18世纪时，法国的拉美特利公开宣布唯物主义是唯一的，而百科全书的主编德尼·狄德罗也怀疑神的存在。\\n另外伏尔泰，孟德斯鸠和其他百科全书派的学者都有涉及政治和伦理领域。他们都认为机械主义才是最终形式——物质是唯一的且处于一直运动的，精神只是人脑的属性。因此他们认为无机物与有机物不可逾越，人的思维是人感官的结果。不过他们仍然是相信经验是一直累积的，在因果性上，他们认为只有必然性才是唯一的，这就成为唯心主义的观念。\\n德国古典哲学.\\n从18世纪中后期开始，直到19世纪初，哲学便进入了近代哲学的总结时期，这就是德国古典哲学时期。有两条线索标志着转折的到来：一、思维与存在的关系更加明确；二、产生了系统辩证法。其代表人物有I.康德、J.G.费希特、F.W.谢林、G.W.F.黑格尔等。 \\n康德.\\n康德给哲学带来了三个标志性的创造：\\n他受到休谟的诸多影响，并为西方哲学带来一次革命。他认为哲学的研究核心就是规定理性的任务。\\n康德同意休谟的理论并认为，存在一些原则，使得心灵对经验和认识加以组织，而证据皆可以在数学中找到。即是，包含在命题里的要比包含在原是概念的定义要多得多。他使用称之为批判哲学的先验方法，来展现经验的某些范畴和形式都必然地被预先存在于人们一切言谈之中。\\n凭借着他的三部“批判性”的著作，为先验方法作出相应的结构：\\n他还为道德哲学奠定了新基础，且他赋予了自由概念的新意义。因为其影响在现代依旧尚存，其理论被人们称为康德主义。\\n费希特和谢林.\\n费希特本来承认斯宾诺莎的机械的因果决定论，但后来受到康德的影响，开始认为因果决定论只是表面，其实质为自我不是必然性的奴仆而是独立自由的主体。就此，他建立了主观的思维与客观的存在之统一说。\\n谢林是从费希特理论出发的，但深受斯宾诺莎和文学上浪漫主义的影响，创立了自己的学说。即他认为自然和精神、存在和思维，客体和主体，表面相反，实则统一，是同一个“绝对”的不同发展阶段，这个“绝对”即是万事万物的根源。他认为艺术才是最直观的理性。\\n黑格尔.\\n黑格尔及其理论的出现将西方的哲学推上一个新高度，他创立了西方哲学史上最庞大的客观唯心主义体系，并系统地阐述了辩证法。他的理论和学说对近现代哲学产生了很深远的影响，并被称为黑格尔主义。\\n从黑格尔的思想体系中发展而成的多种哲学运动。其重点就是以历史和逻辑为主，历史方面，它从不同角度理解“凡是合理的就是现实的”；逻辑方面，它有发现其中所说的“真理即整体”。\\n黑格尔认为哲学的重点是放弃分裂，达到统一。他把以前的时代说成是思维与存在、理想与现实分裂，自由与必然，个人与社会、无限与有限、统一性与多样性分裂之时代。\\n他从康德的“心灵的合理性以及在经验中的积极作用”的概念出发，但反对康德的“超越经验世界和‘物自身’的世界”，并认为心灵和世界一样具有相同基础理性结构。他所认为的普遍性不是脱离特殊的抽象普遍，而是包含特殊在内之普遍，即为具体普遍；他所认为的统一也非脱离矛盾、对立的抽象统一，而是包含它们在内的统一，即为对立统一。上述综合在一起即是他的理论：最真实的无所不包的整体即是“绝对精神”，又是对立的统一。\\n他认为，为了达到这个“绝对精神”，需要经过三个阶段，从逻辑、自然到精神，即是从思维到存在，再到两者统一的过程，从而完成他的统一论。\\n就此，社会和历史的现象，便被赋予一种在哲学史上还是崭新的显赫地位。他还将伦理学划归到这个领域，从而在伦理学理论和对思想的理解中提出重要的路线。\\n现代哲学（19-20世纪）.\\n从19世纪中叶开始，西方哲学就进入现代哲学阶段。因为在19世纪中期，欧洲的工业革命几近完成。\\n现代哲学，特别是19世纪中后期的哲学流派，有叔本华的意志主义，新康德主义，新黑格尔主义，马克思主义。然而此时的哲学与后来的存在主义、现象学等在当代一般归为「欧陆哲学」，与二十世纪以后著重严谨逻辑与语词分析所发展出的「分析哲学」成为风格迥异的两大西方哲学典范。\\n20世纪的西方哲学上主流有两条：\\n现代哲学主要包含以下几种潮流。\\n以黑格尔主义为主的潮流.\\n历程哲学：\\n主流马克思主义：\\n“哲学只是持续地解释世界，而与当下关联的则是改变世界。”\\n西方马克思主义：\\n革新的黑格尔主义：\\n结构主义：\\n后古典现代哲学.\\n分析哲学：\\n实证主义：\\n新康德主义：\\n逻辑实证主义：\\n语言哲学：\\n现象学：\\n唯物论：\\n新托马斯主义：\\n多种主义的第三潮流.\\n科学哲学：\\n意志主义：\\n实用主义：\\n存在主义：\\n解释学：\\n重要哲学流派.\\n德国唯心主义.\\n唯心主义的各种变体在18世纪晚期至20世纪早期的哲学界相当流行。康德主张的先验唯心主义认为人们对事物的理解是有界限的，因为在客观判断条件下很多事情是办不到的。他在1781年发行的作品《纯粹理性批判》试图调和18世纪两大主要的哲学派别：经验主义和理性主义，并且建立一个研究形而上学的新基础。\\n德国唯心主义最著名的作品是黑格尔于1807年出版的《精神现象学》。黑格尔承认自己的理念不是新的，不过他的目标是完成之前的哲学家们的不完整的体系。黑格尔认为哲学的重点是放弃分裂，达到统一。他把以前的时代说成是思维与存在、理想与现实分裂，自由与必然、个人与社会、无限与有限、统一性与多样性分裂之时代。他从康德的“心灵的合理性以及在经验中的积极作用”的概念出发，但反对康德的“超越经验世界和‘物自身’的世界”，并认为心灵和世界一样具有相同基础理性结构。他所认为的普遍性不是脱离特殊的抽象普遍，而是包含特殊在内之普遍，即为具体普遍；他所认为的统一也非脱离矛盾、对立的抽象统一，而是包含它们在内的统一，即为对立统一。上述综合在一起即是他的理论：最真实的无所不包的整体即是“绝对精神”，又是对立的统一。黑格尔认为需要经过三个阶段来达到这个“绝对精神”，从逻辑、自然到精神，即是从思维到存在，再到两者统一的过程，从而完成他的统一论。他还将伦理学划归到这个领域，从而在伦理学理论和对思想的理解中提出重要的路线。\\n马克思主义.\\n马克思主义哲学是马克思和恩格斯建立的以辩证唯物主义为核心的哲学体系。其认为实践是检验哲学之真理性的唯一标准，哲学应伴随着社会、科学技术和文化的发展而不断发展。其主要思想体系在19世纪70年代主要由恩格斯创立，20世纪20年代在苏联形成完整体系——辩证唯物主义和历史唯物主义，这个体系在后来的社会主义国家推动下得以发展。马克思主义哲学宣称自己的理论体系具有科学性，认为哲学可以成为科学的一部分。同时马克思主义哲学认为哲学还具有意识形态的性质。\\n另外马克思主义在政治上也指各种不同的共产主义运动，如由列宁所创立而被斯大林修改的苏联马克思主义，称为马克思列宁主义，为俄国革命以及后来建立的各种共产党之教义。它的旁系包括反斯大林的托洛茨基及其追随者的马克思主义、毛泽东的马克思列宁主义等。\\n实用主义.\\n实用主义产生于19世纪70年代的现代哲学派别，在20世纪的美国成为一种主流思潮。对法律、政治、教育、社会、宗教和艺术的研究产生了很大的影响。实用主义也试图在理性主义及经验主义找出一条中间道路来，是「经验主义思想方法与人类的比较具有宗教性需要的适当的调和者。」\\n现象学.\\n现象学是由德国哲学家胡塞尔在1900年提出的理论，强调对直接直观和经验感知的区分，认为哲学（或至少是现象学）的主要任务是厘清二者之间的关联，并且在直观中获得对本质的认识。现象学是对经验结构与意识结构的哲学性研究。作为一个哲学运动，现象学于二十世纪早期由埃德蒙德·胡塞尔创立，之后被他在德国的哥廷根大学和慕尼黑大学中的一派追随者发展壮大。在此之后现象学传播到法国、美国以及其他地区，并远超出了胡塞尔早期著作的语境。 其他主要哲学家包括海德格、莫里斯·梅洛-庞蒂 以及伊曼纽尔·列维纳斯。\\n存在主义.\\n存在主义是一个哲学的非理性主义思潮，该术语被用在十九世纪晚期到二十世纪的一些哲学家的工作上，尽管他们的学说相差巨大，但他们都相信哲学思考开始于人类主体——而不仅仅是思维主体，而且包括行为、感知、人类个体。存在主义强调个人、独立自主和主观经验，认为人存在的意义是无法经由理性思考而得到答案。在存在主义中，个体的出发点的特征是被称为“存在的态度”，或一种面对显然是一个无意义的或荒谬的世界的迷失和混乱的感觉。很多存在主义者还认为传统的体系和哲学学术无论是内容和风格都过于抽象并远离人类经验。\\n19世纪哲学家克尔凯郭尔和尼采被看作存在主义的先驱，尽管他们没有使用这个术语。然而他们的影响延伸出了存在主义思想。克尔凯郭尔著作主要针对的是黑格尔的唯心主义哲学体系，他认为其忽视或排除了人类的内在主观生命。相反克尔凯郭尔认为\\\"真理是主观的\\\"，主张对一个现实的人类来说最重要的问题是处理个人与存在内在关系的问题。克尔凯郭尔作为一个基督徒相信宗教信仰的真相是一个主观问题，而且人应该用热情去深思这个问题。\"}\n"
     ]
    }
   ],
   "source": [
    "! head -2 wikipedia-zh-cn-8192.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "140d4633-c664-4d71-8e61-51eb180b5b33",
   "metadata": {},
   "source": [
    "### 2 工具安装"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b0e1ed-a1a7-4b98-927e-64d69d274098",
   "metadata": {},
   "source": [
    "```shell\n",
    "git pull https://gitee.com/janelu9/EasyLLM\n",
    "cd EasyLLM\n",
    "pip wheel -e . --no-deps && pip install jllm-*-py3-none-any.whl\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "126c8917-c830-4540-a88a-f911f35acc33",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install jllm-3.2.4.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e475aa-6f85-4db5-bf06-9a411ca553f4",
   "metadata": {},
   "source": [
    "### 3 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3165bcd-0af4-4ae9-bbbc-3880a3b87ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(t='pretrain', i='wikipedia-zh-cn-512.jsonl', o='', n=8388608, c='gzip', batch_size=65536, max_len=8193, cores=-1, max_num=1, max_pixels=2073600, tokenizer='Qwen2.5-7B-Instruct', image_path='', tmp='tmp', stack=True, T=False, C=True, pad=False, filter=False)\n",
      "/mnt\n",
      "########## begine converting pretrain data with 204 executors.###########\n",
      "258it [00:06, 39.14it/s]\n",
      "/mnt/tmp/wikipedia-zh-cn-512-00000 stored in parquet with 258 samples\n",
      "/mnt/wikipedia-zh-cn-512.jsonl has been converted into /mnt/wikipedia-zh-cn-512_Qwen2.5-7B-Instruct successfully!\n"
     ]
    }
   ],
   "source": [
    "!python -m jllm.raw2ids \\\n",
    "    --tokenizer Qwen2.5-7B-Instruct \\\n",
    "    -i wikipedia-zh-cn-512.jsonl \\\n",
    "    --max_len 8193 \\\n",
    "    -t pretrain --stack -C \n",
    "# --stack 拼接token凑成max_len的长度，减少pad_id; \n",
    "# -C 清除缓存重新转换 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d434280-b77e-4fd6-ba6d-0353b0a38b02",
   "metadata": {},
   "source": [
    "### 3.1 数据检查(可选步骤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "80ddfc47-1201-4a3a-9163-27d4a01cba36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "import pyarrow.parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a737ecb1-568b-443f-9721-2373c9e38827",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained('Qwen2.5-7B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1d2dbade-d4d6-4984-b9e3-05a68acf2ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pyarrow.parquet.read_table('wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/wikipedia-zh-cn-512-00000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb4a43c-9561-4383-8034-592267abc476",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids =data['input_ids'].to_numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b9e7bd3-496f-433f-a874-b3b892749e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "57d42ded-1143-453c-a49a-2779bf0b7a93",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>数学是研究数量、结构以及空间等概念及其变化的一门学科，属于形式科学的一种。数学利用抽象化和逻辑推理，从计数、计算、量度、对物体形状及运动的观察发展而成。数学家们拓展这些概念，以公式化新的猜想，以及从选定的公理及定义出发，严谨地推导出一些定理。\n",
      "基础数学的知识与运用是生活中不可或缺的一环。对数学基本概念的完善，早在古埃及、美索不达米亚及古印度历史上的古代数学文本便可观见，而在古希腊那里有更为严谨的处理。从那时开始，数学的发展便持续不断地小幅进展，至16世纪的文艺复兴时期，因为新的科学发现和数学革新两者的交互，致使数学的加速发展，直至今日。数学并成为许多国家及地区的教育中的一部分。\n",
      "数学在许多领域都有应用，包括科学、工程、医学、经济学和金融学等。数学对这些领域的应用通常被称为应用数学，有时亦会激起新的数学发现，并导致全新学科的发展，例如物理学的实质性发展中建立的某些理论激发数学家对于某些问题的不同角度的思考。数学家也研究纯粹数学，就是数学本身的实质性内容，而不以任何实际应用为目标。许多研究虽然以纯粹数学开始，但其过程中也发现许多可用之处。\n",
      "词源.\n",
      "西方语言中“数学”（）一词源自于古希腊语的（），其有“学习”、“学问”、“科学”，还有个较狭义且技术性的意思－「数学研究」，即使在其语源内。其形容词（），意思为「和学习有关的」或「用功的」，亦会被用来指「数学的」。其在英语中表面上的复数形式，及在法语中的表面复数形式'，可溯至拉丁文的中性复数'，由西塞罗译自希腊文复数（），此一希腊语被亚里士多德拿来指「万物皆数」的概念。\n",
      "汉字表示的「数学」一词大约产生于中国宋元时期。多指象数之学，但有时也含有今天上的数学意义，例如，秦九韶的《数学九章》（《永乐大典》记，即《数书九章》也被宋代周密所著的《癸辛杂识》记为《数学大略》）、《数学通轨》（明代柯尚迁著）、《数学钥》（清代杜知耕著）、《数学拾遗》（清代丁取忠撰）。直到1939年，经过中国数学名词审查委员会研究“算学”与“数学”两词的使用状况后，确认以“数学”表示今天意义上的数学含义。\n",
      "历史.\n",
      "数学有着久远的历史。它被认为起源于人类早期的生产活动：中国古代的六艺之一就有「数」，数学一词在西方有希腊语词源（mathematikós），意思是“学问的基础”，源于（máthema，“科学，知识，学问”）。\n",
      "史前的人类就已尝试用自然的法则来衡量物质的多少、时间的长短等抽象的数量关系，比如时间单位有日、季节和年等。算术（加减乘除）也自然而然地产生了。古代的石碑及泥版亦证实了当时已有几何的知识。\n",
      "更进一步则需要写作或其他可记录数字的系统，如符木或于印加帝国内用来储存数据的奇普。历史上曾有过许多不同的记数系统。\n",
      "在最初有历史记录的时候，数学内的主要原理是为了做税务和贸易等相关计算，为了解数字间的关系，为了测量土地，以及为了预测天文事件而形成的。这些可以简单地被概括为数学对数量、结构、空间及时间方面的研究。\n",
      "到了16世纪，算术、初等代数以及三角学等初等数学已大体完备。17世纪变量概念的产生使人们开始研究变化中的量与量的互相关系和图形间的互相变换，微积分的概念也在此时形成。随着数学转向形式化，为研究数学基础而产生的集合论和数理逻辑等也开始发展。数学的重心从求解实际问题转变到对一般形式上的思考。\n",
      "从古至今，数学便一直不断地延展，且与科学有丰富的相互作用，两者的发展都受惠于彼此。在历史上有著许多数学发现，并且直至今日都不断地有新的发现。据米哈伊尔·B·塞甫留克（Mikhail B. Sevryuk）于2006年1月的期刊中所说，「存放于数学评论资料库中论文和书籍的数量自1940年（数学评论的创刊年份）现已超过了一百九十万份，而且每年还增加超过七万五千份。此一学海的绝大部份为新的数学定理及其证明。」\n",
      "形成、纯数学与应用数学及美学.\n",
      "每当有涉及数量、结构、空间及变化等方面的问题时，通常就需要用到数学去解决问题，而这往往也拓展了数学的研究范畴。一开始，数学的运用可见于贸易、土地测量及之后的天文学。今日，所有的科学都存在著值得数学家研究的问题，且数学本身亦给出了许多的问题。牛顿和莱布尼兹是微积分的发明者，费曼发明了费曼路径积分，这是推理及物理洞察二者的产物，而今日的弦理论亦引申出新的数学。一些数学只和生成它的领域有关，且用来解答此领域的更多问题。但一般被一领域生成的数学在其他许多领域内也十分有用，且可以成为一般的数学概念。即使是「最纯的」数学通常亦有实际的用途，此一非比寻常的事实，被1963年诺贝尔物理奖得主维格纳称为「数学在自然科学中不可想像的有效性」。\n",
      "如同大多数的研究领域，科学知识的爆发导致了数学的专业化。主要的分歧为纯数学和应用数学。在应用数学内，又被分成两大领域，并且变成了它们自身的学科——统计学和电脑科学。\n",
      "许多数学家谈论数学的\"优美\"，其内在的美学及美。「简单」和「一般化」即为美的一种。另外亦包括巧妙的证明，如欧几里得对存在无限多质数的证明；又或者是加快计算的数值方法，如快速傅立叶变换。高德菲·哈罗德·哈代在《一个数学家的自白》一书中表明他相信单单是美学上的意义，就已经足够成为数学研究的正当理由。\n",
      "符号、语言与精确性.\n",
      "我们现今所使用的大部分数学符号在16世纪后才被发明出来的。在此之前，数学以文字的形式书写出来，这种形式会限制了数学的发展。现今的符号使得数学对于专家而言更容易掌握，但初学者却常对此望而却步。它被极度的压缩：少量的符号包含著大量的讯息。如同音乐符号一般，现今的数学符号有明确的语法，并且有效地对讯息作编码，这是其他书写方式难以做到的。符号化和形式化使得数学迅速发展，并帮助各个科学领域建立基础支撑理论。\n",
      "数学语言亦对初学者而言感到困难。如“或”和“只”这些字有著比日常用语更精确的意思。亦困恼著初学者的，如“开放”和“域”等字在数学里有著特别的意思。数学术语亦包括如“同胚”及“可积性”等专有名词。但使用这些特别符号和专有术语是有其原因的：数学需要比日常用语更多的精确性。数学家将此对语言及逻辑精确性的要求称为「严谨」。但在现实应用中，舍弃一些严谨性往往会得到更好的结果。\n",
      "严谨是数学证明中很重要且基本的一部份。数学家希望他们的定理以系统化的推理依著公理被推论下去。这是为了避免依著不可靠的直观而推出错误的「定理」，而这情形在历史上曾出现过许多的例子。在数学中被期许的严谨程度因著时间而不同：希腊人期许著仔细的论证，但在牛顿的时代，所使用的方法则较不严谨。牛顿为了解决问题所做的定义，到了十九世纪才重新以小心的分析及正式的证明来处理。今日，数学家们则持续地在争论电脑协助证明的严谨度。当大量的计算难以被验证时，其证明亦很难说是足够地严谨。\n",
      "公理在传统的思想中是「不证自明的真理」，但这种想法是有问题的。在形式上，公理只是一串符号，其只对可以由公理系统导出的公式之内容有意义。希尔伯特计划即是想将所有的数学放在坚固的公理基础上，但依据哥德尔不完备定理，每一相容且能蕴涵皮亚诺公理的公理系统必含有一不可决定的公式；因而所有数学的最终公理化是不可能的。尽管如此，数学常常被想像成只是某种公理化的集合论，在此意义下，所有数学叙述或证明都可以写成集合论的公式。\n",
      "数学作为科学.\n",
      "卡尔·弗里德里希·高斯称数学为「科学的皇后」。在拉丁原文'，以及其德语'中，对应于「科学」的单字的意思皆为知识（领域）。而实际上，science一词在英语内本来就是这个意思，且无疑问地数学在此意义下确实是一门「科学」。将科学限定在自然科学则是在此之后的事。若认为科学是只指物理的世界时，则数学，或至少是纯数学，不会是一门科学。爱因斯坦曾如此描述：「数学定律越和现实有关，它们越不确定；若它们越是确定的话，它们和现实越不会有关。」许多哲学家相信数学在经验上不具可否证性，且因此不是卡尔·波普尔所定义的科学。但在1930年代时，在数理逻辑上的重大进展显示数学不能归并至逻辑内，且波普尔推断「大部份的数学定律，如物理及生物学一样，是假设演绎的：纯数学因此变得更接近其假设为猜测的自然科学，比它现在看起来更接近。」然而，其他的思想家，如较著名的拉卡托斯，便提供了一个关于数学本身的可否证性版本。\n",
      "另一观点则为某些科学领域（如理论物理）是其公理为尝试著符合现实的数学。而事实上，理论物理学家齐曼（John Ziman）即认为科学是一种公众知识，因此亦包含著数学。在任何的情况下，数学和物理科学的许多领域都有著很多相同的地方，尤其是从假设所得的逻辑推论之探索。直觉和实验在数学和科学的猜想建构上皆扮演著重要的角色。实验数学在数学中的重要性正持续地在增加，且计算和模拟在科学及数学中所扮演的角色也越来越加重，减轻了数学不使用科学方法的缺点。在史蒂芬·沃尔夫勒姆2002年的著作《一种新科学》中他提出，计算数学应被视为其自身的一科学领域来探索。\n",
      "数学家对此的态度并不一致。一些研究应用数学的数学家觉得他们是科学家，而那些研究纯数学的数学家则时常觉得他们是在一门较接近逻辑的领域内工作，且因此基本上是个哲学家。许多数学家认为称他们的工作是一种科学，是低估了其美学方面的重要性，以及其做为七大博雅教育之一的历史；另外亦有人认为若忽略其与科学之间的关联，是假装没看到数学和其在科学与工程之间的交互影响，进而促进了数学在许多科学上的发展此一事实。这两种观点之间的差异在哲学上产生了数学是「被创造」（如艺术）或是「被发现」（如科学）的争议。大学院系划分中常见「科学和数学系」，这指出了这两个领域被看作有紧密联系而非一样。实际上，数学家通常会在大体上与科学家合作，但在细节上却会分开。此争议亦是数学哲学众多议题的其中一个。\n",
      "数学的各领域.\n",
      "如上所述，数学主要的学科最先产生于商业上计算的需要、了解数字间的关系、测量土地及预测天文事件。这四种需要大致地与数量、结构、空间及变化（即算术、代数、几何及分析）等数学上广泛的子领域相关连著。除了上述主要的关注之外，亦有用来探索由数学核心至其他领域上之间的连结的子领域：至逻辑、至集合论（基础）、至不同科学的经验上的数学（应用数学）、及较近代的至不确定性的严格研究。\n",
      "基础与哲学.\n",
      "为了阐明数学基础，数学逻辑和集合论等领域被发展了出来。\n",
      "数学逻辑专注于将数学置在一坚固的公理架构上，并研究此一架构的结果。就数学逻辑本身而言，其为哥德尔第二不完备定理所属的领域，而这或许是逻辑学中最广为流传的成果：总是存在不能被证明的真命题。\n",
      "现代逻辑被分成递归论、模型论和证明论，且和理论电脑科学有著密切的关连性，千禧年大奖难题中的P/NP问题就是理论电脑科学中的著名问题。\n",
      "纯粹数学.\n",
      "数量.\n",
      "数量的研究起于数，一开始为熟悉的自然数及整数与被描述在算术内的自然数及整数的算术运算。整数更深的性质于数论中有详细的研究，此一理论包括了如费马最后定理等著名的结果。数论还包括两个被广为探讨的未解问题：孪生质数猜想及哥德巴赫猜想。\n",
      "当数系更进一步发展时，整数被视为有理数的子集，而有理数则包含于实数中，连续的量即是以实数来表示的。实数则可以被进一步广义化成复数。数的进一步广义化可以持续至包含四元数及八元数。从自然数亦可以推广到超限数，它形式化了计数至无限的这一概念。另一个研究的领域为大小，这个导致了基数和之后对无限的另外一种概念：阿列夫数，它允许无限集合之间的大小可以做有意义的比较。\n",
      "结构.\n",
      "许多如数及函数的集合等数学物件都有著内含的结构。这些物件的结构性质被探讨于群、环、-{zh-cn:域;zh-tw:体}-等抽象系统中，该些物件事实上也就是这样的系统。此为代数的领域。在此有一个很重要的概念，即广义化至向量空间的向量，它于线性代数中被研究。向量的研究结合了数学的三个基本领域：数量、结构及空间。向量分析则将其扩展至第四个基本的领域内，即变化。\n",
      "创立于二十世纪三十年代的法国的布尔巴基学派认为：纯粹数学，是研究抽象结构的理论。\n",
      "结构，就是以初始概念和公理出发的演绎系统。\n",
      "布尔巴基学派认为，有三种基本的抽象结构：代数结构（群，环，域……），序结构（偏序，全序……），拓扑结构（邻域，极限，连通性，维数……）。\n",
      "空间.\n",
      "空间的研究源自于几何－尤其是欧几里得几何。三角学则结合了空间及数，且包含有著名的勾股定理。现今对空间的研究更推广到了更高维的几何、非欧几里得几何（其在广义相对论中扮演著核心的角色）及拓扑学。数和空间在解析几何、微分几何和代数几何中都有著很重要的角色。在微分几何中有著纤维丛及流形上的微积分等概念。在代数几何中有著如多项式方程的解集等几何物件的描述，结合了数和空间的概念；亦有著拓扑群的研究，结合了结构与空间。李群被用来研究空间、结构及变化。在其许多分支中，拓扑学可能是二十世纪数学中有著最大进展的领域，并包含有存在已久的庞加莱猜想，以及有争议的四色定理。庞加莱猜想已在2006年确认由俄罗斯数学家格里戈里·佩雷尔曼证明，而四色定理已在1976年由凯尼斯·阿佩尔和沃夫冈·哈肯用电脑证明，而从来没有由人力来验证过。\n",
      "变化.\n",
      "了解及描述变化在自然科学里是一普遍的议题，而微积分更为研究变化的有利工具。函数诞生于此，作为描述一变化的量的核心概念。对于实数及实变函数的严格研究为实分析，而复分析则为复数的等价领域。黎曼猜想——数学最基本的未决问题之一——便是以复分析来描述的。泛函分析注重在函数的（一般为无限维）空间上。泛函分析的众多应用之一为量子力学。许多的问题很自然地会导出一个量与其变化率之间的关系，而这在微分方程中被研究。在自然界中的许多现象可以被动力系统所描述；混沌理论则是对系统的既不可预测而又是决定的行为作明确的描述。\n",
      "离散数学.\n",
      "离散数学是指对理论电脑科学最有用处的数学领域之总称，这包含有可计算理论、计算复杂性理论及资讯理论。可计算理论检验电脑的不同理论模型之极限，这包含现知最有力的模型——图灵机。复杂性理论研究可以由电脑做为较易处理的程度；有些问题即使理论是可以以电脑解出来，但却因为会花费太多的时间或空间而使得其解答仍然不为实际上可行的，尽管电脑硬体的快速进步。最后，资讯理论专注在可以储存在特定媒介内的资料总量，且因此有压缩及熵等概念。\n",
      "作为一相对较新的领域，离散数学有许多基本的未解问题。其中最有名的为P/NP问题——千禧年大奖难题之一。一般相信此问题的解答是否定的。\n",
      "应用数学.\n",
      "应用数学思考将抽象的数学工具运用在解答科学、工商业及其他领域上之现实问题。应用数学中的一重要领域为统计学，它利用机率论为其工具并允许对含有机会成分的现象进行描述、分析与预测。大部份的实验、调查及观察研究需要统计对其资料的分析。（许多的统计学家并不认为他们是数学家，而比较觉得是合作团体的一份子。）数值分析研究有什么计算方法，可以有效地解决那些人力所限而算不出的数学问题；它亦包含了对计算中舍入误差或其他来源的误差之研究。\n",
      "数学奖项.\n",
      "数学奖通常和其他科学的奖项分开。数学上最有名的奖为菲尔兹奖，创立于1936年，每四年颁奖一次。它通常被认为是数学领域的诺贝尔奖。另一个国际上主要的奖项为阿贝尔奖，创立于2003年。两者都颁奖于特定的工作主题，包括数学新领域的创新或已成熟领域中未解决问题的解答。著名的23个问题，称为希尔伯特的23个问题，于1900年由德国数学家大卫·希尔伯特所提出。这一连串的问题在数学家之间有著极高的名望，且至少有九个问题已经被解答了出来。另一新的七个重要问题，称为千禧年大奖难题，发表于2000年。对其每一个问题的解答都有著一百万美元的奖金，而当中只有一个问题（黎曼猜想）和希尔伯特的问题重复。<|endoftext|><|im_start|>哲学是研究普遍的、基本问题的学科，包括存在、知识、价值、理智、心灵、语言、人生、道德等领域。哲学与其他学科不同之处在于哲学有独特之思考方式，例如批判的方式、通常是系统化的方法，并以理性论证为基础。从历史上看，许多单独的学科，例如物理学、生物学等自然科学，或法学、政治学、心理学等社会科学，都曾被视作哲学的一部分或其分支学科。直至其得到后续发展后，才逐渐被视作现代意义上的独立学科。\n",
      "哲学的词义在现代末期发生了变化，成为了今天常见的、更狭隘的含义。在这个新意义上，该术语主要与形而上学、认识论、伦理学和美学等哲学学科相关。除其他主题外，它涵盖了对现实、知识和价值观的理性研究。然而，它不同于其他理性探究学科，例如经验科学和数学。\n",
      "词源.\n",
      "英语词语（）源于古希腊语中的，意思为「爱智慧」，有时也译为「智慧的朋友」，该词由（philos，爱）的派生词（philein，去爱）和（sophia，智慧）组合而成。一般认为，古希腊思想家毕达哥拉斯最先在著作中引入“哲学家”和“哲学”这两个术语。\n",
      "“哲”一词在中国古代指那些善于思辨、学问精深者，如“孔门十哲”、“古圣先哲”等，“哲”或“哲人”意义类似西方近世所谓“哲学家”、“思想家”。1874年，日本启蒙家西周，在《百一新论》中首先用汉语词「哲学」来翻译「philosophy」一词。哲学中的形而上学（英语：metaphysics）的中文名称取自《易经·系辞上传》「形而上者谓之道，形而下者谓之器」一语。\n",
      "哲学的概念.\n",
      "哲学的定义.\n",
      "哲学家们对哲学本身的定义存在分歧。没有共同的共识，这归因于哲学本身的性质是一个开放的哲学问题。许多伟大的哲学家，如柏拉图、黑格尔等，对问题“什么是哲学？”提出了答案，但这些答案在今天不太可能被广泛接受。其中一个原因是哲学的本质本身就是一个哲学问题，因此不应期望得到无争议的答案——如果哲学家们停止争论，哲学家的职业就将结束。\n",
      "英国哲学家罗素对哲学的定义是：\n",
      "胡适在《中国哲学史大纲》中称「凡研究人生切要的问题，从根本上着想，要寻一个根本的解决：这种学问叫做哲学」。\n",
      "虽然哲学源自西方的传统，但许多文明在历史上都存在著一些相似的论题。东亚和南亚的哲学被称之为东方哲学，而北非和中东则因为其和欧洲密切的互动，因此常被视为是西方哲学的一部份。\n",
      "对哲学的主题亦存在许多看法。一些人认为哲学是对问题本身过程的观察。\n",
      "研究基础.\n",
      "古希腊哲学家经常提出问题，他们所提出的问题大概可以归类为三类，这三类问题分别形成了哲学的基础学科——分别是形而上学、伦理学、认识论 。\n",
      "现代哲学上出现\"不要求精确理由\"之哲学理论，例如\"虚假\"(认定本质不可知)，这种现象将不可知论(世界上终究有人不能理解的存在)的重要程度提高了。\n",
      "自亚里士多德时代以来，在古典或者现代哲学当中，逻辑通常都扮演着重要的角色。特别是其提出的三段论，对西方哲学发展有着深远影响。\n",
      "分支.\n",
      "主分支.\n",
      "哲学可以分为很多不同的分支，主要包括形而上学、知识论、伦理学、逻辑学和美学。\n",
      "特殊分支.\n",
      "这些分支是应用在其他学科，或者交叉学科的哲学研究。\n",
      "历史.\n",
      "很多人类社群思考过哲学问题并且互相学习建立了各种哲学流派。\n",
      "东方哲学是通过每个地区的历史时期来组织的。西方哲学一般可以分为三个或更多时期，最重要的是古典哲学、中世纪哲学和近代哲学。\n",
      "古典哲学.\n",
      "古印度.\n",
      "印度哲学的历史源远流长，早在吠陀时代已经开始，至公元前6世纪为全盛时期。当时古印度的思想百花齐放，其中最著名的包括佛教创始人释迦牟尼佛、耆那教创始人笩駄摩那、阿耆多·翅舍钦婆罗、波拘陀·迦旃延、富兰那·迦叶、数论派等。\n",
      "中国.\n",
      "中国哲学的主要部分起源东周时期，当时以诸子百家广为人知，以孔子的儒家、老子的道家、墨子的墨家及晚期的法家为代表，还有一些流派例如农家、阴阳家和名家在之后则名声不显。在秦朝焚书坑儒后除了法家、儒家、道家外其他流派都不再活跃。在当代，中国哲学仍然在亚洲文化扮演一定作用，但是学理上仍在争辩中国哲学是否应归为哲学。\n",
      "如牟宗三曾对哲学下定义:「凡是对人性的活动所及，以理智及观念加以反省说明的，便是哲学。」牟宗三认为，中国有数千年文化史，确有哲学。只是中国哲学重视主体性，西方重视客体性。因此如果以西方的逻辑和知识论等对哲学下定义，中国没有这些。但如果以主体性方向对哲学下定义，中国文化就拥有哲学。反之西方对人生的哲学多表现在文学、艺术、音乐等等。就以西方哲学史来说，没有一章特别谈及耶稣。\n",
      "古希腊-罗马.\n",
      "古希腊-罗马哲学是西方哲学的一个时期，时间为公元前6世纪到公元6世纪。它一般被分为三个时期：前苏格拉底时期、柏拉图和亚里士多德的古典希腊时期、和后亚里士多德（或希腊化）时期：有时候会把新柏拉图主义和基督教哲学家们的古典时代晚期加入作为第四个时期。\n",
      "前苏格拉底时期.\n",
      "在公元前6世纪的希腊，西方哲学就从古代神话和诗歌中脱颖而出，逐步开始对宇宙的组成以及本源的思考而开始了独立发展。前苏格拉底时期的自然派哲学家们多关注自然界，被认为是西方最早的哲学家，不管他们认识以及解释世界的方式是否正确，但是他们的想法之所以有别于迷信的原因在于，这些哲学家是以理性辅佐证据的方式归纳出自然界的现象。诸如：\n",
      "公元前5世纪中期，普罗泰戈拉和高尔吉亚等所形成的辩士学派将研究的重点由自然转移到人类本身。认为“人才是万物之本”。他们都不相信有真正的存在和真理。普罗泰戈拉认为是非善恶都是相对于人的感觉而言，而高尔吉亚却认为所有的都是同样的假，这是怀疑论的雏形。 \n",
      "公元前6世纪末，以毕达哥拉斯为主的毕达哥拉斯学派所主张的哲学与前述的观点既相近又有不同。罗马古代的历史上记载毕达哥拉斯第一个称自己为哲学家，或者说是爱智慧。他认为“一切都是数字”。其意思就是说一切事物的实质和结构都是它们所包含的数字关系所决定的。他称平均、秩序和调和是宇宙的三大基调，并以音乐的调和说明宇宙的调和。他所在的学派将宇宙总结为十种性质相异的组合：有限与无限、奇与偶、少与多、左与右、男与女、静与动、直与曲、光明与黑暗、善与恶、方与圆。至此之后，数学的本质及其地位，一直都是哲学的主要问题之一，数学不受观察和实验造成的不确定性影响，而且是通过纯粹的思想加以理解的。\n",
      "其中关于变与不变的关系的争论，真实世界与直觉世界的差别，真理与意见的矛盾，导致产生了认识论的问题。\n",
      "古典希腊时期.\n",
      "在古典希腊时期西方哲学方法的关键特质被建立：依靠诉诸理性和论证，通过一种批判性的方法来接受或建立观点。这包括苏格拉底被称为苏格拉底反诘法或“反驳论证”方法的辩证法，他主要用其来检验例如善良和公平正义的关键道德概念。这种方法将一个问题分解成一系列的疑问，在对疑问的回答中逐步提取想要找到的答案，其极大影响可以从现在使用的科学方法中看出，在科学方法中假说是第一个阶段。\n",
      "苏格拉底没有直接教过人，但之后的柏拉图深受其影响。而其整个哲学思想来源于两大理论：其一，永远不要做坏事；其二，一个内心真正纯洁且正义的人绝不会做相反之事。他认为真理有其客观性，试图推翻智者们以个人主观感觉为真理的思想。然后提出德的概念，以作为人生行事的方向。对于道德是什么的问题，苏格拉底的回复为“知识即道德。”对于知识是何物的问题，他回答说知识是透过理性而得的概念。苏格拉底开创了认识论和伦理学，如此奠定了他的哲学地位。\n",
      "古典希腊时期的的哲学家中柏拉图和亚里士多德对后世的影响力最大，特别是柏拉图被认为是西方哲学的创始人。哲学家阿尔弗雷德·诺思·怀特黑德评价柏拉图：“欧洲哲学传统最被普遍公认的特点，就是它包含了一系列对柏拉图的注脚。我的意思不是怀疑学者们系统体系的思想是提取自柏拉图的著作。我暗示的是那些他们散落的一般思想的财富。”换言之即使数千年后，人们依旧在试著回答他所提出的问题，这也代表著人们依然为这些问题或是这些问题所延伸的更多问题而感到困惑。\n",
      "毕达哥拉斯的思想对柏拉图产生了显著的影响，并通过柏拉图影响了整个西方哲学。柏拉图和亚里士多德作为最早的古典希腊哲学家批判地引用了其它的一些“智者”，当时这些人在希腊被称为“辩士”并在毕达哥拉斯之前相当普遍。从他们的批判看来，在他们的古典时代一个在更高尚地、纯粹地“爱智慧”（真的哲学家）与那些更早更普遍的旅行教师——经常也通过自己的技艺来赚钱——之间的分水岭之后被建立。\n",
      "希腊化时代.\n",
      "亚里士多德死后，整个哲学界陷入了独立时期，称为希腊化哲学时期。因为整个社会和政治陷入混乱。这段时期产生了斯多葛学派和伊壁鸠鲁学派，以及怀疑主义派、新柏拉图派和新毕达哥拉斯主义。这些学派的共同特点是伦理化。斯多葛学派主要是顺应自然和自制。伊壁鸠鲁学派则是把快乐作为生活的本质和善的标准。而新柏拉图派和新毕达哥拉斯派都是带有宗教主义的哲学，并逐渐产生融化基督教和希腊哲学于一体的理论，即为后来的基督教哲学。\n",
      "直到公元529年，罗马皇帝查士丁尼一世尼命令关闭雅典的柏拉图学院。J.B.伯里称一些余下的学院成员逃入了萨珊王朝首都泰西封。\n",
      "古印度.\n",
      "印度哲学是指起源于印度次大陆的哲学思想，包括、佛教哲学、等，这些印度哲学具有一些共同且复杂的起源，都有有关佛法及业的主题，而且都希望达到个人的解放。这些哲学约在西元前一世纪到西元几世纪的时间成形。\n",
      "中世纪哲学（5-16世纪）.\n",
      "中世纪欧洲.\n",
      "中世纪哲学指的是西欧和中东在中世纪的哲学体系，其时间范围没有定论，大致上是从基督化的罗马帝国时期至文艺复兴时期。中世纪哲学被部分定义为对古典希腊和希腊化哲学的再发现和进一步发展，另一部分是需要解决神学问题并把亚伯拉罕诸教（伊斯兰教、犹太教和基督教）的教条同世俗知识一同整合并推广。\n",
      "文艺复兴人文学者们排斥中世纪时期，把它当作在希腊罗马的古典时代与古典文化“复兴”之间的一个“过渡”的野蛮时期。然而在中世纪这将近一千年中哲学在欧洲仍取得了长足地发展。认为\"在强度、复杂度还有成就上，可以确信地说哲学在十三世纪的兴盛能与公元前四世纪古希腊哲学的黄金时期媲美。\"\n",
      "这个时代讨论的问题有信仰和理智的关系，神的存在与统一，神学话题和形而上学，关于知识、宇宙和个人的问题。\n",
      "中世纪的哲学家包括基督教学者希波的奥古斯丁、波爱修斯、安瑟伦、、皮埃尔·阿伯拉尔、罗吉尔·培根、圣文德、托马斯·阿奎那、邓斯·司各脱、奥卡姆的威廉和让·布里丹等；犹太哲学家迈蒙尼德和吉尔松尼德;还有穆斯林哲学家肯迪、法拉比、海什木、伊本·西那、安萨里、伊本·巴哲、伊本·图费勒、伊本·赫勒敦和伊本·鲁世德等。中世纪的经院哲学传统一直到17世纪仍在活跃，例如弗朗西斯科·苏亚雷斯和等人物。其中托马斯主义之父阿奎那极大地影响了整个天主教欧洲，他特别强调理性和论证，是最先开始使用亚里士多德形而上学和认识论的著作的新译本的学者之一。他的工作明显远离了统治大部分早期经院哲学的新柏拉图主义和奥古斯丁的思想。\n",
      "文艺复兴.\n",
      "从文艺复兴开始，人们的思想开始从清净的僧院走出，来到喧嚣的世界。从而发展自然，也发展人类自身。从而形成人文主义和自然哲学两股既有联系又有区别的想法。\n",
      "\"文艺复兴\"是对中世纪到近代之间过渡时期的通称，那时对古典文献的重新学习帮助把哲学界的兴趣从对逻辑学、形而上学和神学领域的钻研转移到包括道德、语言学和神秘主义的更加广泛的研究。对经典和人文艺术例如历史学和文学的研究在基督教世界学术界中享有前所未有的兴趣，这个趋势被称为人文主义，它受到柏拉图主义、希腊怀疑主义和罗马斯多葛主义的影响。人文主义者的哲学兴趣跟随彼特拉克转移到造物主与其美德上，替代了中世纪时对形而上学和逻辑学的兴趣。\n",
      "那时对古典哲学的研究出现了两种新方式。一方面对亚里士多德的研究因为的影响而产生了变化。阿威罗伊亚里士多德主义者和更正统的天主教亚里士多德主义者譬如艾尔伯图斯·麦格努斯和托马斯·阿奎那之间的分歧最终在文艺复兴发展出一种“人文亚里斯多德哲学”，譬如伯多禄·蓬波纳齐和的思想。另一方面，在一些之前不为西欧所熟知的作品的重发现的帮助下，对柏拉图和新柏拉图主义的研究作为另一个选择变得普遍起来。著名的文艺复兴时期的柏拉图主义者包括库萨的尼古拉，还有之后的马尔西利奥·费奇诺和若望·皮科·德拉·米兰多拉。\n",
      "文艺复兴也重新产生了对反亚里士多德的把自然看作一个有机的、活生生的整体而不取决于神学的理论的兴趣，例如在库萨的尼古拉、尼古拉·哥白尼、焦尔达诺·布鲁诺、波纳蒂特·特勒肖还有托马索·康帕内拉的著作中。在自然哲学中这样的运动与对神秘主义、魔法、赫尔墨斯主义还有占星学等兴趣重燃相契合，它们被认为隐藏着收获知识和掌控自然方法的大门。\n",
      "这些新的哲学运动伴随着欧洲宗教和政治的剧变同时出现：宗教改革和封建制的衰落。虽然参与宗教改革的神学家们对哲学没有直接的兴趣，他们打破了神学和知识权威的传统基础。同时还伴随着信仰主义和怀疑主义的复兴，体现在伊拉斯谟，蒙泰涅和等思想家身上。同时，民族国家政治上逐步的中央集权的过程得到了世俗政治哲学的响应，如尼可罗·马基亚维利（常被描述为第一个现代政治思想家，或者是现代政治思想形成的关键点）、托马斯·莫尔、伊拉斯谟、尤斯图斯·利普修斯、让·博丹和胡果·格老秀斯等的著作。\n",
      "东亚.\n",
      "先秦诸子之后的两汉经学、魏晋玄学等都是中国哲学的一部份，自唐朝起佛教也开始对哲学产生重要影响；不过中世纪中国哲学最主要的部分是宋明理学的发展。\n",
      "宋明理学反对汉代后开始影响儒学的道教和佛教中的迷信和神秘的元素，是一股倡导更加理性和世俗化儒学的哲学运动。尽管理学遭到道教和佛教徒的批评，理学仍借鉴了它们两个的部分术语和概念。然而和佛教和道教把形而上学看作心灵发展、宗教启示的催化剂并且是不朽的不同，宋明理学把形而上学当作建立一个理性的伦理体系的指导。宋明理学的起源可以追溯到唐朝：韩愈和李翱被视为宋代理学的先驱。宋代理学家周敦颐以道教形而上学理论为框架建立了他的伦理哲学体系，他被看作是宋明理学的创始人。\n",
      "在东亚的其他地方，日本哲学形成于本土的神道信仰和佛教、儒家以及另一些中国哲学和印度哲学学派混合发展。与日本类似，在中巫教的情绪化内容被混合到了从中国传入的理学当中。\n",
      "近代哲学（17-19世纪）.\n",
      "主条目：近代哲学\n",
      "早期近代哲学.\n",
      "西方哲学史上的近代早期一般指17世纪和18世纪，其中18世纪常被称为启蒙时代。现代哲学不同于其前身，它和传统权威例如教会、学院、亚里士多德的关系更加独立，出现了对知识基础和形而上学体系建设的新兴趣；和摆脱了自然哲学的近代物理学的出现。从17世纪开始，近代哲学就以认识论为研究重点。由于经验论（经验主义）与唯理论（理性主义）的争论，使物质与精神的关系作为认识论的首要问题突显出来。\n",
      "当时其他的哲学焦点包括精神的天性和其与身体的关系，新的自然科学对诸如自由意志和神的传统上属于神学的话题的影响，和伦理学和政治哲学的世俗基础\n",
      "。这种潮流最早被鲜明地体现在弗兰西斯·培根的被称为用来扩展知识的新的、经验主义的程序，并很快\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(input_ids[idx]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a29a19-4fb7-44a9-ae68-47eff22fa1f2",
   "metadata": {},
   "source": [
    "### 4 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d692b1c-f84a-48ad-9f68-b9695dcfa93f",
   "metadata": {},
   "source": [
    "#### 注: NPU下使用张量并行(model_parallel_size>1)需要Megatron和MindSpeed。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178837a-f241-452f-9a05-7d5b9a124cd3",
   "metadata": {},
   "source": [
    "```shell\n",
    "# 获取megatron\n",
    "git clone https://github.com/NVIDIA/Megatron-LM.git\n",
    "cd Megatron-LM\n",
    "git checkout core_r0.6.0\n",
    "cp -r megatron ../\n",
    "# 安装mindspeed\n",
    "git clone https://gitee.com/ascend/MindSpeed.git\n",
    "cd MindSpeed\n",
    "# checkout commit from MindSpeed core_r0.6.0 in 0923\n",
    "git checkout 4ea42a23 \n",
    "pip install -r requirements.txt \n",
    "pip3 install -e .\n",
    "cd ..\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0f26c72d-502e-4797-ba5f-bdb847b673de",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-27 17:06:32,774] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-27 17:06:34,204] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-11-27 17:06:34,204] [INFO] [runner.py:585:main] cmd = /home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --module --enable_each_rank_log=None jllm.train_pipe --model Qwen2.5-7B-Instruct --num_train_epochs 1 --train_data wikipedia-zh-cn-512_Qwen2.5-7B-Instruct --pipe_parallel_size 2 --model_parallel_size 2 --sequence_parallel_size 2 --per_device_train_batch_size 1 --gradient_accumulation_steps 16 --partition_method fast --output_dir pretrained --max_num_checkpoints 2 --split_dlayer --low_mem --learning_rate 1e-5\n",
      "[2024-11-27 17:06:39,321] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-27 17:06:40,730] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2024-11-27 17:06:40,730] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2024-11-27 17:06:40,730] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2024-11-27 17:06:40,730] [INFO] [launch.py:164:main] dist_world_size=8\n",
      "[2024-11-27 17:06:40,730] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "[2024-11-27 17:06:40,753] [INFO] [launch.py:256:main] process 2241624 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=0', '--model', 'Qwen2.5-7B-Instruct', '--num_train_epochs', '1', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--sequence_parallel_size', '2', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--output_dir', 'pretrained', '--max_num_checkpoints', '2', '--split_dlayer', '--low_mem', '--learning_rate', '1e-5']\n",
      "[2024-11-27 17:06:40,770] [INFO] [launch.py:256:main] process 2241625 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=1', '--model', 'Qwen2.5-7B-Instruct', '--num_train_epochs', '1', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--sequence_parallel_size', '2', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--output_dir', 'pretrained', '--max_num_checkpoints', '2', '--split_dlayer', '--low_mem', '--learning_rate', '1e-5']\n",
      "[2024-11-27 17:06:40,786] [INFO] [launch.py:256:main] process 2241626 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=2', '--model', 'Qwen2.5-7B-Instruct', '--num_train_epochs', '1', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--sequence_parallel_size', '2', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--output_dir', 'pretrained', '--max_num_checkpoints', '2', '--split_dlayer', '--low_mem', '--learning_rate', '1e-5']\n",
      "[2024-11-27 17:06:40,803] [INFO] [launch.py:256:main] process 2241627 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=3', '--model', 'Qwen2.5-7B-Instruct', '--num_train_epochs', '1', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--sequence_parallel_size', '2', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--output_dir', 'pretrained', '--max_num_checkpoints', '2', '--split_dlayer', '--low_mem', '--learning_rate', '1e-5']\n",
      "[2024-11-27 17:06:40,821] [INFO] [launch.py:256:main] process 2241628 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=4', '--model', 'Qwen2.5-7B-Instruct', '--num_train_epochs', '1', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--sequence_parallel_size', '2', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--output_dir', 'pretrained', '--max_num_checkpoints', '2', '--split_dlayer', '--low_mem', '--learning_rate', '1e-5']\n",
      "[2024-11-27 17:06:40,841] [INFO] [launch.py:256:main] process 2241629 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=5', '--model', 'Qwen2.5-7B-Instruct', '--num_train_epochs', '1', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--sequence_parallel_size', '2', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--output_dir', 'pretrained', '--max_num_checkpoints', '2', '--split_dlayer', '--low_mem', '--learning_rate', '1e-5']\n",
      "[2024-11-27 17:06:40,864] [INFO] [launch.py:256:main] process 2241630 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=6', '--model', 'Qwen2.5-7B-Instruct', '--num_train_epochs', '1', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--sequence_parallel_size', '2', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--output_dir', 'pretrained', '--max_num_checkpoints', '2', '--split_dlayer', '--low_mem', '--learning_rate', '1e-5']\n",
      "[2024-11-27 17:06:40,888] [INFO] [launch.py:256:main] process 2241631 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=7', '--model', 'Qwen2.5-7B-Instruct', '--num_train_epochs', '1', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--sequence_parallel_size', '2', '--per_device_train_batch_size', '1', '--gradient_accumulation_steps', '16', '--partition_method', 'fast', '--output_dir', 'pretrained', '--max_num_checkpoints', '2', '--split_dlayer', '--low_mem', '--learning_rate', '1e-5']\n",
      "[2024-11-27 17:06:46,506] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-27 17:06:46,513] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-27 17:06:46,530] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-27 17:06:46,621] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "[2024-11-27 17:06:46,722] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-27 17:06:46,778] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-27 17:06:46,779] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-27 17:06:46,812] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:209: ImportWarning: \n",
      "    *************************************************************************************************************\n",
      "    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..\n",
      "    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..\n",
      "    The backend in torch.distributed.init_process_group set to hccl now..\n",
      "    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..\n",
      "    The device parameters have been replaced with npu in the function below:\n",
      "    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty\n",
      "    *************************************************************************************************************\n",
      "    \n",
      "  warnings.warn(msg, ImportWarning)\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "[2024-11-27 17:07:03,507] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-27 17:07:03,858] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-27 17:07:04,117] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-27 17:07:04,176] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-27 17:07:04,368] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-27 17:07:04,381] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-27 17:07:04,577] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-27 17:07:04,691] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-27 17:07:04,691] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl\n",
      "/mnt/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:309: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "[2024-11-27 17:07:12,736] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-27 17:07:12,806] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-27 17:07:12,829] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-27 17:07:12,905] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-27 17:07:12,906] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-27 17:07:12,907] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(data=0, pipe=0, model=0): 0, ProcessCoord(data=0, pipe=0, model=1): 1, ProcessCoord(data=0, pipe=1, model=0): 2, ProcessCoord(data=0, pipe=1, model=1): 3, ProcessCoord(data=1, pipe=0, model=0): 4, ProcessCoord(data=1, pipe=0, model=1): 5, ProcessCoord(data=1, pipe=1, model=0): 6, ProcessCoord(data=1, pipe=1, model=1): 7}\n",
      "[2024-11-27 17:07:12,912] [INFO] [train_pipe.py:484:custom_partition_layers] Partitioning pipeline stages with method 0, 1, 2\n",
      "stage=0 layers=1\n",
      "     0: ParallelStartPipe 29\n",
      "stage=1 layers=1\n",
      "     1: ParallelEndPipe 27\n",
      "  loss: SeqParallelCrossEntropy\n",
      "[2024-11-27 17:07:12,971] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-27 17:07:13,264] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:456: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//(args.data_parallel_size//args.sequence_parallel_size)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-27 17:07:13,906] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:456: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//(args.data_parallel_size//args.sequence_parallel_size)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-27 17:07:14,012] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:456: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//(args.data_parallel_size//args.sequence_parallel_size)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:456: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//(args.data_parallel_size//args.sequence_parallel_size)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-27 17:07:14,021] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-27 17:07:14,022] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-27 17:07:14,022] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:456: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//(args.data_parallel_size//args.sequence_parallel_size)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-27 17:07:14,133] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:456: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//(args.data_parallel_size//args.sequence_parallel_size)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-27 17:07:14,158] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:456: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//(args.data_parallel_size//args.sequence_parallel_size)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-27 17:07:14,173] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:456: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//(args.data_parallel_size//args.sequence_parallel_size)\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-27 17:07:14,274] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-27 17:07:14,289] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-27 17:07:14,390] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-27 17:07:14,408] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-27 17:07:14,436] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-27 17:07:14,438] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-27 17:07:14,447] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-27 17:07:14,485] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-27 17:07:14,486] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-27 17:07:14,486] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-27 17:07:14,490] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2024-11-27 17:07:14,490] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer\n",
      "[2024-11-27 17:07:14,552] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-27 17:07:14,799] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer\n",
      "[2024-11-27 17:07:14,800] [INFO] [utils.py:782:see_memory_usage] MA 3.63 GB         Max_MA 4.7 GB         CA 5.06 GB         Max_CA 5 GB \n",
      "[2024-11-27 17:07:14,800] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.46 GB, percent = 6.0%\n",
      "[2024-11-27 17:07:15,057] [INFO] [utils.py:781:see_memory_usage] before initializing group 0\n",
      "[2024-11-27 17:07:15,059] [INFO] [utils.py:782:see_memory_usage] MA 3.63 GB         Max_MA 3.63 GB         CA 5.06 GB         Max_CA 5 GB \n",
      "[2024-11-27 17:07:15,059] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.45 GB, percent = 6.0%\n",
      "[2024-11-27 17:07:15,336] [INFO] [utils.py:781:see_memory_usage] after initializing group 0\n",
      "[2024-11-27 17:07:15,337] [INFO] [utils.py:782:see_memory_usage] MA 14.34 GB         Max_MA 14.34 GB         CA 22.93 GB         Max_CA 23 GB \n",
      "[2024-11-27 17:07:15,337] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.45 GB, percent = 6.0%\n",
      "[2024-11-27 17:07:15,598] [INFO] [utils.py:781:see_memory_usage] before initializing group 1\n",
      "[2024-11-27 17:07:15,599] [INFO] [utils.py:782:see_memory_usage] MA 14.34 GB         Max_MA 14.34 GB         CA 22.93 GB         Max_CA 23 GB \n",
      "[2024-11-27 17:07:15,600] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.45 GB, percent = 6.0%\n",
      "[2024-11-27 17:07:15,859] [INFO] [utils.py:781:see_memory_usage] after initializing group 1\n",
      "[2024-11-27 17:07:15,861] [INFO] [utils.py:782:see_memory_usage] MA 14.34 GB         Max_MA 14.34 GB         CA 22.93 GB         Max_CA 23 GB \n",
      "[2024-11-27 17:07:15,861] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.45 GB, percent = 6.0%\n",
      "[2024-11-27 17:07:16,121] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer\n",
      "[2024-11-27 17:07:16,122] [INFO] [utils.py:782:see_memory_usage] MA 14.34 GB         Max_MA 14.34 GB         CA 22.93 GB         Max_CA 23 GB \n",
      "[2024-11-27 17:07:16,123] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.44 GB, percent = 6.0%\n",
      "[2024-11-27 17:07:16,123] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = BF16_Optimizer\n",
      "[2024-11-27 17:07:16,123] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-27 17:07:16,123] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0xfffeed476f40>\n",
      "[2024-11-27 17:07:16,123] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[1e-05, 1e-05], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2024-11-27 17:07:16,124] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2024-11-27 17:07:16,124] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xfffeed4f6490>\n",
      "[2024-11-27 17:07:16,125] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-27 17:07:16,126] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 8\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   loss_scale ................... 1.0\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-27 17:07:16,127] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   steps_per_print .............. 1\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   train_batch_size ............. 16\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-27 17:07:16,128] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-27 17:07:16,129] [INFO] [config.py:1003:print]   zero_enabled ................. False\n",
      "[2024-11-27 17:07:16,129] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-27 17:07:16,129] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0\n",
      "[2024-11-27 17:07:16,129] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": 1, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": false, \n",
      "        \"cpu_checkpointing\": false, \n",
      "        \"contiguous_memory_optimization\": false, \n",
      "        \"number_checkpoints\": null, \n",
      "        \"synchronize_checkpoint_boundary\": false, \n",
      "        \"profile\": false\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }\n",
      "}\n",
      "[2024-11-27 17:07:16,129] [INFO] [engine.py:105:__init__] CONFIG: micro_batches=8 micro_batch_size=1\n",
      "[2024-11-27 17:07:16,129] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-27 17:07:16,676] [INFO] [engine.py:165:__init__] RANK=0 STAGE=0 LAYERS=1 [0, 1) STAGE_PARAMS=1918639360 (1918.639M) TOTAL_PARAMS=7615820800 (7615.821M) UNIQUE_PARAMS=7615820800 (7615.821M)\n",
      "[2024-11-27 17:07:16,677] [INFO] [engine.py:165:__init__] RANK=3 STAGE=1 LAYERS=1 [1, 2) STAGE_PARAMS=1889271040 (1889.271M) TOTAL_PARAMS=7615820800 (7615.821M) UNIQUE_PARAMS=7615820800 (7615.821M)\n",
      "[2024-11-27 17:07:16,677] [INFO] [engine.py:165:__init__] RANK=2 STAGE=1 LAYERS=1 [1, 2) STAGE_PARAMS=1889271040 (1889.271M) TOTAL_PARAMS=7615820800 (7615.821M) UNIQUE_PARAMS=7615820800 (7615.821M)\n",
      "[2024-11-27 17:07:16,677] [INFO] [engine.py:165:__init__] RANK=1 STAGE=0 LAYERS=1 [0, 1) STAGE_PARAMS=1918639360 (1918.639M) TOTAL_PARAMS=7615820800 (7615.821M) UNIQUE_PARAMS=7615820800 (7615.821M)\n",
      "Namespace(local_rank=0, model='Qwen2.5-7B-Instruct', train_data='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', eval_data='', from_ckpt='', resume_ckpt='', only_load_model=False, tag=None, ds_config=None, zero_stage=0, split_dlayer=True, num_layers_per_decoder=None, emb_partitions=1, timeout=1800, pipe_parallel_size=2, encoder_pipe_parallel_size=0, model_parallel_size=2, sequence_parallel_size=2, offload=False, partition_method='fast', multi_layerspec=False, num_train_epochs=1, per_device_train_batch_size=1, gradient_accumulation_steps=16, weight_decay=0.0, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=0, learning_rate=1e-05, seq_len=8193, block_mask=False, skip_train_steps=set(), steps_per_print=1, steps_per_eval=-1, steps_per_checkpoint=-1, checkpoint='', background_executor='process', ckpt_step_gt=0, best_of=1, ckpt_epoch=set(), skip_epoch=set(), max_num_checkpoints=2, only_ckpt_model=False, only_ckpt_lora=False, only_cache_model=False, early_stop=-1, checkpoint_grad_interval=0, no_checkpoint_grad_step=1000000, low_mem=True, no_shuf=False, no_safetensor=False, init=False, cache_model=None, seed=1234, output_dir='pretrained', lora_dim=0, lora_alpha=1, lora_module_name='qkv_proj,o_proj,gate_up_proj,down_proj', only_optimize_lora=False, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None, device='npu', global_rank=0, world_size=8, data_parallel_size=2, num_training_steps=16, block_class_id=-100)\n",
      "Data Reading Info:\n",
      "Rank\tSamples\tLength\tReadTime\n",
      "INFO:numexpr.utils:Note: detected 256 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 256 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "INFO:numexpr.utils:Note: detected 256 virtual cores but NumExpr set to maximum of 64, check \"NUMEXPR_MAX_THREADS\" environment variable.\n",
      "INFO:numexpr.utils:Note: NumExpr detected 256 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n",
      "0\t258\t8193\t1\n",
      "4\t258\t8193\t1\n",
      "Beginning of Epoch: 1/1\n",
      "Partition Rank: 1/1\n",
      "Partition Name: wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/wikipedia-zh-cn-512-00000\n",
      "Total Partition Steps: 16/16\n",
      "/mnt/jllm/core/tensor_parallel/cross_entropy.py:36: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at torch_npu/csrc/aten/common/TensorFactories.cpp:74.)\n",
      "  masked_target[target_mask] = 0\n",
      "/mnt/jllm/core/tensor_parallel/cross_entropy.py:36: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at torch_npu/csrc/aten/common/TensorFactories.cpp:74.)\n",
      "  masked_target[target_mask] = 0\n",
      "/mnt/jllm/core/tensor_parallel/cross_entropy.py:36: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at torch_npu/csrc/aten/common/TensorFactories.cpp:74.)\n",
      "  masked_target[target_mask] = 0\n",
      "/mnt/jllm/core/tensor_parallel/cross_entropy.py:36: UserWarning: AutoNonVariableTypeMode is deprecated and will be removed in 1.10 release. For kernel implementations please use AutoDispatchBelowADInplaceOrView instead, If you are looking for a user facing API to enable running your inference-only workload, please use c10::InferenceMode. Using AutoDispatchBelowADInplaceOrView in user code is under risk of producing silent wrong result in some edge cases. See Note [AutoDispatchBelowAutograd] for more details. (Triggered internally at torch_npu/csrc/aten/common/TensorFactories.cpp:74.)\n",
      "  masked_target[target_mask] = 0\n",
      "[2024-11-27 17:07:35,279] [INFO] [logging.py:96:log_dist] [Rank 0] step=1, skipped=0, lr=[9.903926402016153e-06, 9.903926402016153e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 1 loss: 2.5000 iter time (s): 13.255 samples/sec: 1.207\n",
      "[2024-11-27 17:07:41,473] [INFO] [logging.py:96:log_dist] [Rank 0] step=2, skipped=0, lr=[9.619397662556434e-06, 9.619397662556434e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 2 loss: 2.4688 iter time (s): 5.732 samples/sec: 2.791\n",
      "[2024-11-27 17:07:47,258] [INFO] [logging.py:96:log_dist] [Rank 0] step=3, skipped=0, lr=[9.157348061512728e-06, 9.157348061512728e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 3 loss: 2.4844 iter time (s): 5.779 samples/sec: 2.769\n",
      "[2024-11-27 17:07:52,964] [INFO] [logging.py:96:log_dist] [Rank 0] step=4, skipped=0, lr=[8.535533905932739e-06, 8.535533905932739e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 4 loss: 2.3438 iter time (s): 5.705 samples/sec: 2.805\n",
      "[2024-11-27 17:07:58,637] [INFO] [logging.py:96:log_dist] [Rank 0] step=5, skipped=0, lr=[7.777851165098012e-06, 7.777851165098012e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 5 loss: 2.9219 iter time (s): 5.681 samples/sec: 2.817\n",
      "[2024-11-27 17:08:04,504] [INFO] [logging.py:96:log_dist] [Rank 0] step=6, skipped=0, lr=[6.913417161825449e-06, 6.913417161825449e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 6 loss: 2.5000 iter time (s): 5.856 samples/sec: 2.732\n",
      "[2024-11-27 17:08:10,257] [INFO] [logging.py:96:log_dist] [Rank 0] step=7, skipped=0, lr=[5.975451610080643e-06, 5.975451610080643e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 7 loss: 2.4219 iter time (s): 5.751 samples/sec: 2.782\n",
      "[2024-11-27 17:08:16,093] [INFO] [logging.py:96:log_dist] [Rank 0] step=8, skipped=0, lr=[5e-06, 5e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 8 loss: 2.4375 iter time (s): 5.836 samples/sec: 2.742\n",
      "[2024-11-27 17:08:21,876] [INFO] [logging.py:96:log_dist] [Rank 0] step=9, skipped=0, lr=[4.02454838991936e-06, 4.02454838991936e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 9 loss: 2.1875 iter time (s): 5.781 samples/sec: 2.768\n",
      "[2024-11-27 17:08:27,683] [INFO] [logging.py:96:log_dist] [Rank 0] step=10, skipped=0, lr=[3.0865828381745515e-06, 3.0865828381745515e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 10 loss: 2.2969 iter time (s): 5.806 samples/sec: 2.756\n",
      "[2024-11-27 17:08:33,477] [INFO] [logging.py:96:log_dist] [Rank 0] step=11, skipped=0, lr=[2.2221488349019903e-06, 2.2221488349019903e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 11 loss: 2.2344 iter time (s): 5.792 samples/sec: 2.762\n",
      "[2024-11-27 17:08:39,293] [INFO] [logging.py:96:log_dist] [Rank 0] step=12, skipped=0, lr=[1.4644660940672628e-06, 1.4644660940672628e-06], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 12 loss: 2.4375 iter time (s): 5.814 samples/sec: 2.752\n",
      "[2024-11-27 17:08:45,208] [INFO] [logging.py:96:log_dist] [Rank 0] step=13, skipped=0, lr=[8.426519384872733e-07, 8.426519384872733e-07], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 13 loss: 2.2969 iter time (s): 5.917 samples/sec: 2.704\n",
      "[2024-11-27 17:08:51,955] [INFO] [logging.py:96:log_dist] [Rank 0] step=14, skipped=0, lr=[3.8060233744356634e-07, 3.8060233744356634e-07], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 14 loss: 2.4219 iter time (s): 6.744 samples/sec: 2.373\n",
      "[2024-11-27 17:08:57,727] [INFO] [logging.py:96:log_dist] [Rank 0] step=15, skipped=0, lr=[9.607359798384785e-08, 9.607359798384785e-08], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 15 loss: 2.2969 iter time (s): 5.773 samples/sec: 2.772\n",
      "[2024-11-27 17:09:03,701] [INFO] [logging.py:96:log_dist] [Rank 0] step=16, skipped=0, lr=[0.0, 0.0], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "steps: 16 loss: 2.4062 iter time (s): 5.968 samples/sec: 2.681\n",
      "Wash the memory of train data clean for 1 seconds ......\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "Rank 7: saving the final model to pretrained: tensor-02-of-02-pipeline-02-of-02.safetensors...\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.91it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.95it/s]\n",
      "Rank 2: saving the final model to pretrained: tensor-01-of-02-pipeline-02-of-02.safetensors...\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      " 70%|██████████████████████████████▊             | 7/10 [00:00<00:00,  9.97it/s]Rank 5: saving the final model to pretrained: tensor-02-of-02-pipeline-01-of-02.safetensors...\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.96it/s]\n",
      "100%|███████████████████████████████████████████| 10/10 [00:01<00:00,  9.97it/s]\n",
      "Rank 0: saving the final model to pretrained: tensor-01-of-02-pipeline-01-of-02.safetensors...\n",
      "Rank 7: saved the final model to pretrained: tensor-02-of-02-pipeline-02-of-02.safetensors.\n",
      "Rank 2: saved the final model to pretrained: tensor-01-of-02-pipeline-02-of-02.safetensors.\n",
      "Rank 5: saved the final model to pretrained: tensor-02-of-02-pipeline-01-of-02.safetensors.\n",
      "Rank 0: saved the final model to pretrained: tensor-01-of-02-pipeline-01-of-02.safetensors.\n",
      "[2024-11-27 17:09:16,142] [INFO] [launch.py:351:main] Process 2241630 exits successfully.\n",
      "[2024-11-27 17:09:17,144] [INFO] [launch.py:351:main] Process 2241627 exits successfully.\n",
      "[2024-11-27 17:09:17,144] [INFO] [launch.py:351:main] Process 2241625 exits successfully.\n",
      "[2024-11-27 17:09:18,145] [INFO] [launch.py:351:main] Process 2241628 exits successfully.\n",
      "[2024-11-27 17:09:28,156] [INFO] [launch.py:351:main] Process 2241626 exits successfully.\n",
      "[2024-11-27 17:09:32,160] [INFO] [launch.py:351:main] Process 2241631 exits successfully.\n",
      "[2024-11-27 17:09:37,166] [INFO] [launch.py:351:main] Process 2241629 exits successfully.\n",
      "[2024-11-27 17:09:41,170] [INFO] [launch.py:351:main] Process 2241624 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --module jllm.train_pipe \\\n",
    "    --model Qwen2.5-7B-Instruct \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --train_data wikipedia-zh-cn-512_Qwen2.5-7B-Instruct \\\n",
    "    --pipe_parallel_size 2 \\\n",
    "    --model_parallel_size 2 \\\n",
    "    --sequence_parallel_size 2 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --partition_method fast \\\n",
    "    --output_dir pretrained \\\n",
    "    --max_num_checkpoints 2 \\\n",
    "    --split_dlayer \\\n",
    "    --low_mem \\\n",
    "    --learning_rate 1e-5 |tee pretrain.log\n",
    "#注释：\n",
    "# --model 模型路径至少需要包含config.json\n",
    "# --num_train_epochs 训练轮数\n",
    "# --train_data 训练数据\n",
    "# --pipe_parallel_size 流水线并行个数\n",
    "# --model_parallel_size 张量并行个数\n",
    "# --per_device_train_batch_size 一次训练多少样本\n",
    "# --gradient_accumulation_steps 训练完多少样本后（累加完多少个梯度后）进行一次参数更新\n",
    "# --partition_method fast 流水线拆分策略\n",
    "# --checkpoint checkpoint 模型检查点目录\n",
    "# --output_dir pretrained 最终模型输出目录\n",
    "# --max_num_checkpoints 2 最大保留多少个检查点\n",
    "# --split_dlayer 是否拆分docoder layer\n",
    "# --low_mem 是否使用低内存读取数据\n",
    "# --learning_rate 1e-5 学习率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51a7f04f-55e3-439d-a945-1b0ccda87d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAABaCklEQVR4nO3deXhU5fUH8O+dmWQmy2RfyAZZ2AkCQkAIAlYWl6poa91xt9hgxbbW8lPqUjXVWlu1CmJRbClFq+KCCqKQIJtsIoQlkI2E7CFksk8mM/f3x+ROEiHLTGbm3pl8P88zz9OEe2fOFMycvO855xVEURRBREREJBOV3AEQERHR4MZkhIiIiGTFZISIiIhkxWSEiIiIZMVkhIiIiGTFZISIiIhkxWSEiIiIZMVkhIiIiGSlkTuA/rBYLCgrK4Ner4cgCHKHQ0RERP0giiIaGhoQGxsLlarn9Q+PSEbKysqQkJAgdxhERETkgJKSEsTHx/f45x6RjOj1egDWNxMUFCRzNERERNQf9fX1SEhIsH2O98QjkhFpayYoKIjJCBERkYfpq8SCBaxEREQkKyYjREREJCsmI0RERCQrJiNEREQkKyYjREREJCsmI0RERCQrJiNEREQkKyYjREREJCsmI0RERCQrJiNEREQkKyYjREREJCsmI0RERCQrJiPktWoajXh9Wx6qGlrlDoWIiHrBZIS81pqdRfjL5ly8tb1A7lCIiKgXTEbIaxWdbQIAnKhokDkSIiLqDZMR8lrlBuv2TEF1k8yREBFRb5iMkNcqr2sBAJTWtaClzSxzNERE1BMmI+SV2s0WVDYYbV8X1DTKGA0REfWGyQh5paoGI8wW0fZ1PrdqiIgUy65kJDMzE2lpadDr9YiKisLChQuRm5vb6z0mkwnPPPMMUlJSoNPpMGHCBGzatGlAQRP1pdzQ0u3r/CqujBARKZVdyUh2djYyMjKwZ88ebNmyBSaTCfPnz0dTU8+/dT7xxBN488038dprr+HYsWNYvHgxrr/+enz//fcDDp6oJ2V13WeL5FczGSEiUipBFEWx78surLq6GlFRUcjOzsasWbMueE1sbCwef/xxZGRk2L73s5/9DH5+fli7dm2/Xqe+vh7BwcEwGAwICgpyNFwaRFZtz8fzX5xAkE6D+tZ2jIkJwpcPXyp3WEREg0p/P78HVDNiMBgAAGFhYT1eYzQaodPpun3Pz88PO3bs6PWe+vr6bg8ie0grI+nDIwAABdWNsFgczruJiMiFHE5GLBYLli5divT0dKSmpvZ43YIFC/Dyyy/j1KlTsFgs2LJlCz766COUl5f3eE9mZiaCg4Ntj4SEBEfDpEFKqhmZmhQGX7UKxnYLSuta+riLiIjk4HAykpGRgZycHKxfv77X61555RWMGDECo0ePhq+vL5YsWYK7774bKlXPL71s2TIYDAbbo6SkxNEwaZCSBp7Fh/ojMcIfAFBQw44aIiIlcigZWbJkCTZu3Iht27YhPj6+12sjIyPx8ccfo6mpCadPn8aJEycQGBiI5OTkHu/RarUICgrq9iCyh7RNExOsQ0pkIAB21BARKZVdyYgoiliyZAk2bNiArVu3Iikpqd/36nQ6xMXFob29HR9++CGuu+46u4Ml6g9juxk1jdaBZ7Ehfp3JCDtqiIgUSWPPxRkZGVi3bh0++eQT6PV6VFRUAACCg4Ph5+cHAFi0aBHi4uKQmZkJAPjuu+9QWlqKiRMnorS0FE899RQsFgt+//vfO/mtEFlVGqyJiFajQqi/D5IjAwAwGSEiUiq7kpEVK1YAAObMmdPt+++88w7uuusuAEBxcXG3epDW1lY88cQTKCgoQGBgIK666ir8+9//RkhIyIACJ+pJWUfxamyIHwRB6LIywpoRIiIlsisZ6c9IkqysrG5fz549G8eOHbMrKKKBKOvomokJtraUSysj1Q1GGFpMCPbzkS02IiI6H8+mIa8jddLEBFu3DvU6H0QHaQFY540QEZGyMBkhryOtjMSGdA7b41YNEZFyMRkhr/PjlRGgMxnhyggRkfIwGSGvc+GVEXbUEBEpFZMR8jrSykhsSOfKSDK3aYiIFIvJCHmV5rZ2GFpMADq7aQAgJcqajJw+2wST2SJLbEREdGFMRsirSGPg9VoN9LrOFt6YIB38fNQwmUWU1DbLFR4REV0AkxHyKtJpvTFd6kUAQKUSukxi5VYNEZGSMBkhr1Jed34njYRn1BARKROTEfIqpRfopJGwvZeISJmYjJBXsW3TXGhlJIrbNERESsRkhLxK58CznldG8qoa+3XOEhERuQeTEfIqnQPPzl8ZSYoIgCAAhhYTapva3B0aERH1gMkIeQ1RFC848Eyi81EjruP73KohIlIOJiPkNepb2tHcZgZw4W0agB01RERKxGSEvEZZR/FqWIAvdD7qC15jS0aqmIwQESkFkxHyGp2dNBdeFQG6dtQwGSEiUgomI+Q1ynoZeCaxzRqpYc0IEZFSMBkhryGtjFxo4JlESkZKapvRajK7JS4iIuodkxHyGv1ZGYkI9IVep4FFBE6f5YF5RERKwGSEvEZZL6PgJYIgsKOGiEhhmIyQ1+icvtrzygjAjhoiIqVhMkJewWIRUdHLKPiu2FFDRKQsTEbIK5xtakOb2QJBAIb0lYzYtmnYUUNEpARMRsgrSJ00UXotfNS9/7O2tfdW88A8IiIlYDJCXqE/nTSSYeH+0KgENLWZUVlvdHVoRETUByYj5BX6M2NE4qNWYWi4PwDWjRARKQGTEfIK/e2kkSRHsL2XiEgpmIyQV5BmjPTVSSOxddSwvZeISHZMRsgrSCsjsSH9WxlhRw0RkXIwGSGvYPfKCKewEhEpBpMR8njtZgsq6+1dGbFu05QbWtFkbHdZbERE1DcmI+TxqhqMsIiARiUgIlDbr3tC/H0REegLACis4VYNEZGcmIyQx5PaeqODdFCrhH7fl8ytGiIiRWAyQh5PGngW188tGgkPzCMiUga7kpHMzEykpaVBr9cjKioKCxcuRG5ubp/3/f3vf8eoUaPg5+eHhIQEPPLII2htbXU4aKKupJWRmH4MPOtKqhthRw0RkbzsSkays7ORkZGBPXv2YMuWLTCZTJg/fz6amnr+Yb5u3Tr84Q9/wJNPPonjx49j9erVeO+99/B///d/Aw6eCLBvFHxX7KghIlIGjT0Xb9q0qdvXa9asQVRUFA4cOIBZs2Zd8J5du3YhPT0dt956KwAgMTERt9xyC7777jsHQybqzp5R8F3ZDsyraYLZItpVb0JERM4zoJoRg8EAAAgLC+vxmhkzZuDAgQPYu3cvAKCgoABffPEFrrrqqh7vMRqNqK+v7/Yg6om9o+AlcaF+8NWo0NZuQem5FleERkRE/WDXykhXFosFS5cuRXp6OlJTU3u87tZbb0VNTQ1mzpwJURTR3t6OxYsX97pNk5mZiaefftrR0GiQ6dymsW9lRK0SkBwRgBMVDcivabQdnkdERO7l8MpIRkYGcnJysH79+l6vy8rKwvPPP4833ngDBw8exEcffYTPP/8cf/rTn3q8Z9myZTAYDLZHSUmJo2GSlzO2m1HTaATQ/4FnXbGjhohIfg6tjCxZsgQbN27E9u3bER8f3+u1y5cvxx133IH77rsPADB+/Hg0NTXhgQcewOOPPw6V6vx8SKvVQqvt3/AqGtwqOrZotBoVQv197L6fHTVERPKzKxkRRREPPfQQNmzYgKysLCQlJfV5T3Nz83kJh1qttj0f0UBIWzSxIX4QBPsLUDn4jIhIfnYlIxkZGVi3bh0++eQT6PV6VFRUAACCg4Ph52ddIl+0aBHi4uKQmZkJALjmmmvw8ssvY9KkSZg2bRry8vKwfPlyXHPNNbakhMhRthkjdtaLSGwdNUxGiIhkY1cysmLFCgDAnDlzun3/nXfewV133QUAKC4u7rYS8sQTT0AQBDzxxBMoLS1FZGQkrrnmGjz33HMDi5wInZ00jtSLAEByxzZNTWMb6prbEOLv67TYiIiof+zepulLVlZW9xfQaPDkk0/iySeftCswov4oq+uYMeLgykiAVoOYYB3KDa3Ir27C5GFMRoiI3I1n05BHs80YcXBlBOAkViIiuTEZIY8mrYw4WjMCdHbUFLCjhohIFkxGyKMNtGYEAFKiuDJCRCQnJiPksZrb2mFoMQEY6MoIkxEiIjkxGSGPJc0Y0Ws10OvsH3gmkTpqis82w2S2OCU2IiLqPyYj5LFs9SJ2ntb7Y0OCdPD3VaPdIuL02WZnhEZERHZgMkIeq3PgmeP1IgAgCAK3aoiIZMRkhDxW5yj4ga2MAF3PqGEyQkTkbkxGyGM5a2UE6DoWnu29RETuxmSEPJYz2nolbO8lIpIPkxHyWAMdBd+VrWakqpGnSRMRuRmTEfJIoig6ZRS8ZFi4P1QCUN/ajprGtgE/HxER9R+TEfJI9S3taG4zAxjYwDOJzkeN+FB/ANyqISJyNyYj5JHKOopXwwJ8ofNRO+U52VFDRCQPJiPkkTo7aQa+KiLprBthRw0RkTsxGSGPVNoxY8QZbb0SdtQQEcmDyQh5pHKpk8YJA88ktlkjNUxGiIjcickIeSRbJ40zV0Y6akbOnGtBq8nstOclIqLeMRkhj1TmgpWRsABfhPj7QBSBwhrWjRARuQuTEfJIzpy+KhEEAckR7KghInI3JiPkcSwWERW2bRrnrYwA7KghIpIDkxHyOGeb2tBmtkAQgOggJycj7KghInI7JiPkcaQZI1F6LXzUzv0nbFsZYTJCROQ2TEbI45S5YMaIROqoKahugsXCA/OIiNyByQh5HGllxJmdNJKEMH/4qAW0mMyoqG91+vMTEdH5mIyQx5Hael2xMuKjVmFYODtqiIjcickIeZwyF3XSSGwH5lUxGSEicgcmI+RxOkfBO39lBACSbUWsbO8lInIHJiPkccpdvjLCjhoiIndiMkIepd1sQWW986evdmXbpmEyQkTkFkxGyKNUNRhhEQEftYDIQK1LXkPapqmsN6Kh1eSS1yAiok5MRsijSG290UE6qFSCS14j2M8HkXprosMD84iIXI/JCHkUaeBZrAvaerviVg0RkfswGSGPIq2MxLhg4FlXPDCPiMh9mIyQR3HlKPiuktlRQ0TkNnYlI5mZmUhLS4Ner0dUVBQWLlyI3NzcXu+ZM2cOBEE473H11VcPKHAanFw5Cr4rbtMQEbmPXclIdnY2MjIysGfPHmzZsgUmkwnz589HU1PPS9kfffQRysvLbY+cnByo1WrceOONAw6eBh93rYxI2zRFNc1oN1tc+lpERIOdxp6LN23a1O3rNWvWICoqCgcOHMCsWbMueE9YWFi3r9evXw9/f38mI+QQW82IiwaeSeJC/KDVqGBst+DMuRYkRgS49PWIiAazAdWMGAwGAOcnHL1ZvXo1br75ZgQE9PzD3Wg0or6+vtuDyNhuRk1jGwDXDTyTqFQC60aIiNzE4WTEYrFg6dKlSE9PR2pqar/u2bt3L3JycnDffff1el1mZiaCg4Ntj4SEBEfDJC9S0TEGXqtRIdTfx+WvJ9WNFPCMGiIil3I4GcnIyEBOTg7Wr1/f73tWr16N8ePHY+rUqb1et2zZMhgMBtujpKTE0TDJi0j1InEhfhAE1ww864pn1BARuYddNSOSJUuWYOPGjdi+fTvi4+P7dU9TUxPWr1+PZ555ps9rtVottFrXjPomz+WuGSOSlCgmI0RE7mBXMiKKIh566CFs2LABWVlZSEpK6ve9//vf/2A0GnH77bfbHSQR0PW0XtfWi0iSI6T2Xm7TEBG5kl3bNBkZGVi7di3WrVsHvV6PiooKVFRUoKWlxXbNokWLsGzZsvPuXb16NRYuXIjw8PCBR02DUlldx4wRF3fSSJI7akZqm9pQ29TmltckIhqM7FoZWbFiBQDrILOu3nnnHdx1110AgOLiYqhU3XOc3Nxc7NixA1999ZXjkdKgZ1sZcXEnjcTfV4O4ED+U1rWgoLoRYQH97xojIqL+s3ubpi9ZWVnnfW/UqFH9upeoN9LKiKtnjHSVHBmA0roW5Fc3YkoikxEiIlfg2TTkMWzbNG5aGQE6O2rY3ktE5DpMRsgjNBnbUd/aDsC9KyPsqCEicj0mI+QRpLZevVYDvc71A88knQfmcWWEiMhVmIyQR7AdkOemGSOS4R3bNMW1zTC2m9362kREgwWTEfIInQfkua9eBAAi9VoEajUwW0QUn21262sTEQ0WTEbII0grI+4sXgUAQRC6bNWwboSIyBWYjJBHkFZG3DXwrKvOM2pYN0JE5ApMRsgjuHvgWVe2jpoqrowQEbkCkxHyCO4eBd+VbZumhisjRESuwGSEFE8URXlXRqTBZ1WNnCRMROQCTEZI8epb2tHcZm2rdefAM8nQcH+oVQIajO2objC6/fWJiLwdkxFSvNKOLZqwAF/ofNRuf32tRo2EUOuKTB47aoiInI7JCCle54wR96+KSNhRQ0TkOkxGSPHKpHoRNw8864odNURErsNkhBSv3HZar5wrIxx8RkTkKkxGSPGkThp3T1/tytZRw20aIiKnYzJCiifNGFFCzUhpXQta2nhgHhGRMzEZIcVTwspIaIAvwgJ8AQAFNdyqISJyJiYjpGgWi4gKWwGrfCsjQNe6EW7VEBE5E5MRUrSzTW1oM1sgCEB0kLzJSHIEO2qIiFyByQgpmjRjJEqvhY9a3n+uKVHsqCEicgUmI6RoncWr8tWLSDj4jIjINZiMkKKV1UnFq/Ju0QCdyUhhTSMsFh6YR0TkLExGSNE6R8HLvzISH+oHX7UKrSYLyjriIiKigWMyQopWppBOGgDQqFVIjPAHwK0aIiJnYjJCiiaNgo+TccZIV7a6EXbUEBE5DZMRUjRp4FmM0pIRdtQQETkNkxFSrHazBZX1HQWsCtimAYBkHphHROR0TEZIsaoajLCIgI9aQESgVu5wALC9l4jIFZiMkGJJnTTRQTqoVILM0VhJKyPVDUbUt5pkjoaIyDswGSHFss0YUUBbr0Sv80F0kHWVpoCrI0RETsFkhBTLNn1VAQPPumJHDRGRczEZIcWyddIoaGUEYEcNEZGzMRkhxZJWRpQwCr6rFHbUEBE5FZMRUiylrowks6OGiMip7EpGMjMzkZaWBr1ej6ioKCxcuBC5ubl93ldXV4eMjAzExMRAq9Vi5MiR+OKLLxwOmgaHznNpFLYyEmVNRk6fbYLJbJE5GiIiz2dXMpKdnY2MjAzs2bMHW7Zsgclkwvz589HU1PNviG1tbZg3bx6KiorwwQcfIDc3F2+99Rbi4uIGHDx5L2O7GTWNbQCUMwpeEhOkg5+PGiaziJLaZrnDISLyeBp7Lt60aVO3r9esWYOoqCgcOHAAs2bNuuA9b7/9Nmpra7Fr1y74+PgAABITEx2LlgaNio4tGp2PCiH+PjJH051KJSA5MgBHy+qRX91k27YhIiLHDKhmxGAwAADCwsJ6vObTTz/F9OnTkZGRgejoaKSmpuL555+H2Wzu8R6j0Yj6+vpuDxpcus4YEQRlDDzrSuqoKWARKxHRgDmcjFgsFixduhTp6elITU3t8bqCggJ88MEHMJvN+OKLL7B8+XL89a9/xbPPPtvjPZmZmQgODrY9EhISHA2TPJStXkRhnTQStvcSETmPw8lIRkYGcnJysH79+l6vs1gsiIqKwqpVqzB58mTcdNNNePzxx7Fy5coe71m2bBkMBoPtUVJS4miY5KGU2kkjSYmS2nvZUUNENFB21YxIlixZgo0bN2L79u2Ij4/v9dqYmBj4+PhArVbbvjdmzBhUVFSgra0Nvr6+592j1Wqh1SrjYDSSh23GiMI6aSTSykheVSNEUVTkVhIRkaewa2VEFEUsWbIEGzZswNatW5GUlNTnPenp6cjLy4PF0tkCefLkScTExFwwESECuo6CV+bKSFJEAAQBMLSYUNvUJnc4REQeza5kJCMjA2vXrsW6deug1+tRUVGBiooKtLS02K5ZtGgRli1bZvv6wQcfRG1tLR5++GGcPHkSn3/+OZ5//nlkZGQ4712Q1+ncplHmyojOR21rOeZWDRHRwNiVjKxYsQIGgwFz5sxBTEyM7fHee+/ZrikuLkZ5ebnt64SEBGzevBn79u3DRRddhF//+td4+OGH8Yc//MF574K8TucoeGWujAAsYiUicha7akZEUezzmqysrPO+N336dOzZs8eel6JBrMnYjvrWdgDKXRkBrMlI9slqtvcSEQ0Qz6YhxZHaevU6DfQ6ZQ0864odNUREzsFkhBSn68AzJeM2DRGRczAZIcVR+sAziZSMlNQ2o9XU80RhIiLqHZMRUhxpZUSpA88kEYG+0Os0sIjA6bM8MI+IyFFMRkhxpJURpQ48kwiCwK0aIiInYDJCimObMaLgtl6JLRmpYjJCROQoJiOkOKUKHwXfVWdHDZMRIiJHMRkhRRFFEeV1nrcyUlDD9l4iIkcxGSFFMbSY0NLRmaLkgWeSrts0/RkKSERE52MyQooiddKEBfhC56Pu42r5DQv3h0YloKnNjMp6o9zhEBF5JCYjpCi2GSMesCoCAD5qFYaG+wNg3QgRkaOYjJCilHV00ij5gLwfS45gey8R0UAwGSFFKfegThqJraOG7b1ERA5hMkKK4kkzRiSdg8/YUUNE5AgmI6QoZXWeVTMCdGnv5TYNEZFDmIyQopR7YM1ISqR1m6bM0IomY7vM0RAReR4mI6QYFouICmmbxoNWRkL8fRER6AsAKOTwMyIiuzEZIcWoaTKizWyBIADRQZ6TjABAMg/MIyJyGJMRUgxpDHyUXgsftWf90+SBeUREjvOsn/jk1ToHnnlOvYhEqhthRw0Rkf2YjJBiSKPgY0M8a4sG6Nrey5URIiJ7MRkhxZBWRmI9cmWk8/Res4UH5hER2YPJCClGmQcOPJPEhfrBV6NCW7vFNiuFiIj6h8kIKYYnjoKXqFUCkiOsdSN53KohIrILkxFSDE8cBd8VO2qIiBzDZIQUod1sQWV9RwGrB66MAOyoISJyFJMRUoSqBiMsIuCjFhARqJU7HIdw8BkRkWOYjJAiSEWf0UE6qFSCzNE4hgfmERE5hskIKYLUSeOJbb2S5I5tmprGNtQ1t8kcDRGR52AyQoogddLEeODAM0mAVmM74I91I0RE/cdkhBTB1knjwSsjALdqiIgcwWSEFEGqGYnz4JURgB01RESOYDJCiuA1KyNR7KghIrIXkxFSBNuJvR6/MsJkhIjIXnYlI5mZmUhLS4Ner0dUVBQWLlyI3NzcXu9Zs2YNBEHo9tDpPPsDh5zL2G5GTaO1+8STu2mAzo6a4rPNMJktMkdDROQZ7EpGsrOzkZGRgT179mDLli0wmUyYP38+mpp63x8PCgpCeXm57XH69OkBBU3epaJji0bno0KIv4/M0QzMkCAd/H3VaLeIOH22We5wiIg8gsaeizdt2tTt6zVr1iAqKgoHDhzArFmzerxPEAQMGTLEsQjJ65XVdc4YEQTPHHgmEQQBKZGBOFJqQH51I4Z31JAQEVHPBlQzYjAYAABhYWG9XtfY2Ihhw4YhISEB1113HY4ePTqQl3Wa/+0vQeYXx1HVcSYKyaPMC2aMdCV11BSwo4aIqF8cTkYsFguWLl2K9PR0pKam9njdqFGj8Pbbb+OTTz7B2rVrYbFYMGPGDJw5c6bHe4xGI+rr67s9nK3dbMEr35zCm9sLMPPFbXh8wxEUc1ldFrbiVQ+vF5GMHKIHAKzfV4xzTZzESkTUF4eTkYyMDOTk5GD9+vW9Xjd9+nQsWrQIEydOxOzZs/HRRx8hMjISb775Zo/3ZGZmIjg42PZISEhwNMweqVUCnrluHCYPC0VbuwX/+a4Yl/01C0vXf4/ciganvx71rHMUvHesjNycNhTxoX44fbYZi9ceQFs7C1mJiHrjUDKyZMkSbNy4Edu2bUN8fLxd9/r4+GDSpEnIy8vr8Zply5bBYDDYHiUlJY6E2StBEPCT0dH4YPF0vPfAJZg1MhJmi4iPD5Vhwd+347539+Ng8Tmnvy6dr3MUvHesjIQF+GL1nWkI1GrwXWEtnvj4CERRlDssIiLFsisZEUURS5YswYYNG7B161YkJSXZ/YJmsxlHjhxBTExMj9dotVoEBQV1e7iKIAiYlhyOf90zFZ8tmYmrxg+BIABfH6/EDW/swi2r9mDHqRp+mLhQ58Az71gZAYBRQ/R47ZZJUAnA+/vP4K1vC+QOiYhIsexKRjIyMrB27VqsW7cOer0eFRUVqKioQEtLi+2aRYsWYdmyZbavn3nmGXz11VcoKCjAwYMHcfvtt+P06dO47777nPcunGR8fDDeuG0ytjwyGzdOjodGJWB3wVncvvo7XPf6TmzKqYDFwqTE2TpHwXvHyojkstFReOLqsQCAzC9PYMuxSpkjIiJSJruSkRUrVsBgMGDOnDmIiYmxPd577z3bNcXFxSgvL7d9fe7cOdx///0YM2YMrrrqKtTX12PXrl0YO3as896Fkw2PCsRfbpyA7N9fhrtmJELno8LhMwYsXnsA8/++HR8eOMOBVk7SZGxHfWs7AO/Zpunq7vRE3DZtKEQReHj99zhaZpA7JCIixRFED9h/qK+vR3BwMAwGg0u3bHpyttGId3YW4d3dRWjo+OCMC/HDL2cn4xdTEqDzUbs9Jnu1tVuQU2ZAVX0r5o0dArVKGfM88qoaMPfl7dDrNDjy1AK5w3EJk9mCu97Zi515ZxEbrMPHS9IRpfeeLSkiop709/ObyYg9cbSa8J89xVi9o8A2vjwi0Bf3zEzC7ZcMQ5BOOdNDG43tOHj6HPYX1WJvUS0OldSh1WRdzfnDlaOxeHaKzBFabT9ZjUVv78WoaD02P9Lz4DxPZ2g24fo3dqKgpgkTEkLw3gOXeEQSS0Q0EExGXKjVZMb7+0vwZnYBSjvqHfQ6DRZNH4a705MQEah1e0w1jUZr4lF4DvuKanGsvB7mH9W3BPiq0dRmRkSgFjseu0wRH4bv7SvGYx8ewZxRkVhz91S5w3GpwpomXP/GTtQ1m/DTi2Lw2i2TPH7iLBFRb/r7+W3XOHiy0vmosWh6Im6ZOhSfHirDiux85FU14vVt+Vi9oxA3pw3F/bOSXVaQKYoiimubsa/oHPYV1mJfUS0Kas6f9hkf6oe0xDCkJYZhalIohoYF4LKXslBa14IPDpzB7ZcMc0l89pBGwXvLwLPeJEUEYMVtk3HH6u+w8XA5UiID8ci8kXKHRUQkOyYjA+CjVuFnk+Nx/aQ4bDleiTe25eGHMwas2VWEtXtOY+GkOCyenTLg80nMFhEnKuqxv+gc9hbVYl9hLaoajOddNypaj7Sk0I7kI+yCH/D3X5qEpz47hlXbC3BzWgI06gGdCDBgUieNtww868v0lHA8d30qHvvwCF755hSSIwNw3cQ4ucMiIpIVkxEnUKkELBg3BPPHRmNn3lm8kZWHXfln8cGBM/jw4BlcMW4IfjVnOMbHB/fr+VpNZhw+Y8C+Iuuqx4HT52yFsxIftYDxccFISwrD1MQwTBkWhuB+nHj7i7QEvPLNKRTXNuPLnApcMyHWoffsLLYZI17YSdOTm9KGIr+6Cau2F+DRDw4jIcwfFw8NlTssIiLZMBlxIkEQMHNEBGaOiMD3xefwRlY+thyrxJc5FfgypwKXjohAxmXDMS0prFutQH2rCQdOd265/HDGcN4I8QBfNS4eFoqpiWFISwrDxIQQh2o+/H01uGtGEv729UmsyMrHTy+KkbVuocwwuFZGJI9dMRoF1U34+nglHvjXAXyyJN3r5qwQEfUXC1hdLLeiASuz8/HpD2W2gtKLh4bghovjcaqyAXuLzuFERT1+/LcQEejbpd4jDKOH6J22pXKuqQ0z/rwVLSYz/nXPVMwaGemU57WXKIoY+8fNaDGZse13c5AUESBLHHJpMrbj5yt343h5PUYP0eODB2cgUMvfD4jIe7CbRmFKapvx5vZ8vL//zAUPThsW7m9NPBLDMCUxFEkRAS5dsXjms2N4e2chpieH478PXOKy1+lNXXMbJj6zBQBw4k9XKKK7x91K61pw3T92oqbRiMtHR2HVoimKmQFDRDRQTEYUqqq+Fat3FuL74jqMjQnClETr1ktUkHu3KcrqWjDrxW1ot4j4OCMdExNC3Pr6AHCsrB5XvfotwgN8cWD5PLe/vlJ8X3wON63ag7Z2C+6/NAmPX63c6cRERPbo7+e3vK0Ug1BUkA7LrhyD9385HU9dOw4/vSjW7YkIAMSG+Nm6OFZm5bv99QGg3CCd1ju46kV+bNLQUPz1xgkAgLe+LcT6vcUyR0RE5F5MRgaxxbOTAQCbj1Ugr6rR7a9fZhg8M0b6cs2EWCydOwIA8MTHOdiVXyNzRERE7sNkZBAbEa3H3DHREEVg1Xb3r46UD7IZI315+PIRuHZCLNotIh5cexCFFxhkR0TkjZiMDHIPzrGeUbPh+1JUdKxUuMtgnDHSG0EQ8OLPL8LEhBAYWky4d80+GJpNcodFRORyTEYGucnDQjE1KQwms4jVOwrc+trSuT4xXBmx0fmosWrRZMQG61BQ04QH/3MAJvP53VdERN6EyQjhwY4TfNd9V+zW38SlAtZYrox0E6XXYfVdaQjwVWNX/ln88ZOj8ICmNyIihzEZIcwZFYnRQ/RoajPj33uK3PKaFoto2xbiysj5xsQE4ZWbJ0EQgP/uLcbbO4vkDomIyGWYjBAEQbDVjryzswitJrPLX7OmyQiTWYQgANEytDZ7grljo/H4VWMAAM99fgzbTlTJHBERkWswGSEAwNXjYxAf6oezTW343/4Sl79eeZ11VSRKr4WPzCcHK9m9M5Nwc1oCLCLw0H+/x4mKerlDIiJyOn4KEABAo1bhgVnWuSNvbi9Au4uLJlkv0j+CIOCZ61JxSXIYGo3tuHfNftQ0GuUOi4jIqZiMkM2NkxMQFuCLM+da8PmRcpe+VlnHykgsB571yVejwsrbJyMpIgCldS144F/73bKVRkTkLkxGyMbPV427ZyQCAFZk5bu0g8M2Cp7Fq/0S4u+L1XdOQZBOg4PFdXjsw8PssCEir8FkhLpZND0RAb5qnKhoQNbJape9ThkHntktOTIQK26fDI1KwCeHyvCPrXmyxWIyW/B98Tms2p6P+97dj5kvbMW/dhfJFg+5Tlu7BYve3oul679nAkwuo5E7AFKWYH8f3DJ1KP65oxArsvJx2agol7wOR8E7Jn14BJ65LhX/t+EI/rrlJJIjA3H1RTEuf90mYzu+L67D3qJa7Cusxfcl59Bq6l5X9GZ2Ae64ZBgEQXB5POQ+356qxvaOX0xuuDges0ZGyhwReSMmI3Seey9Nwru7i7C3sBYHTp/D5GGhTn8NqWaEKyP2u3XaUORVNeLtnYX4zfuHEB/qhwkJIU59jZpGI/YXncO+olrsK6rF0bJ6mC3dfysO9ffBlMQwTB4Wipc256K0rgXFtc0YFh7g1FhIXl/mVNj+98rsfCYj5BJMRug8McF+uH5SHN7ffwYrs/Px1qIpTn3+drMFVQ1SAStXRhzx+NVjUHS2CVtPVOH+f+3HJ0vSHT79WBRFnDnXgr2F1sRjb1EtCqrPP6QvLsQPaYmhSEsKw9TEMKREBkKlsq6CbD1ehb1FtdiZd5bJiBcxmS34+nil7etd+WdxqKQOE52c/BIxGaELemBWCv534Ay2HKvEqcoGjIjWO+25KxuMsIiAj1pARKDWac87mKhVAl65eSJ+vmI3cisbcO+a/fjf4ukI0Pb9n7TFIiK3ssGaeHQkIJX157cLj4rWY0qi9eyitMSwXtuwZwwP70hGanDrtKEDem+kHN8V1KKu2YTwAF9cOiICHx8qw8qsfKy8Y7LcoZGXYTJCFzQ8KhDzx0Zj89FKvLm9AC/dOMFpzy3Vi0QH6Wy/WZP99Dof/PPOKbj+jZ04Vl6PR947hJW3Tz7v/1NjuxlHzhhs9R77T59DQ2t7t2s0KgHj44MxNdGaeExJDEWIv2+/Y0kfHoG/f30Ku/JrYLGI/Hv1El/mWFv8542Nxj0zk/DxoTJsPlaB/OpGpEQGyhwdeRMmI9SjxbNTsPloJT7+vhS/mTfSaQPKpE4azhgZuIQwf7x5xxTc8tYefHWsEi9uzkXGZSk4cLqj3qPwHA6dqUNbe/di0wBfNS4eFoq0juRjYkII/HzVDscxMSEEAb5qnGs24XhFPcbFBg/0rZHMzBYRm49at2iuSB2CkdF6zB0Tha+PV2FVdgFe+PlFMkdI3oTJCPVo0tBQXJIchj0Ftfjnt4X44zVjnfK8tk6aENaLOMPkYaF48WcXYel7h7AyOx+rtufjR7WmCA/wtSYeHfUeY2L00DhxDL+PWoWpSWHYlluNXXlnmYx4gQOnz6Gm0Qi9ToMZKREAgAfnpODr41X46PszeGTeSAzx0pqvFzedQF2LCX+6LhVqrvK5BZMR6tWDc4ZjT8FerN9XjId+MhyhAf1fuu9JOWeMON3CSXEoqG7Eq1vzYBGBoWH+SEsMw9Qk6+pHUkSAy1tu04dHYFtuNXbk1eD+jqMFyHNt6uiimTcmGr4aa+I6eVgY0hJDsa/oHN7eWYj/6zjI0ZvsOFWDN7LyAQAXDw3FzyfHyxzR4MBkhHo1a0QExsYE4Vh5Pf61+zQenjtiwM9ZxhkjLvHIvJGYOzYa0UE6WU5Cln573ltYi7Z2i+0DjDyPKIrYfNSajFyROqTbnz04JwX71uzHf/acRsac4Qj295EjRJcQRRF/2XzC9vXftpzENRNioNU4voVJ/cOfFtQrQRCweE4KAGDNrkI0t7X3cUffbCsjrBlxKkEQcFF8iCyJCACMHqJHeIAvWkxmHCqpkyUGco7DZwworWuBv6/6vLkil42KwqhoPZrazFj73WmZInSNzUcr8cMZA/x91YgI1KK0rgXr97r+FHNiMkL9cFXqEAwN88e5ZhPe3zfw/zBt59KwZsSrqFQCpqeEAwB25NXIHA0NhDTo7LJRUdD5dF8VsP6CYt2Ge3tHodcc2mi2iPjrV7kAgHvSk2yrwK9tzXPKL2HUOyYj1CeNWmWrAXjr20KYzJY+7uhZq8mMmsY2AOym8Ubpw61bNbuYjHgsURSxqaOl98dbNJKfXhSLuBA/nG1qw//2e8fKwcffl+JUVSOC/Xxw/6xk3DQlAUPD/FHTaMQ7O4vkDs/rMRmhfrlxcjwiAn1RWteCjYfLHH6eio4tGp2PCiFetNdMVjM7kpFDJXVoMvK3SU90oqIBRWeb4atR4bLRFz6byketwv2XJgEAVn1bgPYB/IKiBG3tFvzt65MArCMNgv184KtR4ZF51tWRN7PzYWg2yRmiSynhAES7kpHMzEykpaVBr9cjKioKCxcuRG5ubr/vX79+PQRBwMKFC+2Nk2Sm81Hj7nTrD58VWfmw/Lh3tJ/KDFLxqh8PVPNCCWH+SAjzQ7tFxN7CWrnDIQdIXTSzRkQisJeJvjelDUVYgC9Kalvw+ZFyd4XnEuv3FePMuRZE6rW4a0ai7fvXTojDqGg96lvb8eb2fPkCdLHXt+Vh2UeHUdvUJlsMdiUj2dnZyMjIwJ49e7BlyxaYTCbMnz8fTU3nn2PxY0VFRfjd736HSy+91OFgSV63XzIMgVoNTlY2YltulUPPUW47II/1It4qvaOrhnUjnklKRq7sYYtG4uertn1wr8wuUMRv145obmvHq9/kAQB+/ZPh3Yb/qVUCfjt/JADgnZ1FtjO1vElJbTNe25qH/+4twU4Z/5u1KxnZtGkT7rrrLowbNw4TJkzAmjVrUFxcjAMHDvR6n9lsxm233Yann34aycmcP+Cpgv18cFvHuSMrsx37LcFWvMp6Ea81o2OrRs4fbOSYgupG5FY2QKMSMHdMdJ/XL5o+DP6+ahwvr0f2yWo3ROh8a3YVoabRiIQwP9yUdv65SvPGRmNiQghaTGa8vjVPhghd6+nPjsHYbsGMlHD89KIY2eIYUM2IwWAAAISFhfV63TPPPIOoqCjce++9/Xpeo9GI+vr6bg9ShntmJsFXrcK+juPl7WUbBc+BZ15rRkdHzYmKBtQ0nn8AHymX1EUzPSW8X/NDQvx9cctU6wf4iizP28YwNJuwsiPuR+aOvOBsHEEQ8PsFowAA6/YWo6S22a0xutI3xyvx9fFKaFQCnrlunKxb5w4nIxaLBUuXLkV6ejpSU1N7vG7Hjh1YvXo13nrrrX4/d2ZmJoKDg22PhIQER8MkJ4sO0uGGi+MAwPYfsT3KOfDM60UEajF6iPWU5135Z2WOhuzRuUXT/9+Q77s0CT5qAd8V1uJg8TlXheYSq77NR31rO0ZGB+K6iXE9XjdjeARmDo+AySzi71+fcmOErtNqMuOpz44CAO6dmYThUc47md0RDicjGRkZyMnJwfr163u8pqGhAXfccQfeeustRERE9Pu5ly1bBoPBYHuUlHhH65i3eGBWMgQB+OZEFXIrGuy6l6PgBwe2+HqeM+eacaTUAJUAzB/X9xaNJCbYz/ZB7sgvKHKpamjF2zuKAAC/nT+qzzNoHu1YHdnw/RmcqrTv554SrczOR0ltC4YE6fDQ5QOfrD1QDiUjS5YswcaNG7Ft2zbEx/c8tz8/Px9FRUW45pproNFooNFo8K9//QuffvopNBoN8vMv/A9Xq9UiKCio24OUIzkyEFeMsxa3vWln7QhHwQ8OUovvznwmI55CWhVJSwxDRKDWrnsXz7bWAn51rBJ5VZ7xQf3Gtny0mMyYmBCC+WP7Tr4mJIRgwbhoWETgr1+ddEOErlN8ttl2/s4TPx3Ta9eUu9iVjIiiiCVLlmDDhg3YunUrkpKSer1+9OjROHLkCA4dOmR7XHvttbjssstw6NAhbr94sMWzrSPiP/2hDGfO9W8PtcnYjvpW6+wJrox4t6lJYdCoBJTUtqD4rPfssXuz/nbRXMjwKD3mdXygv5ld4NS4XKGkthn/6Rhl//sFo/pdK/G7+aMgCMCmoxX4wYOPPHj6s6Noa7cgfXg4rh4vX9FqV3YlIxkZGVi7di3WrVsHvV6PiooKVFRUoKWlxXbNokWLsGzZMgCATqdDampqt0dISAj0ej1SU1Ph6zvwE2BJHhMSQjAjJRztFhH//LawX/dInTR6nUYRmTi5ToBWg4kJIQC4OuIJqupbcaCj3mOBA8kIYD1ADwA+PlRq+29dqV755hRMZhHpw8Nt3V/9MSJaj+snWbek/rK5/zO2lOTrY5X45kQVfNQCnr42VTHznuxKRlasWAGDwYA5c+YgJibG9njvvfds1xQXF6O83LMH4FD/SD981u8r7tewnNKOGSMcAz84sMXXc2w+WgFRBCYmhDjcdn/x0FBMSwqDySxidT9/QZHDqcoGfHTwDADg0QWj7b7/kbkj4aMWsCOvxuNqolpNZjy9USpaTcbwqECZI+pk9zbNhR533XWX7ZqsrCysWbOmx+dYs2YNPv74YwfDJSWZOTwCqXFBaDVZ8O6uoj6vlzppOPBscJDqRnbnn3V4Yi+5x5cD2KLpSjrhe93eYtQ1yzfNszd//eokLCIwv2N+iL0Swvxt7cwvbs71qGFvK7KsRasxwTo89JPhcofTDc+mIYcJgmCrHXl3d1GfZ5FIM0Y48GxwmJgQAj8fNc42teGEnV1X5D61TW34rmN0vz0tvRcyZ2QkRg/Ro7nNjH/tPu2M8Jzqh5I6bDpaAUEAftfRHeOIJT8ZDj8fNQ6V1OHr445No3a302ebsKKj4WD5T8ciQGFb5UxGaECuTI3BsHB/1DWbsH5f7y3YnDEyuPhqVJiaZB2IuIt1I4r19bFKmC0ixsYEYWi4/4CeSxAE2/btml1FaGkzOyNEp3npK2udx/UT4zAy2vG5GlF6He5OT7Q+5+ZcmBW+8ieKIp761Fq0eumIiAGvgLkCkxEaELVKwAOzrG19q78tQFt7z6d3csbI4DOTdSOK92WOtcbPWR9QV4+PQUKYH2qb2vD+fuXMiNqVX4NvT9XARy3gkXkjB/x8v5yVgiCdBrmVDfjsB8dPMneHr49XYVtuNXzUAp66Vt5Jqz1hMkID9rOL4xERqEWZoRWf9vIfpe3EXtaMDBozhltHw39XWAuThx8z743qW022Aw2vcFIyolGr8MCl1l9QVm0vUMTfuyiKtu6Xm9OGIiFsYCtAABDs74NfdmxTv7zlZK+/iMmppc2Mpz61Fq3ed2kyUiKVU7TaFZMRGjCdjxr3zrTOnFmZnX/BYkVRFG0n9rKbZvAYMyQIYQG+aG4z45AHz2XwVluPV8FkFpESGYARA9i2+LEbpyQgPMAXpXUt+Pyw/N2V3xyvwvfFddD5qJxauHl3eiIiArUorm1W1CpQVyuy8lBa14JYBRatdsVkhJzitkuGQq/VIK+qEd+cOL+gy9BiQovJun88hDUjg4ZKJWB6snV1hFs1yuPIWTT9ofNR22oqVmTly9pxYrGItlqRu9OTEBXkvJ8//r4a2wf8q9+cUlyNTFFNE1Z2DKFb/tOx8PdVVtFqV0xGyCmCdD647ZJhAKyZ+I9/+JR1rIqEB/hC56N2e3wkn85zanhonpI0t7Uj66T1FwdnbdF0dccliQjwVSO3sgHbcuXrOPnscBlOVDRAr9Ng8awUpz//zVMTEBfih6oGI97dXeT053eUKIp46rOjaDNbi1Zd8XfsTExGyGnuSU+Er0aFg8V12FfU/fROaSIjZ4wMPukddSMHi8/12f5N7pOdW41WkwUJYX4YF+v887+C/X1w6zTrPI6VWfKMiDeZLXh5i/UcmcWzUxDs7+P019Bq1LaC2BVZ+ahvNTn9NRyx5VglsjqKVp9WaNFqV0xGyGmignT42cXWgxNXZOV1+zPOGBm8hob5Iy7ED+0WEXuLauUOhzpIg86uGDfEZR9U985Mho9awN6iWhw47f6/+/f3l+D02WZEBPrirhmJLnud6yfFYXhUIAwtJry1Xf6zeVrazHj6s2MArKesJyu0aLUrJiPkVL+clQyVAGzLrcbx8nrb93la7+AlCIKtxdfTxmd7K2O7GVtPSFs0rjsobUiwDjdMkn5Bce+HdKvJjFe/OQUAWHLZcJcO+VKrBPxuvnV1ZPWOQtQ0Gl32Wv3xRkfRalyIHzIuU27RaldMRsipEiMCbMVwb3ZM+wO6joLnyshgJLX47mTdiCLszKtBo7Ed0UFaTHJgJLo9HpidDEEAvj5eiZOV7pvE++6uIlTWGxEX4odbOraLXGnBuCG4KD4YzW1mvL4tr+8bXKSwpsl2crLSi1a7YjJCTieNiP/scDlKaq3Hx3du03BlZDCakWJdGTlWXo+zMv/WSMCXRzq3aFQq19YSpEQGYsFYa/Hkyi6/oLhSfavJNvp86dwR0GpcXzQvCAIe7Rgx/589xSitc//JxaIo4slPrUWrs0dGYsG4aLfH4CgmI+R04+ODcemICJgtIt761pqhl9sGnnFlZDCK1GsxqmOOxe4Cro7IyWS2YMvxSgCu3aLpSjpA79NDZW75kP7n9gLUNZuQEhmA6yfFufz1JDOHR2B6cjjazBa88vVJt72uZPPRSmw/WQ1ftUqxk1Z7wmSEXEJaHXlvXwmqG4yo6FgZYTIyeKXbRsMzGZHTdwW1qGs2ITzA13Z2kKtNTAjB9ORwtFtE/PNb19aO1DQa8c8dhQCA380fBY3afR9zgiDg0SusqyMfHDiDvKpGt712c1s7/rSxs2g1KSLAba/tDExGyCVmpITjovhgGNst+OtXuTCZRagEIFqvlTs0konU4stD8+QlnUUzb2w01C7eoulKOkBv/d4SnGtqc9nrvLEtH81tZoyPC5ZltsbFQ0Mxd0w0LCLwty3uWx15fZvnFa12xWSEXEIQBDworY50jEmO0uvc+lsKKcvUpDCoVQJOn2221RKRe5ktIjYflbZo3PtBfemICIyLDUKLyeyy4WCldS1Yu+c0AODRBaNk26b43YKREATg8yPlyCk1uPz1CqobsaqjpfiP14yFn6/nDZbkJwO5zPxxQ5AUEQBpGCsHng1uep0PJnZ0bnB1RB4Hi8+hptEIvU5jKyp2F0EQbNu3a3YVobnN+QPwXv36FNrMFlySHIZLR7j3/XU1ekgQrpsQCwC2A/pcRSpaNZlFzBkVifljPadotSsmI+QyapWAX85Ktn3NA/IoPYUtvnKSumjmjYmGr8b9P/6vTB2CYeH+qGs24b19zj1YLr+6ER8cPAMAeHTBaNmLNx+ZNxIalYDsk9X4zoVF25uPVuDbUzXWotVrPKtotSsmI+RS118ch6iOOhG29dIMafhZfo2sh6cNRqIoYvPRjpZemc4p0ahVuP9S6y8ob20vgMlscdpzv7zlJMwWEXPHRGHysFCnPa+jhoUH4Ka0BADW1RFX/HtvbmvHMx2TVhfPTkaihxWtdsVkhFxKq1Hj8avHICJQiwUKP6iJXG/S0BDofFSoaWxDrhsHYBFw+IwBpXUt8PdVY9bISNni+PnkeEQEalFmaMWnh8qc8pw5pQZ8frgcggD8dv4opzynM/z68hHQ+aiw//Q5lxwW+I+teSgztCI+1A8PzvG8otWumIyQy103MQ77n5iLtET3tBGScmk1akxN4laNHKSzaC4bFSXrydk6HzXumZkIAHhzez4sloGvGLz0lbUu49oJsRgT4/xD/xwVHaTDnR1n4vxl80mnvFdJfnWjbY7Tk9eM88ii1a6YjBCRW0l1Izynxn1EUcSmjpZeJRwlf9u0YQjUanCystF2Ro6j9hbWIiu3GhqVgEfmjnRShM6zeFYK9FoNjpfXY+ORcqc8pyiKeKqjaPUno6Mwd0yUU55XTkxGiMitpOFnewrOOrVmgHqWW9mAorPN8NWocNlo+T+4gv18cNsl1vNiVgxgRLwoivjL5hMAgF+kJSiyZiI0wBcPdBTyv/xVrlP+zX+Z01G0qlHhyWvGemzRaldMRojIrcbGBCHE3wdNbWYcPlMndziDgtRFM2tEJAJdeHqtPe5NT4KvWoUDp89hX1GtQ8+RlVuNfUXnoNWo8OufjHByhM5z98wkhAf4ouhsMz44cGZAz9Vk7Jy0unh2CoaFKy8BcwSTESJyK5VKwAy2+LrVpo56kSsVsEUjiQrS4WeTrefGrMiyf3XEYhFtMzzunJGIIQru1gvUamxTUV/5+hRaTWaHn+u1rXko7yha/VXHVFtvwGSEiNxOGri1g3UjLldQ3YjcygZoVALmjlHWQKwHZqVAEICtJ6pwoqLerns/P1KOY+X1CNRqbNOelezWaUMRG6xDRX2rbUqsvfKqGm1n+zx1zThZC5GdjckIEbmdVDfyffE5l0zipE5SF830lHAE+/vIHE13SREBttWaN7P7f4Beu9mClzvOfbn/0mSEBvi6JD5n0vmosbSjwPb1bXloaDXZdb9UtNpuEXH56CjM9dBJqz1hMkJEbpcY7o+4ED+YzCL2FZ2TOxyvJg06uzI1RuZILkwaEf/pD2X9PrPogwNnUFjThLAAX9x7aZIrw3OqGy6OQ3JkAM41m7C642Th/vriSAV25ElFq+NcFKF8mIwQkdsJQmfdCFt8XefMuWYcPmOASgDmj1Pmb9IXxYdg5vAImC1ivz6gW01mvPLNKQDAr+akKKYgtz80ahV+O886lO2f3xaitp+nF3ctWv3VnBQMDfd3WYxyYTJCRLKQtmpYN+I6UuFqWmIYIgK1MkfTM2l1ZP2+YpxtNPZ67do9p1FuaEVMsA63XzLMHeE51ZWpQ5AaF4RGYzve2JbXr3te3XoKFfWtGBrmb/v/ytswGSEiWcwYbl0ZOVZej3P9/A2R7CMlI0oYdNab9OHhGB8XjFaTBe/uKurxukZjO97o6Lx5+PIRHlnAqVIJ+F3HyPp/7TmNckNLr9fnVTVg9bfWFaOnrh3rke+5P5iMEJEsovQ6jIwOhCgCu114qulgVVXfigPF1nocpScjgiDgwY421Xd3n0aT8cJFzas7tjaSIwLw88nx7gzRqWaPjMTUpDC0tVvwaseW04WIoog/fmItWp07Jgo/Ga3MrTZnYDJCRLJhi6/rbD5WCVEEJiaEICbYT+5w+rRg3BAkRQTA0GLCf/cWn/fntU1ttrNYfjN/JDRqz/34EgQBv19gXR15f7+1GPdCNh4ux678s9B6adFqV577t0lEHk+qG2ERq/NJZ9EoadBZb9QqwTY2/Z/fFqKtvfvY9JXZ+Wg0tmNsTBCuUmhnkD2mJIbhJ6OjYLaItjblrhqN7Xj2c6lodTgSwryvaLUrJiNEJJtpyWFQqwQUnW1GaV3ve+fUf+ea2rCnwDpiXaktvRdyw8VxiNJrUVHfik8Oldq+X2FotdWSPHrFKKhUnn8WCwBb7chnP5ThWFn3oW+vfnMKlfVGDA3zxy9nJ8sRnlvZlYxkZmYiLS0Ner0eUVFRWLhwIXJzc3u956OPPsKUKVMQEhKCgIAATJw4Ef/+978HFDQReYcgnQ8uig8GAOzk6ojTbDlWCbNFxNiYII9qA9Vq1LhnpnVuyMrsfFgsIgBrN4mx3YK0xFDMGRkpZ4hONTY2CNdMiAUAvPRV52fpqcoGvL3D+4tWu7IrGcnOzkZGRgb27NmDLVu2wGQyYf78+WhquvB+FwCEhYXh8ccfx+7du3H48GHcfffduPvuu7F58+YBB09Eni+9o26EyYjzfNmxRaP0wtULuW3aUOh1GuRXN+Hr45UoqmnC+/tKAACPLhjtFSfUdvWbeSOhVgnYeqIK+4tquxWtzhsb7dVFq13ZNS1m06ZN3b5es2YNoqKicODAAcyaNeuC98yZM6fb1w8//DDeffdd7NixAwsWLLAvWiLyOunDI/CPbXnYlX8Woih63YeNu9W3mmwHEHpKvUhXep0Pbr9kGFZk5eONrHwMDfNHu0XEnFHWDhRvkxQRgF9Micd/95bgxc25uG3aUOwusBat/vGnY+UOz20GVDNiMBgAWFc/+kMURXzzzTfIzc3tMXkBAKPRiPr6+m4PIvJOFw8Lgc5HheoGI05VNcodjsfbdqIKbWYLUiIDMCJaL3c4Drk7PRG+GhUOldTh0x/KAHTWV3ijX18+Ar4aFfYW1uL/PjoCAFhymfcXrXblcDJisViwdOlSpKenIzU1tddrDQYDAgMD4evri6uvvhqvvfYa5s2b1+P1mZmZCA4Otj0SEhIcDZOIFE6rUSMt0foLzY5Tyt+qaTdb8M9vC5B9slruUC7oyyPKPoumP6L0OtzYZY7I1RfFIDUuWMaIXCsm2A+LOqbJNrWZkRjuj/tneX/RalcOJyMZGRnIycnB+vXr+7xWr9fj0KFD2LdvH5577jn85je/QVZWVo/XL1u2DAaDwfYoKSlxNEwi8gDSvJFd+cpPRtbsKsKznx/HnW/vxb8dPAreVZrb2pF1sgqAZ9aLdPXArGSoVQLUKgG/nTdS7nBc7leXDbeds/PkteMGRdFqVw6dMLRkyRJs3LgR27dvR3x831PwVCoVhg8fDgCYOHEijh8/jszMzPPqSSRarRZarXLPUSAi55o5PAIvAPiuoBbtZotiB1pV1rfib11mQiz/OAfNxnb8UiHnhWTnVqPVZEF8qB/GxQbJHc6ADAsPwH/vvwSCACRHBsodjsuFBfhi/QOXoLrBiMtGRckdjtvZlYyIooiHHnoIGzZsQFZWFpKSHDu62WKxwGjs/TAkIho8xsYGIdjPB4YWEw6XGnDx0FC5Q7qg5z4/jqY2My4eGoLpKeF4fVs+Mr88gaY2Mx6ZO0L24tsvc6QtmiGyx+IM3liw2htv3orqi13JSEZGBtatW4dPPvkEer0eFRXWf/jBwcHw87OOG160aBHi4uKQmZkJwFr/MWXKFKSkpMBoNOKLL77Av//9b6xYscLJb4WIPJVaJWB6cjg2Ha3AzlM1ikxGduXX4NMfyqASgGeuS0VqXDACtBq8uCkXr35zCk3Gdjxx9RjZkgBjuxlbT0hbNJ5bL0KDk11roStWrIDBYMCcOXMQExNje7z33nu2a4qLi1FeXm77uqmpCb/61a8wbtw4pKen48MPP8TatWtx3333Oe9dEJHHSx/RMW9EgXUjJrMFT35yFABw+yXDbL/B/mrOcDx9rfXMkNU7CvF/G3Jg7hjU5W4782rQaGxHdJAWkxJCZImByFF2b9P05ceFqc8++yyeffZZu4IiosEnPSUcAHDwdB1a2szw81VOAd+anUU4VdWI8ABf/HZe9xbTO2ckws9XjT98eBj/3VuMlrZ2vHTjBLfXvUhdNFeMG+I149Jp8FBmlRgRDTpJEQGICdahzWzBvqJaucOxqTC04u9fW4tW/3DlaAT7+5x3zS+mJODVWyZBoxLw8aEy/Oo/B2FsN7stRpPZgi3HKwFwi4Y8E5MRIlIEQRBsLb5K2qp57ovOotWfXdxz9+BPL4rFm3dMhq9Gha+OVeK+d/ejpc09Ccl3BbWoazYhLMAXaYnKq7ch6guTESJSjJkjrFs1uzrGmcttV34NPutStNrX9sflY6Lxzl1p8PdV49tTNbjz7b1oaDW5PM5NR611evPHRiu2LZqoN/xXS0SKIa2M5JQZUNfcJmssJrMFf+woWr2jS9FqX9KHR+Df906DXqfB3qJa3PbP73CuyXXvxWIRsfmotEXj2YPOaPBiMkJEihEdpMPwqECIIrA7X97VkXd2FiKvo2j1N3aeizJ5WCj+e/8lCAvwxeEzBty8ag+qGlpdEueB4nOobjBCr9PYkjkiT8NkhIgUZeZw+etGrEWrpwB0FK36nV+02pfUuGC898AliNJrkVvZgJve3IPSuhZnh2rropk3Jhq+Gv5IJ8/Ef7lEpCgzUuSvG3nui+NobjNj8rDQXotW+zIiWo//LZ6OuBA/FNY04Rcrd6OopslpcYqiiM1HrcnIAm7RkAdjMkJEijItORwqASioaUKZC1YS+rIrr2vR6rgBz+wYFh6A/y2ejuSIAJTWteDGN3fjZGWDU2I9fMaA0roW+PuqMXtkpFOek0gOTEaISFGC/XxwUXwIAOtUUXdqa7fgj59ai1YXTU/EuFjnnBUSG+KH9345HaOH6FHdYMRNb+7GkTOGAT/vpo5VkctGRQ26U17JuzAZISLFSR/esVXj5iJWqWg1ItAXjzj52PpIvRbrH7gEExJCcK7ZhFvf2jOg4W6iKGJTx8F47KIhT8dkhIgUJ10afpZX069jKJyh3NCCV76RilbHOFS02pcQf1/8575pmJYUhgZjOxat3osdpxxb/cmtbEBhTRN8NSpcNnrwHTlP3oXJCBEpzsXDQqHVqFDVYEReVaNbXvO5z61Fq1OGheKGSXEue51ArQZr7p6K2SMj0WIy4541+7DlWKXdzyN10cwaEYlArV3HjBEpDpMRIlIcnY8aaYlhANxTN7IzrwYbD5f3e9LqQPn5qrFq0WRcMW4I2swWLF57AJ/+UGbXc3CLhrwJkxEiUqQZHXUjO11cN9LWbsEfP8kBYC1aHRsb5NLXk2g1avzj1km4YVIczBYRD6//Hu/tK+7XvQXVjcitbIBGJWDemGgXR0rkekxGiEiRpLqRPQVn0W62uOx13tlZiPzqJpcUrfZFo1bhpRsn4LZpQyGKwGMfHsHbOwr7vE/qopmeEn7BU4SJPA2TESJSpNS4YATpNGhobceR0oG3wV5I16LVZS4qWu2LSiXg2YWpeGBWMgDgmY3H8Pq2vF7vkbZorkyNcXl8RO7AZISIFEmtEjA9xbUtvs92FK2mJYbihotdV7TaF0EQsOzK0XhkrnVl5i+bc/HCphMX7CQ6c64Zh88YIAjA/HHcoiHvwGSEiBQrfXhni6+z7ThVg887ilafvjYVguDaotW+CIKAh+eOwBNXjwEArMjKx1OfHoXF0j0hkVZF0hLDEBGodXucRK7AZISIFEs6hXb/6XNoNZmd9rxt7RY8+an7i1b7475Lk/H89eMhCMC7u0/j9x8e7lYz07lFwy4a8h5MRohIsVIiAzAkSIe2dgv2F51z2vO+bSta1bq9aLU/bp02FH/7xUSoVQI+OHAGD68/hLZ2C6rqW3Gg2Pr/A1t6yZswGSEixRIEoUuLr3O2asrqWvCqrWh1tCxFq/2xcFIcXr/1YviqVfj8SLltFokoAhMTQhAT7Cd3iEROw2SEiBRNavHd5aS6kecUUrTaH1ekDsFbd06BzkeFrSeq8NwXxwFwi4a8D5MRIlI0qYj1cKkBhmbTgJ5rx6kafH6kHGqVgGeuk79otT9mj4zEu3dPRaBWA6m5hls05G2YjBCRog0J1iElMgCiCOwucLzFt63dgj/ailaHYUyMcopW+zItORz/uW8aovRaXD46CsPCA+QOicipeLoSESle+vAI5Fc3YVd+jcOrAqt3FKJAwUWrfZmQEILdyy6H2sXn5hDJgSsjRKR4UovvDgfrRroWrf7fVaMRpFNm0WpfmIiQt2IyQkSKNz05HCoBKKhuQoWh1e77n/v8OFpM1qLV6ycpu2iVaDBiMkJEihfs74PxccEA7J/G+u2pao8rWiUabJiMEJFHmCGNhrdj3oix3YwnPzkKwPOKVokGEyYjROQRZnY5p+ZCB8hdyOodhSio8dyiVaLBgskIEXmEycNC4atRobLeiPzqpj6vL6trwWvf5AEAHr/ac4tWiQYDJiNE5BF0PmpMGRYKANjVj62aZz8/hhaTGVMTw7BwIotWiZSMyQgReQxpGuuOU70nI9tPVuOLIxXWotWF41i0SqRwTEaIyGNIyciegrMwWy5cN2JsN+OpT61Fq3dOT8ToISxaJVI6JiNE5DHGxwVDr9OgvrUdOaWGC14jFa1G6rVYOm+EmyMkIkfYlYxkZmYiLS0Ner0eUVFRWLhwIXJzc3u956233sKll16K0NBQhIaGYu7cudi7d++AgiaiwUmtEnBJcjiAC7f4lnYpWvXkSatEg41dyUh2djYyMjKwZ88ebNmyBSaTCfPnz0dTU8+V7VlZWbjllluwbds27N69GwkJCZg/fz5KS0sHHDwRDT5dW3x/7NmNLFol8kSC2N+G/Quorq5GVFQUsrOzMWvWrH7dYzabERoain/84x9YtGhRv+6pr69HcHAwDAYDgoK4/0s0mOVVNWDuy9uh1ajww5PzofNRAwCyT1bjzrf3Qq0S8PmvZ7JWhEgB+vv5PaCaEYPBumcbFhbW73uam5thMpl6vcdoNKK+vr7bg4gIAFIiAxGl18LYbsHB0+cAdC9avWsGi1aJPI3DyYjFYsHSpUuRnp6O1NTUft/32GOPITY2FnPnzu3xmszMTAQHB9seCQkJjoZJRF5GEARbV41UN/LPbwtRKBWtzmXRKpGncTgZycjIQE5ODtavX9/ve/785z9j/fr12LBhA3Q6XY/XLVu2DAaDwfYoKSlxNEwi8kK2eSN5Z61Fq1tPAQAev2oM9CxaJfI4GkduWrJkCTZu3Ijt27cjPj6+X/e89NJL+POf/4yvv/4aF110Ua/XarVaaLVaR0IjokEgfbi1o+bImTr84cPDaDVZMDUpDNdNjJU5MiJyhF0rI6IoYsmSJdiwYQO2bt2KpKSkft334osv4k9/+hM2bdqEKVOmOBQoEZEkJtgPyREBsIjAt6dqoFYJ+NN1qZy0SuSh7EpGMjIysHbtWqxbtw56vR4VFRWoqKhAS0uL7ZpFixZh2bJltq9feOEFLF++HG+//TYSExNt9zQ2NjrvXRDRoCNt1QDWotVRQ/QyRkNEA2FXMrJixQoYDAbMmTMHMTExtsd7771nu6a4uBjl5eXd7mlra8PPf/7zbve89NJLznsXRDTozBoZCQAsWiXyAgOaM+IunDNCRD8miiLe31+CSUNDMTKaqyJEStTfz2+HCliJiOQmCAJuShsqdxhE5AQ8KI+IiIhkxWSEiIiIZMVkhIiIiGTFZISIiIhkxWSEiIiIZMVkhIiIiGTFZISIiIhkxWSEiIiIZMVkhIiIiGTFZISIiIhkxWSEiIiIZMVkhIiIiGTFZISIiIhk5RGn9oqiCMB6FDERERF5BulzW/oc74lHJCMNDQ0AgISEBJkjISIiIns1NDQgODi4xz8XxL7SFQWwWCwoKyuDXq+HIAhyhzNg9fX1SEhIQElJCYKCguQOx+UG2/sFBt975vv1bny/3s2V71cURTQ0NCA2NhYqVc+VIR6xMqJSqRAfHy93GE4XFBQ0KP6hSwbb+wUG33vm+/VufL/ezVXvt7cVEQkLWImIiEhWTEaIiIhIVkxGZKDVavHkk09Cq9XKHYpbDLb3Cwy+98z36934fr2bEt6vRxSwEhERkffiyggRERHJiskIERERyYrJCBEREcmKyQgRERHJismIG2VmZiItLQ16vR5RUVFYuHAhcnNz5Q7Lbf785z9DEAQsXbpU7lBcprS0FLfffjvCw8Ph5+eH8ePHY//+/XKH5RJmsxnLly9HUlIS/Pz8kJKSgj/96U99nkHhSbZv345rrrkGsbGxEAQBH3/8cbc/F0URf/zjHxETEwM/Pz/MnTsXp06dkidYJ+jt/ZpMJjz22GMYP348AgICEBsbi0WLFqGsrEy+gAeor7/frhYvXgxBEPD3v//dbfE5W3/e7/Hjx3HttdciODgYAQEBSEtLQ3FxsctjYzLiRtnZ2cjIyMCePXuwZcsWmEwmzJ8/H01NTXKH5nL79u3Dm2++iYsuukjuUFzm3LlzSE9Ph4+PD7788kscO3YMf/3rXxEaGip3aC7xwgsvYMWKFfjHP/6B48eP44UXXsCLL76I1157Te7QnKapqQkTJkzA66+/fsE/f/HFF/Hqq69i5cqV+O677xAQEIAFCxagtbXVzZE6R2/vt7m5GQcPHsTy5ctx8OBBfPTRR8jNzcW1114rQ6TO0dffr2TDhg3Ys2cPYmNj3RSZa/T1fvPz8zFz5kyMHj0aWVlZOHz4MJYvXw6dTuf64ESSTVVVlQhAzM7OljsUl2poaBBHjBghbtmyRZw9e7b48MMPyx2SSzz22GPizJkz5Q7Dba6++mrxnnvu6fa9G264Qbzttttkisi1AIgbNmywfW2xWMQhQ4aIf/nLX2zfq6urE7Varfjf//5Xhgid68fv90L27t0rAhBPnz7tnqBcqKf3e+bMGTEuLk7MyckRhw0bJv7tb39ze2yucKH3e9NNN4m33367LPFwZURGBoMBABAWFiZzJK6VkZGBq6++GnPnzpU7FJf69NNPMWXKFNx4442IiorCpEmT8NZbb8kdlsvMmDED33zzDU6ePAkA+OGHH7Bjxw5ceeWVMkfmHoWFhaioqOj27zo4OBjTpk3D7t27ZYzMfQwGAwRBQEhIiNyhuITFYsEdd9yBRx99FOPGjZM7HJeyWCz4/PPPMXLkSCxYsABRUVGYNm1ar1tXzsRkRCYWiwVLly5Feno6UlNT5Q7HZdavX4+DBw8iMzNT7lBcrqCgACtWrMCIESOwefNmPPjgg/j1r3+Nd999V+7QXOIPf/gDbr75ZowePRo+Pj6YNGkSli5dittuu03u0NyioqICABAdHd3t+9HR0bY/82atra147LHHcMstt3jtYXIvvPACNBoNfv3rX8sdistVVVWhsbERf/7zn3HFFVfgq6++wvXXX48bbrgB2dnZLn99jzi11xtlZGQgJycHO3bskDsUlykpKcHDDz+MLVu2uGfPUWYWiwVTpkzB888/DwCYNGkScnJysHLlStx5550yR+d877//Pv7zn/9g3bp1GDduHA4dOoSlS5ciNjbWK98vdTKZTPjFL34BURSxYsUKucNxiQMHDuCVV17BwYMHIQiC3OG4nMViAQBcd911eOSRRwAAEydOxK5du7By5UrMnj3bpa/PlREZLFmyBBs3bsS2bdsQHx8vdzguc+DAAVRVVeHiiy+GRqOBRqNBdnY2Xn31VWg0GpjNZrlDdKqYmBiMHTu22/fGjBnjlkp0OTz66KO21ZHx48fjjjvuwCOPPDIoVsEAYMiQIQCAysrKbt+vrKy0/Zk3khKR06dPY8uWLV67KvLtt9+iqqoKQ4cOtf38On36NH77298iMTFR7vCcLiIiAhqNRrafYVwZcSNRFPHQQw9hw4YNyMrKQlJSktwhudTll1+OI0eOdPve3XffjdGjR+Oxxx6DWq2WKTLXSE9PP69V++TJkxg2bJhMEblWc3MzVKruv8+o1Wrbb1jeLikpCUOGDME333yDiRMnAgDq6+vx3Xff4cEHH5Q3OBeREpFTp05h27ZtCA8Plzskl7njjjvOq3NbsGAB7rjjDtx9990yReU6vr6+SEtLk+1nGJMRN8rIyMC6devwySefQK/X2/aVg4OD4efnJ3N0zqfX68+rhwkICEB4eLhX1sk88sgjmDFjBp5//nn84he/wN69e7Fq1SqsWrVK7tBc4pprrsFzzz2HoUOHYty4cfj+++/x8ssv45577pE7NKdpbGxEXl6e7evCwkIcOnQIYWFhGDp0KJYuXYpnn30WI0aMQFJSEpYvX47Y2FgsXLhQvqAHoLf3GxMTg5///Oc4ePAgNm7cCLPZbPsZFhYWBl9fX7nCdlhff78/TrZ8fHwwZMgQjBo1yt2hOkVf7/fRRx/FTTfdhFmzZuGyyy7Dpk2b8NlnnyErK8v1wcnSwzNIAbjg45133pE7NLfx5tZeURTFzz77TExNTRW1Wq04evRocdWqVXKH5DL19fXiww8/LA4dOlTU6XRicnKy+Pjjj4tGo1Hu0Jxm27ZtF/xv9s477xRF0dreu3z5cjE6OlrUarXi5ZdfLubm5sob9AD09n4LCwt7/Bm2bds2uUN3SF9/vz/m6a29/Xm/q1evFocPHy7qdDpxwoQJ4scff+yW2ARR9KJxiURERORxWMBKREREsmIyQkRERLJiMkJERESyYjJCREREsmIyQkRERLJiMkJERESyYjJCREREsmIyQkRERLJiMkJERESyYjJCREREsmIyQkRERLJiMkJERESy+n/kO9dOCBbBSwAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!grep  'steps:.*loss:' pretrain.log|awk '{print $2,$4}'>pretrain.loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xy=np.loadtxt('pretrain.loss')  \n",
    "plt.plot(xy[:,0], xy[:,1])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c4dde6-e42f-4a69-be4f-dea56f349737",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!grep  'steps:.*loss:' pretrain.log|awk '{print $2,$4}'>pretrain.loss\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "xy=np.loadtxt('pretrain.loss')  \n",
    "plt.plot(xy[:,0], xy[:,1])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bca33a0-d00d-47eb-af28-bac44d9327b4",
   "metadata": {},
   "source": [
    "##### 正常情况下形如："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b911c41-6d82-4702-9656-74f3d5f762fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAGdCAYAAADAAnMpAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAA9hAAAPYQGoP6dpAAA2R0lEQVR4nO3deXhUhb3/8c+ZSTJZSAIkJCEhEPY1LALSgBsSRQSqt62laoXi1V69uPK0VbRCrQvaVgutFiqt2+3PpbVqLSAoQUpVLJsoW0jYEcjGkgkJ2WbO74+ZDESIJJDMmeX9ep48lclM8g1Phbfne84cwzRNUwAAABaxWT0AAAAIb8QIAACwFDECAAAsRYwAAABLESMAAMBSxAgAALAUMQIAACxFjAAAAEtFWD1Ac7jdbh06dEjx8fEyDMPqcQAAQDOYpqmKigqlp6fLZmv6+EdQxMihQ4eUmZlp9RgAAOA8HDhwQF26dGny80ERI/Hx8ZI8P0xCQoLF0wAAgOZwOp3KzMz0/T3elKCIkYbVTEJCAjECAECQOdcpFpzACgAALEWMAAAASxEjAADAUsQIAACwFDECAAAsRYwAAABLESMAAMBSxAgAALAUMQIAACzV4hhZvXq1Jk+erPT0dBmGoXffffecr1m1apUuuugiORwO9erVSy+//PJ5jAoAAEJRi2OksrJSQ4YM0fPPP9+s5+/Zs0cTJ07U2LFjtWnTJt1333267bbbtHz58hYPCwAAQk+L700zYcIETZgwodnPX7hwobp3765nnnlGktS/f399/PHH+u1vf6vx48e39NsDAIAQ0+bnjKxZs0a5ubmNHhs/frzWrFnT5GtqamrkdDobfbQ2t9vU3zd8pdteWa/yqrpW//oAAKB52jxGioqKlJqa2uix1NRUOZ1OnTx58qyvmTt3rhITE30fmZmZrT6XzWbohdW7tWJ7sT7YVtTqXx8AADRPQF5NM2vWLJWXl/s+Dhw40CbfZ+LgzpKkJZsPt8nXBwAA59bmMZKWlqbi4uJGjxUXFyshIUExMTFnfY3D4VBCQkKjj7ZwbbYnRj4uLGNVAwCARdo8RnJycpSXl9fosQ8//FA5OTlt/a3PqVdKO/VLi1e929RyVjUAAFiixTFy4sQJbdq0SZs2bZLkuXR306ZN2r9/vyTPimXq1Km+599xxx3avXu3fvaznyk/P19/+MMf9Ne//lX3339/6/wEF2ii9+jIki9Z1QAAYIUWx8j69es1bNgwDRs2TJI0c+ZMDRs2TLNnz5YkHT582BcmktS9e3ctWbJEH374oYYMGaJnnnlGf/rTnwLmst5rveeNfLKzTMcqay2eBgCA8GOYpmlaPcS5OJ1OJSYmqry8vE3OH5kw/9/aftipp7+brSkju7b61wcAIBw19+/vgLyaxt8meY+OLGZVAwCA3xEjOnVVzae7jugoqxoAAPyKGJHUPTlOA9MT5HKbWr6Vq2oAAPAnYsSr4Q3QlvIGaAAA+BUx4jXxtFXNkRM1Fk8DAED4IEa8uiXFaVBGw6qm+NwvAAAArYIYOc3E7HRJ0pLNhyyeBACA8EGMnKZhVbNm1xGVsaoBAMAviJHTdE2K1eAuiXKb0rItXFUDAIA/ECNfw71qAADwL2LkaxreAO0/e46opKLa4mkAAAh9xMjXZHaM1ZDM9nKb0nJWNQAAtDli5CwmNaxqeAM0AADaHDFyFhOy0yRJ/9lzlFUNAABtjBg5iy4dYjU0s71MrqoBAKDNESNNmOS9V81irqoBAKBNESNNmOA9b2Td3qMqdrKqAQCgrRAjTchoH6OLunpWNe9zIisAAG2GGPkGEwc33KuGGAEAoK0QI9/gWu9VNev2HlNROasaAADaAjHyDTonxmhEtw6SpKUcHQEAoE0QI+cw0XtVDTECAEDbIEbOYcKgzjIMaf2+YzpcftLqcQAACDnEyDmkJUaftqrhDdAAAGhtxEgzTGy4V82XhyyeBACA0EOMNMOEbM+qZuP+4zp4nFUNAACtiRhphtSEaI3M6iiJN0ADAKC1ESPNxL1qAABoG8RIM10zKE2GIW06cFwHjlZZPQ4AACGDGGmmlPhojeruXdVs4egIAACthRhpgVP3quESXwAAWgsx0gLXDEyTzZC+YFUDAECrIUZaoFO8Q6O6J0ni7eEBAGgtxEgLNdyrZgkxAgBAqyBGWuiaQZ5VzZdflWv/EVY1AABcKGKkhZLbOZTT07Oq4egIAAAXjhg5DxOzG66q4V41AABcKGLkPIwfmCq7zdCWg07tLau0ehwAAIIaMXIekto5NJpVDQAArYIYOU8Tsz1X1XCJLwAAF4YYOU9XD0yT3WZo6yGn9rCqAQDgvBEj56ljXJRvVcPREQAAzh8xcgEmed8AbfGXxAgAAOeLGLkAVw9IU4TN0PbDTu0qPWH1OAAABCVi5AJ0iIvSmF7JkqSlHB0BAOC8ECMXiHvVAABwYYiRCzR+QJoi7Ybyiyq0s6TC6nEAAAg6xMgFSoyN1CXeVc2SL4ssngYAgOBDjLSCiYM996rhEl8AAFqOGGkFVw1IVaTd0I7iChUWs6oBAKAliJFWkBgTqUt7d5LEiawAALQUMdJKGu5Vs4RLfAEAaBFipJXkDkhVlN2mwpITKmBVAwBAsxEjrSQxJlKX9fFcVcPbwwMA0HzESCvyvQHal4dkmqbF0wAAEByIkVaU2z9VURE27Sqt1A5WNQAANAsx0orioyN1eR/PVTXcqwYAgOYhRlrZJO+qZvHmw6xqAABoBmKklY3zrmp2l1Yqv4hVDQAA50KMtLJ2jghd4V3V8J4jAACcGzHSBnxX1bCqAQDgnIiRNjCuf6ocETbtKavUtsNOq8cBACCgESNtoJ0jQmP7pkhiVQMAwLkQI22EVQ0AAM1DjLSRK/ulKDrSpn1HqrT1EKsaAACaQoy0kThHhK7s513VbGZVAwBAU84rRp5//nllZWUpOjpao0aN0tq1a7/x+fPmzVPfvn0VExOjzMxM3X///aqurj6vgYPJtdkN96phVQMAQFNaHCNvvvmmZs6cqTlz5mjjxo0aMmSIxo8fr5KSkrM+/7XXXtODDz6oOXPmaPv27frzn/+sN998Uw899NAFDx/oGlY1+49WactBVjUAAJxNi2Pk2Wef1e23367p06drwIABWrhwoWJjY/Xiiy+e9fmffvqpxowZo5tuuklZWVm6+uqrdeONN57zaEooiI2K0Lh+qZKkxZsPWTwNAACBqUUxUltbqw0bNig3N/fUF7DZlJubqzVr1pz1NaNHj9aGDRt88bF7924tXbpU1157bZPfp6amRk6ns9FHsPJdVcOqBgCAs4poyZPLysrkcrmUmpra6PHU1FTl5+ef9TU33XSTysrKdMkll8g0TdXX1+uOO+74xjXN3Llz9eijj7ZktIA1tm+KYiLt+urYSX35VbmGZLa3eiQAAAJKm19Ns2rVKj355JP6wx/+oI0bN+rtt9/WkiVL9NhjjzX5mlmzZqm8vNz3ceDAgbYes83ERNk1rj9X1QAA0JQWHRlJTk6W3W5XcXFxo8eLi4uVlpZ21tc88sgjuuWWW3TbbbdJkrKzs1VZWakf//jHevjhh2WzndlDDodDDoejJaMFtEmDO2vxl4e15MvDmjWhnwzDsHokAAACRouOjERFRWn48OHKy8vzPeZ2u5WXl6ecnJyzvqaqquqM4LDb7ZIUNudQXNE3RbFRdh08flKbDhy3ehwAAAJKi9c0M2fO1KJFi/TKK69o+/btuvPOO1VZWanp06dLkqZOnapZs2b5nj958mQtWLBAb7zxhvbs2aMPP/xQjzzyiCZPnuyLklAXHWlXbn/PeTZLWdUAANBIi9Y0kjRlyhSVlpZq9uzZKioq0tChQ7Vs2TLfSa379+9vdCTk5z//uQzD0M9//nMdPHhQnTp10uTJk/XEE0+03k8RBK7N7qz3vjikJV8e1kPX9mdVAwCAl2EGwa7E6XQqMTFR5eXlSkhIsHqc81Jd59Lwxz5UZa1Lb//vaF3UtYPVIwEA0Kaa+/c396bxk+hIu3IHeI4eLfmSVQ0AAA2IET+a6L1XzdLNh+V2B/wBKQAA/IIY8aPL+nRSO0eEDpdX6/MDx6weBwCAgECM+FF0pF1XeVc1i1nVAAAgiRjxO1Y1AAA0Roz42aV9khXviFCxs0Yb9rOqAQCAGPEzR4RdVw3kqhoAABoQIxZgVQMAwCnEiAUu6Z2s+OgIlVTUaP0+VjUAgPBGjFjAEWHX1QM8dzle8uUhi6cBAMBaxIhFJg32rmq2FMnFqgYAEMaIEYuM6ZWshOgIlVbUaN3eo1aPAwCAZYgRi0RF2DR+YMOqhqtqAADhixix0ETvqub9LYdZ1QAAwhYxYqExvZKVGBOpshO1+s+eI1aPAwCAJYgRC0XabbrGu6pZuplVDQAgPBEjFrvWu6pZtqVI9S63xdMAAOB/xIjFRvdMUvtYz6pm7R6uqgEAhB9ixGKnr2oWs6oBAIQhYiQATGRVAwAIY8RIAMjpkaQOsZE6Wlmrz3azqgEAhBdiJABE2G26ZpDn6MiSzdyrBgAQXoiRADHptFVNHasaAEAYIUYCxKjuHZUUF6VjVXVas4s3QAMAhA9iJEB4VjW8ARoAIPwQIwFkYrZ3VbOVVQ0AIHwQIwHk4u4dldwuSser6vQpqxoAQJggRgLI6auaJV9yVQ0AIDwQIwFmYna6JGn51mLV1rOqAQCEPmIkwHhWNQ6Vn6zTJ7vKrB4HAIA2R4wEGLvN0LXZDasarqoBAIQ+YiQANVxV88HWIlY1AICQR4wEoBFZHZUS75Czul6f7GRVAwAIbcRIALLbDE3wXlWzmFUNACDEESMBauJgz1U1H2wrUk29y+JpAABoO8RIgBrRrYNS4h2qqK7Xx4WsagAAoYsYCVA2m6FrvSeyclUNACCUESMBbNJgT4x8uK1Y1XWsagAAoYkYCWAXde2gtIRoVdTU69+sagAAIYoYCWCNVzXcqwYAEJqIkQA30buqWbG9hFUNACAkESMBblhme6UnRutETb1WF5RaPQ4AAK2OGAlwNpuhCQ2rms1cVQMACD3ESBDwrWq4qgYAEIKIkSAwLLO9MtrHqLLWpVU7WNUAAEILMRIEDMPQtdmee9WwqgEAhBpiJEg03Ksmb3uxTtayqgEAhA5iJEgM6ZKojPYxqqp1adWOEqvHAQCg1RAjQcIwDN/bwy9mVQMACCHESBBpuKpm5fYSVjUAgJBBjASR7IxEZXaM0ck6lz5iVQMACBHESBDxXFXTcK8aVjUAgNBAjASZSdneq2ryi1VVW2/xNAAAXDhiJMgMykhQ146xqq5za2U+qxoAQPAjRoKMYRi+E1lZ1QAAQgExEoQmes8bWZlfosoaVjUAgOBGjAShgekJykqKVU29W3msagAAQY4YCUKNVzWHLJ4GAIALQ4wEqYneq2o+2lGqE6xqAABBjBgJUv07x6tHcpxq693K215s9TgAAJw3YiRI8QZoAIBQQYwEsYbzRlYVlKqius7iaQAAOD/ESBDrlxavHp0aVjVcVQMACE7ESBAzDEOTvKuaxaxqAABBihgJchMHe66qWV1QKierGgBAECJGglyf1HbqldJOtS63VmzjqhoAQPA5rxh5/vnnlZWVpejoaI0aNUpr1679xucfP35cM2bMUOfOneVwONSnTx8tXbr0vAZGY4Zh+N4enqtqAADBqMUx8uabb2rmzJmaM2eONm7cqCFDhmj8+PEqKTn7CZS1tbW66qqrtHfvXr311lvasWOHFi1apIyMjAseHh4NV9WsLixV+UlWNQCA4NLiGHn22Wd1++23a/r06RowYIAWLlyo2NhYvfjii2d9/osvvqijR4/q3Xff1ZgxY5SVlaXLL79cQ4YMueDh4dEnNV69U9qpzmWyqgEABJ0WxUhtba02bNig3NzcU1/AZlNubq7WrFlz1te89957ysnJ0YwZM5SamqpBgwbpySeflMvlavL71NTUyOl0NvrAN/Pdq2YzqxoAQHBpUYyUlZXJ5XIpNTW10eOpqakqKio662t2796tt956Sy6XS0uXLtUjjzyiZ555Ro8//niT32fu3LlKTEz0fWRmZrZkzLDUcN7IvwtLVV7FqgYAEDza/Goat9utlJQUvfDCCxo+fLimTJmihx9+WAsXLmzyNbNmzVJ5ebnv48CBA209ZtDrnRqvvqnxqnOZ+mDb2cMQAIBA1KIYSU5Olt1uV3Fx4/MSiouLlZaWdtbXdO7cWX369JHdbvc91r9/fxUVFam2tvasr3E4HEpISGj0gXNjVQMACEYtipGoqCgNHz5ceXl5vsfcbrfy8vKUk5Nz1teMGTNGO3fulNvt9j1WUFCgzp07Kyoq6jzHxtk03Djv48IyHa86e+gBABBoWrymmTlzphYtWqRXXnlF27dv15133qnKykpNnz5dkjR16lTNmjXL9/w777xTR48e1b333quCggItWbJETz75pGbMmNF6PwUkSb1S2qlfWrzq3aY+2MpVNQCA4BDR0hdMmTJFpaWlmj17toqKijR06FAtW7bMd1Lr/v37ZbOdapzMzEwtX75c999/vwYPHqyMjAzde++9euCBB1rvp4DPpMGdlV9UocWbD+v7IznxFwAQ+AzTNE2rhzgXp9OpxMRElZeXc/7IOewuPaErn/mX7DZD6x/OVYc4VmEAAGs09+9v7k0TYnp0aqf+nRPkcnNVDQAgOBAjIWiS96qaxdyrBgAQBIiRENRwVc2nu47oaCVX1QAAAhsxEoK6J8dpYLpnVbN8K6saAEBgI0ZClO8N0FjVAAACHDESoib6VjVlOnKixuJpAABoGjESorolxSk7I1FuU1rGqgYAEMCIkRDGqgYAEAyIkRDWsKr5bPcRlbGqAQAEKGIkhGV2jNXgLt5VzRZWNQCAwESMhLiGoyOsagAAgYoYCXENb4D2nz1HVFJRbfE0AACciRgJcZkdYzUks73cprScVQ0AIAARI2FgUjb3qgEABC5iJAxMyE6TJK3de1QlTlY1AIDAQoyEgS4dYjWsa3uZpvQ+qxoAQIAhRsIEV9UAAAIVMRImGq6qWbfvqIpZ1QAAAggxEibS28foooZVzWaOjgAAAgcxEkYmDk6XJC0hRgAAAYQYCSPXeq+qWbf3mIrKWdUAAAIDMRJGOifGaES3DpKkpRwdAQAECGIkzEwc7L2qhhgBAAQIYiTMTBjUWYYhbdh3TIeOn7R6HAAAiJFwk5YYrZHdOkpiVQMACAzESBhiVQMACCTESBiaMChNhiF9vv+4DrKqAQBYjBgJQykJ0RqZ5VnV8AZoAACrESNhapJ3VbOYe9UAACxGjISpa7yrmk0HjuvA0SqrxwEAhDFiJEylxEdrVHeuqgEAWI8YCWOTvPeqmZ9XqHV7j1o8DQAgXBEjYeyGEV10Sa9kVdW6NO3FtVq7hyABAPgfMRLGHBF2/WnaCF3a2xMkP3pprT7bfcTqsQAAYYYYCXPRkXYtmnoqSKa/tE5rdhEkAAD/IUbgC5LL+3TSyTqXbn15nT7dVWb1WACAMEGMQJInSP54y3Bd0fe0INlJkAAA2h4xAp/oSLsW/nC4xvbtpOo6t6a/vE6fECQAgDZGjKCR6Ei7Ft4yXFf2S1FNvVu3vrxOHxcSJACAtkOM4AyOCLsW/PAi5fb3BMl/v7JOqwtKrR4LABCiiBGclSPCrudvvki5/VNVU+/Wba+u178IEgBAGyBG0CRHhF1/uPkiXTUgVbX1bt3+6nqt2lFi9VgAgBBDjOAbRUXY9PxNF2n8QE+Q/PjVDfoonyABALQeYgTnFBVh03M3XaRrBqap1uXW//zfBq3ML7Z6LABAiCBG0CyRdpt+f9MwTRjkCZI7/m+j8rYTJACAC0eMoNki7Tb97sZhmpjd2RMkf9mgFdsIEgDAhSFG0CKRdpvm/WCoJg7urDqXqTv/3wZ9sLXI6rEAAEGMGEGLRdptmj9lqCYPSVedy9SM1zZqOUECADhPxAjOS4Tdpt9+f4i+3RAk/2+jlm0hSAAALUeM4LxF2G169vtDdN3QdNW7Td312ka9v/mw1WMBAIIMMYIL4gmSofqvYRmeIHn9cy0lSAAALRBh9QAIfnabod/cMESGpLc/P6i7X/9cpilNHNzZ6tEAAEGAGEGrsNsM/fqGIZIhvb3xoO5543O5TVOTh6RbPRoAIMARI2g1dpuhX39viGyGobc2fKX73twkU9K3CRIAwDcgRtCq7DZDv/ruYBmS/rbhK933xucyTVPXDc2wejQAQIDiBFa0OpvN0NPfHawpIzLlNqX739ykdz8/aPVYAIAARYygTdhshuZ+J1s/GOkJkpl/3aR3Pv/K6rEAAAGIGEGbsdkMPflf2brx4q7eIPlCf99AkAAAGiNG0KZsNkNPXD9IN43qKtOUfvLWF3qLIAEAnIYYQZuz2Qw9ft0g/fBbniD56Vtf6G/rD1g9FgAgQBAj8AubzdBj1w3SLd/qJtOUfvb3L/XXdQQJAIAYgR8ZhqFfXjdQ03JOBcmb6/ZbPRYAwGLECPzKMAz94tsD9aPRWZKkB/6+Wa+vJUgAIJwRI/A7wzA0Z/IATR+TJUma9fZmvfYfggQAwhUxAksYhqHZkwbo1jHdJUkPvbNZf/lsn8VTAQCsQIzAMoZh6JFJ/XXbJZ4g+fm7W/R/BAkAhJ3zipHnn39eWVlZio6O1qhRo7R27dpmve6NN96QYRi6/vrrz+fbIgQZhqGHJ/bX7Zd6guSRd7fo1TV7rR0KAOBXLY6RN998UzNnztScOXO0ceNGDRkyROPHj1dJSck3vm7v3r36yU9+oksvvfS8h0VoMgxDD13bX/9zWQ9J0ux/bNUrn+61digAgN+0OEaeffZZ3X777Zo+fboGDBighQsXKjY2Vi+++GKTr3G5XLr55pv16KOPqkePHhc0MEKTYRh6cEI/3XF5T0nSnPe26qVP9lg8FQDAH1oUI7W1tdqwYYNyc3NPfQGbTbm5uVqzZk2Tr/vlL3+plJQU/fd//3ezvk9NTY2cTmejD4Q+wzD0wDV9decVniB59J/b9OePCRIACHUtipGysjK5XC6lpqY2ejw1NVVFRUVnfc3HH3+sP//5z1q0aFGzv8/cuXOVmJjo+8jMzGzJmAhihmHoZ+P7asZYT5A8tnib/vTv3RZPBQBoS216NU1FRYVuueUWLVq0SMnJyc1+3axZs1ReXu77OHCAtw0PJ4Zh6CdX99XdV/aSJD2+ZDtBAgAhLKIlT05OTpbdbldxcXGjx4uLi5WWlnbG83ft2qW9e/dq8uTJvsfcbrfnG0dEaMeOHerZs+cZr3M4HHI4HC0ZDSHGMAzNvKqPDEm/W7lTjy/ZLrdp6seXnfn/FwBAcGvRkZGoqCgNHz5ceXl5vsfcbrfy8vKUk5NzxvP79eunzZs3a9OmTb6Pb3/72xo7dqw2bdrE+gXfyDAMzby6r+4d11uS9OTSfC381y6LpwIAtLYWHRmRpJkzZ2ratGkaMWKELr74Ys2bN0+VlZWaPn26JGnq1KnKyMjQ3LlzFR0drUGDBjV6ffv27SXpjMeBptx/VR8ZhjRvRaGeej9fpinfSa4AgODX4hiZMmWKSktLNXv2bBUVFWno0KFatmyZ76TW/fv3y2bjjV3Ruu7L7SNDhn67okBPL8uX2zQ1Y2wvq8cCALQCwzRN0+ohzsXpdCoxMVHl5eVKSEiwehxY6Pd5hXrmwwJJ0k+u7qO7ruxt8UQAgKY09+9vDmEgqNw9rrd+Or6vJOk3HxTo93mFFk8EALhQxAiCzoyxvXxB8syHBZq/giABgGBGjCAozRjbSw9c00+S9NsVBfqtd3UDAAg+xAiC1p1X9NSsCZ4gmZ9XqGc/LFAQnAIFAPgaYgRB7X8u76mHrvUEye/yCvVbggQAgg4xgqD348t66ucT+0vyvFvrMx8QJAAQTIgRhITbLu3hC5LnPtqp33ywgyABgCBBjCBk3HZpD82eNECS9PxHu/Sr5QQJAAQDYgQh5dZLuusXkz1BsmDVLj29jCABgEBHjCDk/GhMdz367YGSpIX/2uW9nw1BAgCBihhBSJo2Oku/vM4TJH9cvVtPLt1OkABAgCJGELKm5mTpses9d4de9O89emIJQQIAgYgYQUi75Vvd9MR/eYLkTx/v0WOLCRIACDTECELezaO66cn/ypYkvfjJHv1y8TaCBAACCDGCsHDTqK566jueIHnpk7169J8ECQAECmIEYeMHF3fV09/NlmFIL3+6V3Pe20qQAEAAIEYQVqaM7KqnvzNYhiG9umafZv+DIAEAqxEjCDvfH5mpX33XEyT/99k+Pfj3zXJW11k9FgCELWIEYemGEZn69feGyDCkN9cf0CVPrdS8FQUqP0mUAIC/ESMIW98b3kV/njZCvVPayVldr3krCnXJ0yv12w+JEgDwJ8MMgoW50+lUYmKiysvLlZCQYPU4CDFut6mlWw7rd3mFKig+IUmKd0Ro+pgs3XpJd7WPjbJ4QgAITs39+5sYAbzcblPLthZp/opC7SiukCS1c0ToR6OzdNulRAkAtBQxApwnt9vU8q1Fmp9XqPyiU1EybXQ33XZJD3WII0oAoDmIEeACud2mPthWrN/lFWrbYackKS7Krqmjs3T7pT3UkSgBgG9EjACtxDRNfbitWPPzCrX1kCdKYqPsuiWnm358aQ8ltXNYPCEABCZiBGhlpmlqxfYSzc8r0JaDniiJibRrak433X5ZDyUTJQDQCDECtBHTNLUyv0Tz8wr15VflkjxR8sNvddWPL+upTvFECQBIxAjQ5kzT1KodpZq3okBfeKMkOtKmm0d10/9c3kMp8dEWTwgA1iJGAD8xTVOrCko1f0WhNh04LklyRHii5I7LeyglgSgBEJ6IEcDPTNPU6sIyzV9RoI37j0vyRMmNF3fVnVf0VCpRAiDMECOARUzT1Mc7yzRvRaE27DsmSYqKsOnGkZm684peSkskSgCEB2IEsJhpmvpk5xHNzyvQur3eKLHb9IOLM3XnFT3VOTHG4gkBoG0RI0CAME1Ta3Yd0by8Qq3dc1SSJ0q+P7KL/veKXkpvT5QACE3ECBCA1uw6onkrCvQfb5RE2g19f0Sm/ndsL2UQJQBCDDECBLDPdh/R/BWFWrP7iCRPlHxveKb+94qeyuwYa/F0ANA6iBEgCPxn9xHNzyvUp7s8URJhM/S94V00Y2wvogRA0CNGgCCybu9RzV9RqI93lknyRMl3LsrQXWN7q2sSUQIgOBEjQBDasO+o5q0o1L8LPVFitxn6zrAM3XVlL3VLirN4OgBoGWIECGIb9h3T/LxCrS4oleSJkuuHZujuK3spK5koARAciBEgBGzcf0y/yyvUqh2eKLEZ0vVDPUdKenRqZ/F0APDNiBEghGw6cFy/yyvUyvwSSZ4ouc4bJT2JEgABihgBQtAX3ijJOy1KJg9J191X9lavFKIEQGAhRoAQtvmrcs3PK9SK7cWSJMOQJg1O1z1X9lLv1HiLpwMAD2IECANbDpbrd3mF+mDbqSiZmN1Z94zrrT5ECQCLESNAGNl6yBMly7eeipJrB3mipG8aUQLAGsQIEIa2HXLq9ysL9f6WIt9jEwal6Z5xvdW/M//uAPAvYgQIY/lFTv0+b6eWbD7se+yagZ4oGZDOv0MA/IMYAaAdRRX63cpCLd18WA3/pl89IFX3jOutQRmJ1g4HIOQRIwB8Coor9PuVO7X4y0O+KMntn6r7cokSAG2HGAFwhp0lnih574tTUTKuX4puGNFFl/TupHaOCGsHBBBSiBEATdpZckLPrSzUe18cktv7J0CU3aZRPTpqXL8UjeufqsyO3C0YwIUhRgCc067SE/p/n+1XXn6x9h2pavS5PqntdGW/VOX2T9Gwrh1ktxkWTQkgWBEjAJrNNE3tKq3Uyvxi5W0v0fp9x+Ryn/qjoUNspK7om6Jx/VN0WZ9OSoiOtHBaAMGCGAFw3o5X1epfBaVamV+iVTtKVX6yzve5CJuhkVkdNa6/Z53TPTnOwkkBBDJiBECrqHe5tWHfMa3ML9GK7cXaVVrZ6PM9kuN0pfc8kxFZHRRpt1k0KYBAQ4wAaBN7yyq1Mr9EK/NL9J89R1TnOvVHSHx0hC7v00nj+qfoij4p6hAXZeGkAKxGjABocxXVdfp3YZnytpfoox0lOlpZ6/uczZCGd+vgOwm2V0o7GQYnwQLhhBgB4Fcut6lNB477ToLNL6po9PnMjjEa1y9V4/qn6OLuHeWIsFs0KQB/IUYAWOqrY1X6KL9EK7aXaM2uI6p1uX2fi4uy69LennXO2H4pSm7nsHBSAG2FGAEQMCpr6vXJTs86Z+WOEpVW1Pg+ZxjSkC7tlds/RVf2S1X/zvGsc4AQQYwACEhut6kth8qVt71EefnF2nLQ2ejznROjdWW/FOX2T1VOzyRFR7LOAYIVMQIgKBSVV+ujHSXK216sj3eWqbru1DonOtKmS3ola1z/VF3ZL0WpCdEWTgqgpYgRAEGnus6lNbuOKM97Euzh8upGnx+UkeA7CXZQeqJsvEU9ENCIEQBBzTRNbT9coZX5xVqxvURffHVcp/9p1SneoSu9b1F/Se9kxUZxx2Eg0BAjAEJKaUWNVu3wvNna6oJSVda6fJ+LirApp0eScr1X53TpwB2HgUDQpjHy/PPP69e//rWKioo0ZMgQ/f73v9fFF1981ucuWrRIr776qrZs2SJJGj58uJ588skmn382xAiA09XUu7R2z1HfSbAHjp5s9Pl+afEa5706Z2hme+44DFikzWLkzTff1NSpU7Vw4UKNGjVK8+bN09/+9jft2LFDKSkpZzz/5ptv1pgxYzR69GhFR0fr6aef1jvvvKOtW7cqIyOjVX8YAOHHNE3tLDmhvHzPSbAb9h3TaTccVlJclO+Ow5f2TlY8dxwG/KbNYmTUqFEaOXKknnvuOUmS2+1WZmam7r77bj344IPnfL3L5VKHDh303HPPaerUqc36nsQIgOY6Vum54/CK7cX6V0GpKqrrfZ+LtBu6uHtH30mw3ZK44zDQlpr793eLzviqra3Vhg0bNGvWLN9jNptNubm5WrNmTbO+RlVVlerq6tSxY8eWfGsAaJYOcVG6fliGrh+WoTqXW+v3HlPe9mKtzC/R7rJKfbLziD7ZeUS/XLxNPTvFKbd/qkb3SlZ2RqI6cmM/wBItipGysjK5XC6lpqY2ejw1NVX5+fnN+hoPPPCA0tPTlZub2+RzampqVFNz6h0anU5nk88FgKZE2m3K6ZmknJ5J+vmkAdpdekIr80uUt71E6/Ye1a7SSu0q3a0/rt4tScpoH6NBGQnKzkjUoIxEZWckKom3qgfanF+vhXvqqaf0xhtvaNWqVYqObvrNi+bOnatHH33Uj5MBCAc9OrVTj07tdNulPVR+sk7/LizVyu0l+vzAce0pq9TB4yd18PhJLd9a7HtN58RoX5g0REqneAIFaE0tOmektrZWsbGxeuutt3T99df7Hp82bZqOHz+uf/zjH02+9je/+Y0ef/xxrVixQiNGjPjG73O2IyOZmZmcMwKgzTir67T1oFNbDpZr88FybTlYrt1llWd9blrCaYHSJUGD0hOVwrvDAmdok3NGoqKiNHz4cOXl5flixO12Ky8vT3fddVeTr/vVr36lJ554QsuXLz9niEiSw+GQw8F/eQDwn4ToSN9Kp0FFdZ22HXL64mSzN1CKnNUqclZrxfZTR1BS4h2N1jvZXRJ5+3qgmVq8ppk5c6amTZumESNG6OKLL9a8efNUWVmp6dOnS5KmTp2qjIwMzZ07V5L09NNPa/bs2XrttdeUlZWloqIiSVK7du3Url27VvxRAKB1xUdHalSPJI3qcSpQTtTUNwqULQfLtav0hEoqajyXF+eX+J7bqSFQ0hM8kdIlUWkJ0dyVGPiaFsfIlClTVFpaqtmzZ6uoqEhDhw7VsmXLfCe17t+/Xzabzff8BQsWqLa2Vt/73vcafZ05c+boF7/4xYVNDwB+1s4RoYu7d9TF3U9dEVhVeypQGiJlZ8kJlVbUaGW+511jGyS3i/IdPRnk/UhPJFAQ3ng7eABoA1W19dp+uKLROSiFJSfkcp/5R25SXJQGZiQq+7QreTLaxxAoCHrcmwYAAkx1nUvbDjt9653NB50qLK5Q/VkCpUNspO/IScOVPF06ECgILsQIAASB6jqX8osqPEdPvvIcRSloIlDax0ZqUHriaWueBHXtGEugIGARIwAQpKrrXCoormh0Fc+OogrVuc784zohOqLROSjZGYnqlkSgIDAQIwAQQmrqXSooOuEJlEOeSMk/XKFal/uM58ZHR2hQuufqnYHpnvNQspLiZOPuxfAzYgQAQlxtvVsFxY1Pkt1eVKHa+rMEiiNCA7xhkt3FcxSlO4GCNkaMAEAYqnO5VVh8whcomw+Wa/thp2rOEihxUXYNTE/UwIwE9UiOU7ekOHVPjlN6+xjZiRS0AmIEACBJqne5VVjiWfFs9QbKtsNOVdedGSiSFGk3lNkhVlnJceqWFKuspDhlJccpKylWGe1jFGG3nfV1wNcRIwCAJtW73NpVWqnNB8uVf9ipvUeqtO9IpfYdrTrrmqdBhM1QZsfYU5GSFKtuyXHKSopTlw4xiiRUcJo2uTcNACA0RNht6psWr75p8Y0ed7lNFTmrta+sUnuOVGrfkSrtLavUXu8/19S7taesUnvKKiWVNnqt3WaoS4cYz7onKVbdkuKUlRzrDZVYRUUQKjg7jowAAJrF7TZVXFGtPWVnRsreI5VNrn0kyWZIGR1ivEdTPOuf7t7zVDI7xsgRYffjTwJ/YU0DAPAbt9tUSUWN9h6p9EaKZ+3TEC4n61xNvtZmSOntY86IlKykWGV2jFV0JKESrIgRAEBAME1TpRU1p46oHKn0Rovnn6tqmw4Vw5DSE2M856h4T6JtOKG2K6ES8IgRAEDAM01TpSdqtO9IlTdWPEdV9nrD5URN/Te+Pj0xutG5KQ3/3K1jnGKiCBWrESMAgKBmmqaOVNaede2zt6xSFecIlbSE6DPWPg2XK8dGcf2GPxAjAICQZZqmjlbW+iLl68HirP7mUEmJd3jXPZ6rfrp0iFFqQrTSEqKVlhjN+qeVECMAgLBkmqaOV9WdcW5KQ6wcr6o759doHxuptIRopSZEq3Oi53/TEk/FSlpCtNrHRnJDwnPgfUYAAGHJMAx1iItSh7goDeva4YzPH6+qPWPtc+j4SRU7q1XkrFZ1nVvHq+p0vKpO+UUVTX4fR4TtjEg5/ehKWmK0UuIdvBFcMxAjAICw0j42SkNjozQ0s/0ZnzNNU86T9TrsPKmi8mpPoJTXqMj76yJnjYqd1TpaWauaerf2H63S/qNVTX4vw5CS4hynHV1xeGMlxvu/DqUmRCs+OrINf+LAR4wAAOBlGIYSYyOVGBupfmlNrxWq61wqcdaoyHs0paj8pIrKa3xHVxpCpt5tquxEjcpO1GjzwfImv147R4RSExy+oyudvUdbUk87ypIc5wjZuywTIwAAtFB0pF1dk2LVNSm2yee43Z6rgYqd1Tpc7omU4vJTsdLw64qaep2oqdeJ0nrtKq1s8utF2AylxDuUmnjaeSxnWRMF48m3xAgAAG3AZjPUKd6hTvEODcpIbPJ5J2rqT1sJfS1WvP9ceqJG9W5Th8qrdai8Wp9/w/dtOPk27bSjK50To5XaEC0BePItMQIAgIXaOSLUK6WdeqW0a/I5dS63SitqfEdTDjfEy9cCpqa++Sffnn7CbefEaN14cVdlJce1xY94TsQIAAABLtJuU3r7GKW3j2nyOaZpqvxknYq8a6Hi046uHC4/dR7Lsao61dS7te9IlfYdOXXy7dUD04gRAABw/gzDUPvYKLWPjTrnybfFX1sFHS6vVrdvOP+lrREjAACEkehIu7p57+MTKHgnFgAAYCliBAAAWIoYAQAAliJGAACApYgRAABgKWIEAABYihgBAACWIkYAAICliBEAAGApYgQAAFiKGAEAAJYiRgAAgKWIEQAAYKmguGuvaZqSJKfTafEkAACguRr+3m74e7wpQREjFRUVkqTMzEyLJwEAAC1VUVGhxMTEJj9vmOfKlQDgdrt16NAhxcfHyzCMVvu6TqdTmZmZOnDggBISElrt6waTcP89CPefX+L3gJ8/vH9+id+Dtvz5TdNURUWF0tPTZbM1fWZIUBwZsdls6tKlS5t9/YSEhLD8P+Dpwv33INx/fonfA37+8P75JX4P2urn/6YjIg04gRUAAFiKGAEAAJYK6xhxOByaM2eOHA6H1aNYJtx/D8L955f4PeDnD++fX+L3IBB+/qA4gRUAAISusD4yAgAArEeMAAAASxEjAADAUsQIAACwVFjGyOrVqzV58mSlp6fLMAy9++67Vo/kV3PnztXIkSMVHx+vlJQUXX/99dqxY4fVY/nVggULNHjwYN+b/OTk5Oj999+3eizLPPXUUzIMQ/fdd5/Vo/jNL37xCxmG0eijX79+Vo/lVwcPHtQPf/hDJSUlKSYmRtnZ2Vq/fr3VY/lNVlbWGf8fMAxDM2bMsHo0v3C5XHrkkUfUvXt3xcTEqGfPnnrsscfOeR+ZthAU78Da2iorKzVkyBDdeuut+s53vmP1OH73r3/9SzNmzNDIkSNVX1+vhx56SFdffbW2bdumuLg4q8fziy5duuipp55S7969ZZqmXnnlFV133XX6/PPPNXDgQKvH86t169bpj3/8owYPHmz1KH43cOBArVixwvfriIjw+SPx2LFjGjNmjMaOHav3339fnTp1UmFhoTp06GD1aH6zbt06uVwu36+3bNmiq666SjfccIOFU/nP008/rQULFuiVV17RwIEDtX79ek2fPl2JiYm65557/DpL+Pybd5oJEyZowoQJVo9hmWXLljX69csvv6yUlBRt2LBBl112mUVT+dfkyZMb/fqJJ57QggUL9Nlnn4VVjJw4cUI333yzFi1apMcff9zqcfwuIiJCaWlpVo9hiaefflqZmZl66aWXfI91797dwon8r1OnTo1+/dRTT6lnz566/PLLLZrIvz799FNdd911mjhxoiTPkaLXX39da9eu9fssYbmmQWPl5eWSpI4dO1o8iTVcLpfeeOMNVVZWKicnx+px/GrGjBmaOHGicnNzrR7FEoWFhUpPT1ePHj108803a//+/VaP5DfvvfeeRowYoRtuuEEpKSkaNmyYFi1aZPVYlqmtrdVf/vIX3Xrrra16Q9ZANnr0aOXl5amgoECS9MUXX+jjjz+25D/Ww/LICE5xu9267777NGbMGA0aNMjqcfxq8+bNysnJUXV1tdq1a6d33nlHAwYMsHosv3njjTe0ceNGrVu3zupRLDFq1Ci9/PLL6tu3rw4fPqxHH31Ul156qbZs2aL4+Hirx2tzu3fv1oIFCzRz5kw99NBDWrdune655x5FRUVp2rRpVo/nd++++66OHz+uH/3oR1aP4jcPPvignE6n+vXrJ7vdLpfLpSeeeEI333yz/4cxw5wk85133rF6DMvccccdZrdu3cwDBw5YPYrf1dTUmIWFheb69evNBx980ExOTja3bt1q9Vh+sX//fjMlJcX84osvfI9dfvnl5r333mvdUBY7duyYmZCQYP7pT3+yehS/iIyMNHNycho9dvfdd5vf+ta3LJrIWldffbU5adIkq8fwq9dff93s0qWL+frrr5tffvml+eqrr5odO3Y0X375Zb/PwpGRMHbXXXdp8eLFWr16tbp06WL1OH4XFRWlXr16SZKGDx+udevWaf78+frjH/9o8WRtb8OGDSopKdFFF13ke8zlcmn16tV67rnnVFNTI7vdbuGE/te+fXv16dNHO3futHoUv+jcufMZRwL79++vv//97xZNZJ19+/ZpxYoVevvtt60exa9++tOf6sEHH9QPfvADSVJ2drb27dunuXPn+v3oGDEShkzT1N1336133nlHq1atCruT1pridrtVU1Nj9Rh+MW7cOG3evLnRY9OnT1e/fv30wAMPhF2ISJ6TeXft2qVbbrnF6lH8YsyYMWdc0l9QUKBu3bpZNJF1XnrpJaWkpPhO5AwXVVVVstkanzpqt9vldrv9PktYxsiJEyca/dfPnj17tGnTJnXs2FFdu3a1cDL/mDFjhl577TX94x//UHx8vIqKiiRJiYmJiomJsXg6/5g1a5YmTJigrl27qqKiQq+99ppWrVql5cuXWz2aX8THx59xjlBcXJySkpLC5tyhn/zkJ5o8ebK6deumQ4cOac6cObLb7brxxhutHs0v7r//fo0ePVpPPvmkvv/972vt2rV64YUX9MILL1g9ml+53W699NJLmjZtWlhd2i15rip84okn1LVrVw0cOFCff/65nn32Wd16663+H8bvi6EA8NFHH5mSzviYNm2a1aP5xdl+dknmSy+9ZPVofnPrrbea3bp1M6OiosxOnTqZ48aNMz/44AOrx7JUuJ0zMmXKFLNz585mVFSUmZGRYU6ZMsXcuXOn1WP51T//+U9z0KBBpsPhMPv162e+8MILVo/kd8uXLzclmTt27LB6FL9zOp3mvffea3bt2tWMjo42e/ToYT788MNmTU2N32cxTNOCt1oDAADw4n1GAACApYgRAABgKWIEAABYihgBAACWIkYAAICliBEAAGApYgQAAFiKGAEAAJYiRgAAgKWIEQAAYCliBAAAWIoYAQAAlvr/9wMNKIz6V1QAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xy[:,0], 1/xy[:,0])  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac91532-f8f0-4bb5-9467-11e3bd00a575",
   "metadata": {},
   "source": [
    "### 5 参数转换"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f01fbc2-a6bf-4d5a-9835-5b73550015f7",
   "metadata": {},
   "source": [
    "#### 5.1 checkpoint转huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "84f687c0-1659-4f5a-b5d3-762c59821ba8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-11-11 22:43:35,757] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-11 22:43:37,166] [WARNING] [runner.py:212:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2024-11-11 22:43:37,167] [INFO] [runner.py:585:main] cmd = /home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgMywgNCwgNSwgNiwgN119 --master_addr=127.0.0.1 --master_port=29500 --module --enable_each_rank_log=None jllm.train_pipe --model Qwen2.5-7B-Instruct --train_data wikipedia-zh-cn-512_Qwen2.5-7B-Instruct --pipe_parallel_size 2 --model_parallel_size 2 --partition_method fast --split_dlayer --num_train_epochs 0 --from_ckpt checkpoint --output_dir pretrained\n",
      "[2024-11-11 22:43:42,305] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-11 22:43:43,721] [INFO] [launch.py:146:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]}\n",
      "[2024-11-11 22:43:43,722] [INFO] [launch.py:152:main] nnodes=1, num_local_procs=8, node_rank=0\n",
      "[2024-11-11 22:43:43,722] [INFO] [launch.py:163:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3, 4, 5, 6, 7]})\n",
      "[2024-11-11 22:43:43,722] [INFO] [launch.py:164:main] dist_world_size=8\n",
      "[2024-11-11 22:43:43,722] [INFO] [launch.py:168:main] Setting ASCEND_RT_VISIBLE_DEVICES=0,1,2,3,4,5,6,7\n",
      "[2024-11-11 22:43:43,740] [INFO] [launch.py:256:main] process 73485 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=0', '--model', 'Qwen2.5-7B-Instruct', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--partition_method', 'fast', '--split_dlayer', '--num_train_epochs', '0', '--from_ckpt', 'checkpoint', '--output_dir', 'pretrained']\n",
      "[2024-11-11 22:43:43,753] [INFO] [launch.py:256:main] process 73486 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=1', '--model', 'Qwen2.5-7B-Instruct', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--partition_method', 'fast', '--split_dlayer', '--num_train_epochs', '0', '--from_ckpt', 'checkpoint', '--output_dir', 'pretrained']\n",
      "[2024-11-11 22:43:43,766] [INFO] [launch.py:256:main] process 73487 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=2', '--model', 'Qwen2.5-7B-Instruct', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--partition_method', 'fast', '--split_dlayer', '--num_train_epochs', '0', '--from_ckpt', 'checkpoint', '--output_dir', 'pretrained']\n",
      "[2024-11-11 22:43:43,779] [INFO] [launch.py:256:main] process 73488 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=3', '--model', 'Qwen2.5-7B-Instruct', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--partition_method', 'fast', '--split_dlayer', '--num_train_epochs', '0', '--from_ckpt', 'checkpoint', '--output_dir', 'pretrained']\n",
      "[2024-11-11 22:43:43,793] [INFO] [launch.py:256:main] process 73489 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=4', '--model', 'Qwen2.5-7B-Instruct', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--partition_method', 'fast', '--split_dlayer', '--num_train_epochs', '0', '--from_ckpt', 'checkpoint', '--output_dir', 'pretrained']\n",
      "[2024-11-11 22:43:43,807] [INFO] [launch.py:256:main] process 73490 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=5', '--model', 'Qwen2.5-7B-Instruct', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--partition_method', 'fast', '--split_dlayer', '--num_train_epochs', '0', '--from_ckpt', 'checkpoint', '--output_dir', 'pretrained']\n",
      "[2024-11-11 22:43:43,827] [INFO] [launch.py:256:main] process 73491 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=6', '--model', 'Qwen2.5-7B-Instruct', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--partition_method', 'fast', '--split_dlayer', '--num_train_epochs', '0', '--from_ckpt', 'checkpoint', '--output_dir', 'pretrained']\n",
      "[2024-11-11 22:43:43,850] [INFO] [launch.py:256:main] process 73492 spawned with command: ['/home/ma-user/anaconda3/envs/PyTorch-2.1.0/bin/python3.9', '-u', '-m', 'jllm.train_pipe', '--local_rank=7', '--model', 'Qwen2.5-7B-Instruct', '--train_data', 'wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', '--pipe_parallel_size', '2', '--model_parallel_size', '2', '--partition_method', 'fast', '--split_dlayer', '--num_train_epochs', '0', '--from_ckpt', 'checkpoint', '--output_dir', 'pretrained']\n",
      "[2024-11-11 22:43:49,170] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "[2024-11-11 22:43:49,342] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-11 22:43:49,364] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "[2024-11-11 22:43:49,442] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "[2024-11-11 22:43:49,575] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-11 22:43:49,633] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "[2024-11-11 22:43:49,648] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "[2024-11-11 22:43:49,682] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to npu (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-devel package with yum\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "FlashAttention is not installed.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
      "pip install xformers.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:209: ImportWarning: \n",
      "    *************************************************************************************************************\n",
      "    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..\n",
      "    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..\n",
      "    The backend in torch.distributed.init_process_group set to hccl now..\n",
      "    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..\n",
      "    The device parameters have been replaced with npu in the function below:\n",
      "    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Generator, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty\n",
      "    *************************************************************************************************************\n",
      "    \n",
      "  warnings.warn(msg, ImportWarning)\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "FlashAttention is not installed.\n",
      "[2024-11-11 22:44:05,228] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-11 22:44:05,522] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-11 22:44:05,631] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-11 22:44:06,588] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-11 22:44:06,632] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "[2024-11-11 22:44:06,632] [INFO] [comm.py:683:init_distributed] Initializing TorchBackend in DeepSpeed with backend hccl\n",
      "[2024-11-11 22:44:06,643] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:06,695] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:07,381] [INFO] [comm.py:652:init_distributed] cdb=None\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/mnt/jllm/train_pipe.py:303: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  args.seq_len = int(open(os.path.join(args.train_data,[f for f in os.listdir(args.train_data) if f[-4:] == '.crc'][0])).read().split()[1])\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/ma-user/anaconda3/envs/PyTorch-2.1.0/lib/python3.9/site-packages/torch_npu/contrib/transfer_to_npu.py:171: RuntimeWarning: torch.jit.script will be disabled by transfer_to_npu, which currently does not support it.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "[2024-11-11 22:44:14,331] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-11 22:44:14,342] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-11 22:44:14,410] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-11 22:44:14,517] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-11 22:44:14,541] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "SEED_LAYERS=False BASE_SEED=1234 SEED_FN=None\n",
      "Using topology: {ProcessCoord(data=0, pipe=0, model=0): 0, ProcessCoord(data=0, pipe=0, model=1): 1, ProcessCoord(data=0, pipe=1, model=0): 2, ProcessCoord(data=0, pipe=1, model=1): 3, ProcessCoord(data=1, pipe=0, model=0): 4, ProcessCoord(data=1, pipe=0, model=1): 5, ProcessCoord(data=1, pipe=1, model=0): 6, ProcessCoord(data=1, pipe=1, model=1): 7}\n",
      "[2024-11-11 22:44:14,548] [INFO] [train_pipe.py:475:custom_partition_layers] Partitioning pipeline stages with method 0, 1, 2\n",
      "stage=0 layers=1\n",
      "     0: ParallelStartPipe 29\n",
      "stage=1 layers=1\n",
      "     1: ParallelEndPipe 27\n",
      "  loss: ParallelCrossEntropy\n",
      "[2024-11-11 22:44:14,561] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-11 22:44:14,730] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-11 22:44:14,815] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:447: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//args.data_parallel_size\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:14,934] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:447: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//args.data_parallel_size\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:15,035] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:447: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//args.data_parallel_size\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:15,073] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:447: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//args.data_parallel_size\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:15,165] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.15.1, git-hash=unknown, git-branch=unknown\n",
      "[2024-11-11 22:44:15,165] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-11 22:44:15,259] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-11 22:44:15,329] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:447: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//args.data_parallel_size\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:15,358] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:447: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//args.data_parallel_size\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:15,373] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:447: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//args.data_parallel_size\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:15,376] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "Warning: The torch.npu.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='npu') to create tensors.\n",
      "/mnt/jllm/train_pipe.py:447: ResourceWarning: unclosed file <_io.TextIOWrapper name='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct/.wikipedia-zh-cn-512-00000.crc' mode='r' encoding='UTF-8'>\n",
      "  int(open(os.path.join(args.train_data,f)).read().split()[0])//args.per_device_train_batch_size//args.data_parallel_size\n",
      "ResourceWarning: Enable tracemalloc to get the object allocation traceback\n",
      "[2024-11-11 22:44:15,435] [INFO] [config.py:733:__init__] Config mesh_device None world_size = 2\n",
      "[2024-11-11 22:44:15,625] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-11-11 22:44:15,626] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-11-11 22:44:15,626] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-11-11 22:44:15,630] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2024-11-11 22:44:15,630] [INFO] [logging.py:96:log_dist] [Rank 0] Creating BF16 optimizer\n",
      "[2024-11-11 22:44:15,638] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-11 22:44:15,641] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-11 22:44:15,653] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-11 22:44:15,654] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-11 22:44:15,713] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-11 22:44:15,970] [INFO] [utils.py:781:see_memory_usage] begin bf16_optimizer\n",
      "[2024-11-11 22:44:15,971] [INFO] [utils.py:782:see_memory_usage] MA 3.67 GB         Max_MA 4.82 GB         CA 5.06 GB         Max_CA 5 GB \n",
      "[2024-11-11 22:44:15,972] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.2 GB, percent = 6.0%\n",
      "[2024-11-11 22:44:16,231] [INFO] [utils.py:781:see_memory_usage] before initializing group 0\n",
      "[2024-11-11 22:44:16,232] [INFO] [utils.py:782:see_memory_usage] MA 3.67 GB         Max_MA 3.67 GB         CA 5.06 GB         Max_CA 5 GB \n",
      "[2024-11-11 22:44:16,233] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.19 GB, percent = 6.0%\n",
      "[2024-11-11 22:44:16,512] [INFO] [utils.py:781:see_memory_usage] after initializing group 0\n",
      "[2024-11-11 22:44:16,513] [INFO] [utils.py:782:see_memory_usage] MA 14.39 GB         Max_MA 14.39 GB         CA 22.93 GB         Max_CA 23 GB \n",
      "[2024-11-11 22:44:16,513] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 60.2 GB, percent = 6.0%\n",
      "[2024-11-11 22:44:16,648] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:16,649] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:16,649] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_01_model_states.pt...\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_03_model_states.pt...\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_01_model_states.pt.\n",
      "[2024-11-11 22:44:16,650] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_03_model_states.pt.\n",
      "[2024-11-11 22:44:16,651] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:16,651] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_00-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:16,651] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:16,651] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_01-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:16,651] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_02_model_states.pt...\n",
      "[2024-11-11 22:44:16,651] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_00-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:16,652] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_02_model_states.pt.\n",
      "[2024-11-11 22:44:16,652] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_01-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:16,805] [INFO] [utils.py:781:see_memory_usage] before initializing group 1\n",
      "[2024-11-11 22:44:16,806] [INFO] [utils.py:782:see_memory_usage] MA 14.39 GB         Max_MA 14.39 GB         CA 22.93 GB         Max_CA 23 GB \n",
      "[2024-11-11 22:44:16,807] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 61.58 GB, percent = 6.1%\n",
      "[2024-11-11 22:44:17,134] [INFO] [utils.py:781:see_memory_usage] after initializing group 1\n",
      "[2024-11-11 22:44:17,136] [INFO] [utils.py:782:see_memory_usage] MA 14.39 GB         Max_MA 14.39 GB         CA 22.93 GB         Max_CA 23 GB \n",
      "[2024-11-11 22:44:17,136] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 64.71 GB, percent = 6.4%\n",
      "[2024-11-11 22:44:17,446] [INFO] [utils.py:781:see_memory_usage] end bf16_ optimizer\n",
      "[2024-11-11 22:44:17,447] [INFO] [utils.py:782:see_memory_usage] MA 14.39 GB         Max_MA 14.39 GB         CA 22.93 GB         Max_CA 23 GB \n",
      "[2024-11-11 22:44:17,448] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 67.71 GB, percent = 6.7%\n",
      "[2024-11-11 22:44:17,448] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = BF16_Optimizer\n",
      "[2024-11-11 22:44:17,448] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-11-11 22:44:17,448] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = <torch.optim.lr_scheduler.LambdaLR object at 0xffff01f98c40>\n",
      "[2024-11-11 22:44:17,448] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.0003, 0.0003], mom=[(0.9, 0.95), (0.9, 0.95)]\n",
      "[2024-11-11 22:44:17,449] [INFO] [config.py:999:print] DeepSpeedEngine configuration:\n",
      "[2024-11-11 22:44:17,450] [INFO] [config.py:1003:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-11-11 22:44:17,450] [INFO] [config.py:1003:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True, 'use_gds': False}\n",
      "[2024-11-11 22:44:17,450] [INFO] [config.py:1003:print]   amp_enabled .................. False\n",
      "[2024-11-11 22:44:17,450] [INFO] [config.py:1003:print]   amp_params ................... False\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   bfloat16_enabled ............. True\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0xffff01f5bbe0>\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   communication_data_type ...... None\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   curriculum_enabled_legacy .... False\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   curriculum_params_legacy ..... False\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   data_efficiency_enabled ...... False\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   dataloader_drop_last ......... False\n",
      "[2024-11-11 22:44:17,451] [INFO] [config.py:1003:print]   disable_allgather ............ False\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   dump_state ................... False\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   eigenvalue_enabled ........... False\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   eigenvalue_verbose ........... False\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   elasticity_enabled ........... False\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   fp16_auto_cast ............... None\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   fp16_enabled ................. False\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   global_rank .................. 0\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   grad_accum_dtype ............. None\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   gradient_accumulation_steps .. 1\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   gradient_clipping ............ 1.0\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-11-11 22:44:17,452] [INFO] [config.py:1003:print]   graph_harvesting ............. False\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   initial_dynamic_scale ........ 1\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   load_universal_checkpoint .... False\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   loss_scale ................... 1.0\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   memory_breakdown ............. False\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   mics_hierarchial_params_gather  False\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   mics_shard_size .............. -1\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName')\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   optimizer_name ............... None\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   optimizer_params ............. None\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   pld_enabled .................. False\n",
      "[2024-11-11 22:44:17,453] [INFO] [config.py:1003:print]   pld_params ................... False\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   prescale_gradients ........... False\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   scheduler_name ............... None\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   scheduler_params ............. None\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   sparse_attention ............. None\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   sparse_gradients_enabled ..... False\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   steps_per_print .............. 1\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   timers_config ................ enabled=True synchronized=True\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   train_batch_size ............. 2\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   use_data_before_expert_parallel_  False\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   use_node_local_storage ....... False\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   wall_clock_breakdown ......... False\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   weight_quantization_config ... None\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   world_size ................... 2\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   zero_allow_untested_optimizer  True\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   zero_config .................. stage=0 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500000000 overlap_comm=False load_from_fp32_weights=True elastic_checkpoint=False offload_param=DeepSpeedZeroOffloadParamConfig(device='none', nvme_path=None, buffer_count=5, buffer_size=100000000, max_in_cpu=1000000000, pin_memory=False) offload_optimizer=DeepSpeedZeroOffloadOptimizerConfig(device='none', nvme_path=None, buffer_count=4, pin_memory=False, pipeline_read=False, pipeline_write=False, fast_init=False, ratio=1.0) sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=30000000 param_persistence_threshold=10000 model_persistence_threshold=9223372036854775807 max_live_parameters=30000000 max_reuse_distance=1000000000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=False pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   zero_enabled ................. False\n",
      "[2024-11-11 22:44:17,454] [INFO] [config.py:1003:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-11-11 22:44:17,455] [INFO] [config.py:1003:print]   zero_optimization_stage ...... 0\n",
      "[2024-11-11 22:44:17,455] [INFO] [config.py:989:print_user_config]   json = {\n",
      "    \"train_batch_size\": 2, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": 1, \n",
      "    \"zero_allow_untested_optimizer\": true, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"offload_param\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"offload_optimizer\": {\n",
      "            \"device\": \"none\"\n",
      "        }, \n",
      "        \"stage3_param_persistence_threshold\": 1.000000e+04, \n",
      "        \"stage3_max_live_parameters\": 3.000000e+07, \n",
      "        \"stage3_prefetch_bucket_size\": 3.000000e+07, \n",
      "        \"memory_efficient_linear\": false\n",
      "    }, \n",
      "    \"activation_checkpointing\": {\n",
      "        \"partition_activations\": false, \n",
      "        \"cpu_checkpointing\": false, \n",
      "        \"contiguous_memory_optimization\": false, \n",
      "        \"number_checkpoints\": null, \n",
      "        \"synchronize_checkpoint_boundary\": false, \n",
      "        \"profile\": false\n",
      "    }, \n",
      "    \"bf16\": {\n",
      "        \"enabled\": true\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"hybrid_engine\": {\n",
      "        \"enabled\": false, \n",
      "        \"max_out_tokens\": 512, \n",
      "        \"inference_tp_size\": 1, \n",
      "        \"release_inference_cache\": false, \n",
      "        \"pin_parameters\": true, \n",
      "        \"tp_gather_partition_size\": 8\n",
      "    }\n",
      "}\n",
      "[2024-11-11 22:44:17,455] [INFO] [engine.py:105:__init__] CONFIG: micro_batches=1 micro_batch_size=1\n",
      "[2024-11-11 22:44:17,455] [INFO] [engine.py:146:__init__] is_pipe_partitioned= True is_grad_partitioned= True\n",
      "[2024-11-11 22:44:17,991] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_01-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:18,004] [INFO] [engine.py:165:__init__] RANK=3 STAGE=1 LAYERS=1 [1, 2) STAGE_PARAMS=1889271040 (1889.271M) TOTAL_PARAMS=7615820800 (7615.821M) UNIQUE_PARAMS=7615820800 (7615.821M)\n",
      "[2024-11-11 22:44:18,004] [INFO] [engine.py:165:__init__] RANK=0 STAGE=0 LAYERS=1 [0, 1) STAGE_PARAMS=1918639360 (1918.639M) TOTAL_PARAMS=7615820800 (7615.821M) UNIQUE_PARAMS=7615820800 (7615.821M)\n",
      "[2024-11-11 22:44:18,004] [INFO] [engine.py:165:__init__] RANK=1 STAGE=0 LAYERS=1 [0, 1) STAGE_PARAMS=1918639360 (1918.639M) TOTAL_PARAMS=7615820800 (7615.821M) UNIQUE_PARAMS=7615820800 (7615.821M)\n",
      "[2024-11-11 22:44:18,005] [INFO] [engine.py:165:__init__] RANK=2 STAGE=1 LAYERS=1 [1, 2) STAGE_PARAMS=1889271040 (1889.271M) TOTAL_PARAMS=7615820800 (7615.821M) UNIQUE_PARAMS=7615820800 (7615.821M)\n",
      "[2024-11-11 22:44:18,006] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_01-model_01-model_states.pt...\n",
      "[2024-11-11 22:44:18,007] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:18,007] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:18,007] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:18,008] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:18,008] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_03_model_states.pt...\n",
      "[2024-11-11 22:44:18,008] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:18,008] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:18,008] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_00_model_states.pt...\n",
      "[2024-11-11 22:44:18,008] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:18,008] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_01_model_states.pt...\n",
      "[2024-11-11 22:44:18,008] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_03_model_states.pt.\n",
      "[2024-11-11 22:44:18,009] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_01-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:18,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:18,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_01_model_states.pt.\n",
      "[2024-11-11 22:44:18,009] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_00-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:18,009] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_00_model_states.pt.\n",
      "[2024-11-11 22:44:18,010] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/mp_rank_02_model_states.pt...\n",
      "[2024-11-11 22:44:18,010] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_00-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:18,010] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/mp_rank_02_model_states.pt.\n",
      "[2024-11-11 22:44:18,011] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_01-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:18,016] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_00-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:18,030] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_00-model_01-model_states.pt...\n",
      "[2024-11-11 22:44:18,243] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_01-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:18,266] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_01-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:18,282] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_00-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:18,300] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_00-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:19,326] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_01-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:19,340] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_01-model_01-model_states.pt...\n",
      "[2024-11-11 22:44:19,346] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_00-model_01-model_states.pt.\n",
      "[2024-11-11 22:44:19,467] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_00-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:19,483] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_00-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:19,532] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_01-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:19,548] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_01-model_00-model_states.pt...\n",
      "[2024-11-11 22:44:19,551] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_00-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:19,568] [INFO] [torch_checkpoint_engine.py:27:load] [Torch] Loading checkpoint from checkpoint/8/layer_00-model_01-model_states.pt...\n",
      "Rank 5: saving the final model to pretrained: tensor-02-of-02-pipeline-01-of-02.safetensors...\n",
      "[2024-11-11 22:44:19,574] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_01-model_01-model_states.pt.\n",
      "Rank 7: saving the final model to pretrained: tensor-02-of-02-pipeline-02-of-02.safetensors...\n",
      "[2024-11-11 22:44:19,922] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_01-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:19,940] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_00-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:20,788] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_01-model_01-model_states.pt.\n",
      "[2024-11-11 22:44:20,854] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_00-model_00-model_states.pt.\n",
      "[2024-11-11 22:44:20,943] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_01-model_00-model_states.pt.\n",
      "Namespace(local_rank=0, model='Qwen2.5-7B-Instruct', train_data='wikipedia-zh-cn-512_Qwen2.5-7B-Instruct', eval_data='', from_ckpt='checkpoint', resume_ckpt='', only_load_model=False, tag='8', ds_config=None, zero_stage=0, split_dlayer=True, num_layers_per_decoder=None, emb_partitions=1, timeout=1800, pipe_parallel_size=2, encoder_pipe_parallel_size=0, model_parallel_size=2, offload=False, partition_method='fast', multi_layerspec=False, num_train_epochs=0, per_device_train_batch_size=1, gradient_accumulation_steps=1, weight_decay=0.0, lr_scheduler_type=<SchedulerType.COSINE: 'cosine'>, num_warmup_steps=0, learning_rate=0.0003, seq_len=8193, block_mask=False, skip_train_steps=set(), steps_per_print=1, steps_per_eval=-1, steps_per_checkpoint=-1, checkpoint='', background_executor='process', ckpt_step_gt=0, best_of=1, ckpt_epoch=set(), skip_epoch=set(), max_num_checkpoints=-1, only_ckpt_model=False, only_ckpt_lora=False, only_cache_model=False, early_stop=-1, checkpoint_grad_interval=0, no_checkpoint_grad_step=1000000, low_mem=False, no_shuf=False, no_safetensor=False, init=False, cache_model=None, seed=1234, output_dir='pretrained', lora_dim=0, lora_alpha=1, lora_module_name='qkv_proj,o_proj,gate_up_proj,down_proj', only_optimize_lora=False, deepspeed=False, deepspeed_config=None, deepscale=False, deepscale_config=None, device='npu', global_rank=0, world_size=8, data_parallel_size=2, num_training_steps=0, block_class_id=-100)\n",
      "Rank 0: saving the final model to pretrained: tensor-01-of-02-pipeline-01-of-02.safetensors...\n",
      "Rank 2: saving the final model to pretrained: tensor-01-of-02-pipeline-02-of-02.safetensors...\n",
      "[2024-11-11 22:44:21,438] [INFO] [torch_checkpoint_engine.py:29:load] [Torch] Loaded checkpoint from checkpoint/8/layer_00-model_01-model_states.pt.\n",
      "Rank 5: saved the final model to pretrained: tensor-02-of-02-pipeline-01-of-02.safetensors.\n",
      "Rank 7: saved the final model to pretrained: tensor-02-of-02-pipeline-02-of-02.safetensors.\n",
      "[2024-11-11 22:44:28,899] [INFO] [launch.py:351:main] Process 73488 exits successfully.\n",
      "[2024-11-11 22:44:28,899] [INFO] [launch.py:351:main] Process 73491 exits successfully.\n",
      "Rank 0: saved the final model to pretrained: tensor-01-of-02-pipeline-01-of-02.safetensors.\n",
      "Rank 2: saved the final model to pretrained: tensor-01-of-02-pipeline-02-of-02.safetensors.\n",
      "[2024-11-11 22:44:29,900] [INFO] [launch.py:351:main] Process 73489 exits successfully.\n",
      "[2024-11-11 22:44:30,901] [INFO] [launch.py:351:main] Process 73486 exits successfully.\n",
      "[2024-11-11 22:44:38,910] [INFO] [launch.py:351:main] Process 73490 exits successfully.\n",
      "[2024-11-11 22:44:39,911] [INFO] [launch.py:351:main] Process 73492 exits successfully.\n",
      "[2024-11-11 22:44:47,920] [INFO] [launch.py:351:main] Process 73487 exits successfully.\n",
      "[2024-11-11 22:44:57,930] [INFO] [launch.py:351:main] Process 73485 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "!deepspeed --module jllm.train_pipe \\\n",
    "    --model Qwen2.5-7B-Instruct \\\n",
    "    --train_data wikipedia-zh-cn-512_Qwen2.5-7B-Instruct \\\n",
    "    --pipe_parallel_size 2 \\\n",
    "    --model_parallel_size 2 \\\n",
    "    --partition_method fast \\\n",
    "    --split_dlayer \\\n",
    "    --num_train_epochs 0 \\\n",
    "    --from_ckpt checkpoint \\\n",
    "    --output_dir pretrained\n",
    "#--model 模型路径\n",
    "#--train_data 训练数据\n",
    "#--pipe_parallel_size 流水线长度\n",
    "#--partition_method 流水线拆分方法\n",
    "#--split_dlayer 将decoder layer拆开,使流水线分布更均匀\n",
    "#--from_ckpt 加载模型参数的checkpoint路径\n",
    "#--output_dir  输出huggingface格式模型的路径"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7956374-28f3-4248-a8b8-0cdbfb21a0e4",
   "metadata": {},
   "source": [
    "#### 5.2 合并拆分张量(model_parallel_size>2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bfeb9775-1548-461f-816e-42708bb2a5ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 162/162 [00:02<00:00, 56.17it/s]\n",
      "100%|█████████████████████████████████████████| 177/177 [00:03<00:00, 55.44it/s]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "!python -m jllm.cat2hf \\\n",
    "    -C pretrained \n",
    "# -C 合并前的模型路径\n",
    "# -H 合并后的huggingface格式的模型路径。不填的自行创建为pretrained_hf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912a9764-6243-44c7-a1b6-cb1a34cfc0f1",
   "metadata": {},
   "source": [
    "### 6 模型测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6181c5c9-757f-4259-a551-68901c209216",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer=AutoTokenizer.from_pretrained('Qwen2.5-7B-Instruct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3182983b-1459-4db8-aefd-b4b017f2553c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b9d172ab75472b9d8965b65656806b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    'pretrained_hf',\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac4396f4-8a4a-4b3e-b99c-3751f6093b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='数学是研究数量、结构以及空间等概念及其变化的一门学科，属于'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a6f28f48-626a-4fdf-8039-ae32c8455620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36fea6b9-b426-437c-85fc-52e5b4682aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "[W VariableFallbackKernel.cpp:51] Warning: CAUTION: The operator 'aten::isin.Tensor_Tensor_out' is not currently supported on the NPU backend and will fall back to run on the CPU. This may have performance implications. (function npu_cpu_fallback)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'数学是研究数量、结构以及空间等概念及其变化的一门学科，属于形式科学的一种。数学是人类对事物的抽象结构与模式进行严格描述的一种通用手段，可以应用于现实世界的任何问题。数学家和哲学家对数学的确切范围和定义有一系列的看法。数学在人类历史发展和社会生活中发挥着不可替代的作用，也是学习和研究现代科学技术必不可少的基本工具。数学是研究数量、结构、变化、空间以及信息等概念的一门学科，从某种角度看属于形式科学的一种。数学家和哲学家对数学的确切范围和定义有一系列的看法。而在人类历史发展和社会生活中，数学也发挥着不可替代的作用，也是学习和研究'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "st=time()\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=128\n",
    ")\n",
    "du=time()-st\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d693b60-901e-4c44-952b-e8b3163bc313",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.7173362469042015"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "128/du"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcee376-a9f8-48c6-9999-25067705d852",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
